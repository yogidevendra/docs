{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials, operator library and code samples to rapidly create DataTorrent applications using Java or dtAssemble.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demos/", 
            "text": "Running Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications and they are available for import from the \nAppHub\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nAppHub\n section of the DataTorrent console.\n\n\nSelect one of the available packages, such as \nPi Demo\n and click \nImport\n button.\n\n\nImported application packages and included applications will be listed under \nDevelop \n Application Packages\n page.\n\n\n\n\nLaunching Demo Applications\n\n\nOnce imported, applications can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nDevelop \n Application Packages\n, and select one of the imported demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and click the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by clicking the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nConfiguring Launch Parameters\n\n\nSome applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.\n\n\n\n\n\n\nNavigate to \nAppHub\n of the DataTorrent console.  In this example, we will use \nTwitter Demo\n application package. Click \nImport\n button. \n\n\n\n\n\n\nNavigate to \nDevelop \n Application Packages\n. From the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboard\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the Monitor section of \ndtManage\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Running Apps"
        }, 
        {
            "location": "/demos/#running-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications and they are available for import from the  AppHub  section of the DataTorrent management console.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  AppHub  section of the DataTorrent console.  Select one of the available packages, such as  Pi Demo  and click  Import  button.  Imported application packages and included applications will be listed under  Develop   Application Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#launching-demo-applications", 
            "text": "Once imported, applications can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.    Navigate to  Develop   Application Packages , and select one of the imported demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and click the launch button.     Proceed with default options on launch confirmation screen by clicking the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Launching Demo Applications"
        }, 
        {
            "location": "/demos/#configuring-launch-parameters", 
            "text": "Some applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.    Navigate to  AppHub  of the DataTorrent console.  In this example, we will use  Twitter Demo  application package. Click  Import  button.     Navigate to  Develop   Application Packages . From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboard .", 
            "title": "Configuring Launch Parameters"
        }, 
        {
            "location": "/demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the Monitor section of  dtManage  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nThe DataTorrent Sandbox provides a quick and simple way to experience\nDataTorrent RTS without setting up and managing a complete Hadoop cluster.\nThe Sandbox has the latest version of DataTorrent RTS Enterprise Edition\npre-installed along with all the Hadoop services required to launch and run\nthe included demo applications.\n\n\nInstallation\n\n\nYou'll need to install Virtual Box in order to run the sandbox.\nOracle VirtualBox is a virtual machine manager (version 4.3 or later)\nand can be downloaded from \nVirtualBox\n.\n\n\nDownload the sandbox from\n\ndatatorrent.com/download\n and import it\ninto Virtual Box. Then select the newly imported image in the left pane of the\nVirtual Box console and click the \nStart\n button to\nstart the sandbox virtual machine. When the machine comes up, you should see a\nbrowser window showing a login dialog which is discussed in the next section.\n\n\nAccessing Console\n\n\nWhen accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\n\n\n\n\nIf the various HDFS services are not yet ready, you may see a red warning saying\nHDFS is not ready like this:\n\n\n\n\nThis warning should disappear after a few minutes and the console should appear\nlooking something like this:\n\n\n\n\nIf the warning persists after several minutes, check that all required services\nare running as described in the \nService Management\n section below.\nPlease go to \nTroubleshooting\n for detailed instructions.\n\n\nInside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting \nhttp://localhost:9090/\n\n\nConfiguring the Sandbox\n\n\nThe sandbox is configured with 6GB RAM; if your development machine has 16GB or\nmore, you can increase the sandbox RAM to 8GB or more using the VirtualBox\nconsole. This will yield better performance and support larger applications.\n\n\nAdditionally, you can change the network adapter from \nNAT\n to\n\nBridged Adapter\n; this will allow you to login to the sandbox from your\nhost as well as transfer files back and forth using \nssh\n and \nscp\n on Linux\nor \nssh\n tools like \nPuTTY\n and \npscp\n on Windows.\n\n\nA simpler option for sharing files is to mount a shared folder from the host\non the guest; this may be the only option in some environments where the\nsandbox is unable to acquire a separate IP address from DHCP. See, for example,\n\nhttp://www.htpcbeginner.com/mount-virtualbox-shared-folder-on-ubuntu-linux/\n or\n\nhttp://www.htpcbeginner.com/setup-virtualbox-shared-folders-linux-windows/\n.\n\n\nOf course all such configuration must be done when when the sandbox is not running.\n\n\nRunning Demo Applications\n\n\nOnce authenticated, you can continue to \nDemo Applications\n section to learn how to import, launch, and run demo applications.\n\n\nService Management \n\n\nThe sandbox automatically launches the following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nDepending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see the warning message described above.\n\n\nIf the warning persists after several minutes, check the status of each of these\nfollowing services: you can do that with a shell script like this:\n\n\n#!/bin/bash\nservices='hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-yarn-resourcemanager hadoop-yarn-nodemanager dtgateway'\nfor s in $services; do\n    sudo service \"$s\" status\ndone\n\n\n\nIf any of these services are not running, you can start them by running a similar\nscript but with \nstatus\n replaced by \nstart\n.\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "The DataTorrent Sandbox provides a quick and simple way to experience\nDataTorrent RTS without setting up and managing a complete Hadoop cluster.\nThe Sandbox has the latest version of DataTorrent RTS Enterprise Edition\npre-installed along with all the Hadoop services required to launch and run\nthe included demo applications.", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#installation", 
            "text": "You'll need to install Virtual Box in order to run the sandbox.\nOracle VirtualBox is a virtual machine manager (version 4.3 or later)\nand can be downloaded from  VirtualBox .  Download the sandbox from datatorrent.com/download  and import it\ninto Virtual Box. Then select the newly imported image in the left pane of the\nVirtual Box console and click the  Start  button to\nstart the sandbox virtual machine. When the machine comes up, you should see a\nbrowser window showing a login dialog which is discussed in the next section.", 
            "title": "Installation"
        }, 
        {
            "location": "/sandbox/#accessing-console", 
            "text": "When accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.     If the various HDFS services are not yet ready, you may see a red warning saying\nHDFS is not ready like this:   This warning should disappear after a few minutes and the console should appear\nlooking something like this:   If the warning persists after several minutes, check that all required services\nare running as described in the  Service Management  section below.\nPlease go to  Troubleshooting  for detailed instructions.  Inside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting  http://localhost:9090/", 
            "title": "Accessing Console"
        }, 
        {
            "location": "/sandbox/#configuring-the-sandbox", 
            "text": "The sandbox is configured with 6GB RAM; if your development machine has 16GB or\nmore, you can increase the sandbox RAM to 8GB or more using the VirtualBox\nconsole. This will yield better performance and support larger applications.  Additionally, you can change the network adapter from  NAT  to Bridged Adapter ; this will allow you to login to the sandbox from your\nhost as well as transfer files back and forth using  ssh  and  scp  on Linux\nor  ssh  tools like  PuTTY  and  pscp  on Windows.  A simpler option for sharing files is to mount a shared folder from the host\non the guest; this may be the only option in some environments where the\nsandbox is unable to acquire a separate IP address from DHCP. See, for example, http://www.htpcbeginner.com/mount-virtualbox-shared-folder-on-ubuntu-linux/  or http://www.htpcbeginner.com/setup-virtualbox-shared-folders-linux-windows/ .  Of course all such configuration must be done when when the sandbox is not running.", 
            "title": "Configuring the Sandbox"
        }, 
        {
            "location": "/sandbox/#running-demo-applications", 
            "text": "Once authenticated, you can continue to  Demo Applications  section to learn how to import, launch, and run demo applications.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/sandbox/#service-management", 
            "text": "The sandbox automatically launches the following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see the warning message described above.  If the warning persists after several minutes, check the status of each of these\nfollowing services: you can do that with a shell script like this:  #!/bin/bash\nservices='hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-yarn-resourcemanager hadoop-yarn-nodemanager dtgateway'\nfor s in $services; do\n    sudo service \"$s\" status\ndone  If any of these services are not running, you can start them by running a similar\nscript but with  status  replaced by  start .", 
            "title": "Service Management "
        }, 
        {
            "location": "/sandbox/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/aws_emr_manual/", 
            "text": "Overview\n\n\nThis document describes steps to run DT apps on AWS cluster. Users can easily try out apps from the \nAppHub\n by downloading the app installers from the DataTorrent website. A zip package containing \nbash\n scripts will be downloaded on user\u2019s machine and user needs to follow the instructions below to deploy apps.\n\n\nSetup\n\n\n\n\nInstall the AWS Command Line Interface\n\n\nRefer to the \nlink\n for instructions\n\n\n\n\n\n\nConfigure the AWS Command Line Interface\n\n\n\n\n  $ aws configure\n  AWS Access Key ID [None]: MENTION ACCESS_KEY_ID\n  AWS Secret Access Key [None]: MENTION SECRET_ACCESS_KEY\n  Default region name [None]: MENTION REGION\n  Default output format [None]: _Press ENTER_\n\n\n\n\n\nFor detailed instructions, refer to the \nlink\n\n\n\n\n\n\nVerify that AWS Command Line Interface is configured by ensuring the following returns correct values\n\n\n\n\n$ aws configure get aws_access_key_id\n$ aws configure get aws_secret_access_key\n$ aws configure get region\n\n\n\n\n\n\n\n\n\n\n\nPackage contents\n\n\nThe zip file (see \nend-to-end steps\n below) will contain following files:\n\n\n\n\n\n\nDTapp-EMR-Deploy.sh\n - Script that user needs to execute to set up AWS EMR Cluster. The cluster will be created and DT-RTS installed along with the app.\n\n\n\n\n\n\nconfig.properties\n - This file contains properties used by the script. File has sample values for the properties as mentioned below.\n\n(\nNote:\n \nSample values for the properties should be sufficient unless it needs to be changed.\n)\n\n\n\n\n\n\n\n    \n\n        \n\n        \n\n        \n\n    \n\n    \n\n        \n\n            \nProperty\n\n            \nSample Value\n\n            \nDescription\n\n        \n\n        \n\n            \nSECURITY_GROUP_NAME\n\n            \nDTapp-security-group\n\n            \nSecurity Group with this name will be created and inbound rule with user machine\u2019s external IP address will be added to it.\n\n        \n\n        \n\n            \nBUCKET_NAME\n\n            \ndtapp-emr\n\n            \nS3 bucket name. A bucket with this name will be created and script that installs DT-RTS will be placed into it. This script is used as a bootstrap script during cluster launch.\n\n        \n\n        \n\n            \nS3_REGION\n\n            \nus-east-1\n\n            \nRegion where S3 bucket is created\n\n        \n\n        \n\n            \nLOG_URI\n\n            \ns3n://dtapp-logs\n\n            \nS3 URI where log files would be stored\n\n        \n\n        \n\n            \nCLUSTER_NAME\n\n            \nDTapp\n\n            \nName of the EMR cluster\n\n        \n\n        \n\n            \nCORE_INSTANCE_TYPE\n\n            \nm1.large\n\n            \nType of machine for core instance\n\n        \n\n        \n\n            \nCORE_INSTANCE_COUNT\n\n            \n1\n\n            \nNumber of core instances to be created\n\n        \n\n        \n\n            \nMASTER_INSTANCE_TYPE\n\n            \nm1.medium\n\n            \nType of machine for master instance\n\n        \n\n        \n\n            \nMASTER_INSTANCE_COUNT\n\n            \n1\n\n            \nNumber of Master instances to be created\n\n        \n\n    \n\n\n\n\n\n\n\n\n\nDTapp-EMR-Terminate.sh\n - Script to terminate the cluster\n\n\n\n\n\n\nREADME\n - Readme file with instructions\n\n\n\n\n\n\nEnd to End workflow\n\n\nThe steps below describe the end to end flow to run any app provided in the AppHub. Let's consider that we want to run Kinesis to S3 app on the AWS cluster.\n\n\n\n\n\n\nDownload the zip file for the app.\n\nhttps://www.datatorrent.com/downloads/aws-apps/app-kinesis-to-s3.zip\n\n\n\n\n\n\nExtract the zip\n\n\nunzip app-kinesis-to-s3.zip\n\n\n\n\n\n\nVerify the files. The directory structure should be as follows:\n\n\nuser@localhost:~/Downloads/app-kinesis-to-s3$ ls\nconfig.properties  DTapp-EMR-Deploy.sh  DTapp-EMR-Terminate.sh README\n\n\n\n\n\n\n\nEdit \nconfig.properties\n file. Modify the properties as described in the \nPackage Contents\n section.\n\n\n\n\n\n\nAssign Execute permissions.\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ chmod u+x /path/to/app-kinesis-to-s3/DTapp-EMR-*\n\n\n\n\n\n\n\nExecute the shell script.\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Deploy.sh\n\n\n\n\n\n\n\nWait till the EMR cluster is up. Once the cluster is up, a link to configure the DT-RTS will be provided. Click on the link. E.g  \nhttp://ec2-54-91-43-25.compute-1.amazonaws.com:9090\n\n\n\n\n\n\nDT-RTS Welcome Screen will appear. Click \ncontinue\n\n  \n\n\n\n\n\n\nDefault paths for Hadoop and the DFS root will be automatically populated, e.g. \n/usr/bin/hadoop\n and \n/user/dtadmin/datatorrent\n. You can leave these values alone unless you have some specific reason to change them. Click \ncontinue\n\n  \n\n\n\n\n\n\nA valid license would be needed to continue further. Click on \nRequest License\n.\n    \n\n\n\n\n\n\nDownload the appropriate license.\n    \n\n\n\n\n\n\nUpload the license. Once the license is successfully uploaded, Click Continue\n    \n\n    \n\n\n\n\n\n\nOnce the configuration is complete, Click Continue\n  \n\n\n\n\n\n\nThe app is imported and is ready to launch\n  \n\n  To know more about how to configure a particular app, visit the \nApp Templates\n section of our \ndocumentation\n\n\n\n\n\n\nTo terminate the AWS cluster, execute the termination script:\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Terminate.sh", 
            "title": "AWS"
        }, 
        {
            "location": "/aws_emr_manual/#overview", 
            "text": "This document describes steps to run DT apps on AWS cluster. Users can easily try out apps from the  AppHub  by downloading the app installers from the DataTorrent website. A zip package containing  bash  scripts will be downloaded on user\u2019s machine and user needs to follow the instructions below to deploy apps.", 
            "title": "Overview"
        }, 
        {
            "location": "/aws_emr_manual/#setup", 
            "text": "Install the AWS Command Line Interface  Refer to the  link  for instructions    Configure the AWS Command Line Interface     $ aws configure\n  AWS Access Key ID [None]: MENTION ACCESS_KEY_ID\n  AWS Secret Access Key [None]: MENTION SECRET_ACCESS_KEY\n  Default region name [None]: MENTION REGION\n  Default output format [None]: _Press ENTER_   For detailed instructions, refer to the  link    Verify that AWS Command Line Interface is configured by ensuring the following returns correct values   $ aws configure get aws_access_key_id\n$ aws configure get aws_secret_access_key\n$ aws configure get region", 
            "title": "Setup"
        }, 
        {
            "location": "/aws_emr_manual/#package-contents", 
            "text": "The zip file (see  end-to-end steps  below) will contain following files:    DTapp-EMR-Deploy.sh  - Script that user needs to execute to set up AWS EMR Cluster. The cluster will be created and DT-RTS installed along with the app.    config.properties  - This file contains properties used by the script. File has sample values for the properties as mentioned below. ( Note:   Sample values for the properties should be sufficient unless it needs to be changed. )    \n     \n         \n         \n         \n     \n     \n         \n             Property \n             Sample Value \n             Description \n         \n         \n             SECURITY_GROUP_NAME \n             DTapp-security-group \n             Security Group with this name will be created and inbound rule with user machine\u2019s external IP address will be added to it. \n         \n         \n             BUCKET_NAME \n             dtapp-emr \n             S3 bucket name. A bucket with this name will be created and script that installs DT-RTS will be placed into it. This script is used as a bootstrap script during cluster launch. \n         \n         \n             S3_REGION \n             us-east-1 \n             Region where S3 bucket is created \n         \n         \n             LOG_URI \n             s3n://dtapp-logs \n             S3 URI where log files would be stored \n         \n         \n             CLUSTER_NAME \n             DTapp \n             Name of the EMR cluster \n         \n         \n             CORE_INSTANCE_TYPE \n             m1.large \n             Type of machine for core instance \n         \n         \n             CORE_INSTANCE_COUNT \n             1 \n             Number of core instances to be created \n         \n         \n             MASTER_INSTANCE_TYPE \n             m1.medium \n             Type of machine for master instance \n         \n         \n             MASTER_INSTANCE_COUNT \n             1 \n             Number of Master instances to be created \n         \n         DTapp-EMR-Terminate.sh  - Script to terminate the cluster    README  - Readme file with instructions", 
            "title": "Package contents"
        }, 
        {
            "location": "/aws_emr_manual/#end-to-end-workflow", 
            "text": "The steps below describe the end to end flow to run any app provided in the AppHub. Let's consider that we want to run Kinesis to S3 app on the AWS cluster.    Download the zip file for the app. https://www.datatorrent.com/downloads/aws-apps/app-kinesis-to-s3.zip    Extract the zip  unzip app-kinesis-to-s3.zip    Verify the files. The directory structure should be as follows:  user@localhost:~/Downloads/app-kinesis-to-s3$ ls\nconfig.properties  DTapp-EMR-Deploy.sh  DTapp-EMR-Terminate.sh README    Edit  config.properties  file. Modify the properties as described in the  Package Contents  section.    Assign Execute permissions.   user@localhost:~/Downloads/app-kinesis-to-s3$ chmod u+x /path/to/app-kinesis-to-s3/DTapp-EMR-*    Execute the shell script.   user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Deploy.sh    Wait till the EMR cluster is up. Once the cluster is up, a link to configure the DT-RTS will be provided. Click on the link. E.g   http://ec2-54-91-43-25.compute-1.amazonaws.com:9090    DT-RTS Welcome Screen will appear. Click  continue \n      Default paths for Hadoop and the DFS root will be automatically populated, e.g.  /usr/bin/hadoop  and  /user/dtadmin/datatorrent . You can leave these values alone unless you have some specific reason to change them. Click  continue \n      A valid license would be needed to continue further. Click on  Request License .\n        Download the appropriate license.\n        Upload the license. Once the license is successfully uploaded, Click Continue\n     \n        Once the configuration is complete, Click Continue\n      The app is imported and is ready to launch\n   \n  To know more about how to configure a particular app, visit the  App Templates  section of our  documentation    To terminate the AWS cluster, execute the termination script:   user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Terminate.sh", 
            "title": "End to End workflow"
        }, 
        {
            "location": "/create/", 
            "text": "Creating Applications\n\n\nHere are a few links to get you started:\n\n\n\n\n\n\nBeginner's Guide\n An introductory guide that shows you how to generate a new\n  ready-to-run Apex application using the maven archetype and a good deal of the application\n  landscape at an introductory level.\n\n\n\n\n\n\nOperator Development\n\n\n\n\n\n\nOperator Library\n\n\n\n\n\n\nDemo Videos\n\n\n\n\n\n\nApplication Development Tutorials\n\n\nExplore Apache Apex application development with one of the tutorial applications below.\n\n\n\n\n\n\nTop N Words\n is a complete guide to writing your first Apache Apex application using \nJava\n or \ndtAssemble\n\n\n\n\n\n\n\n\nSales Dimensions\n is an introduction to assembling and visualizing sales analytics application with \ndtAssemble\n\n\n\n\n\n\n\n\nAdvanced Topics\n\n\n\n\nApplication Development\n - comprehensive guide to developing Apache Apex applications\n\n\nApplication Packaging\n - creating application packages, changing settings, and launching application packages\n\n\nOperator Development\n - creating new operators for Apache Apex applications\n\n\ndtGateway REST API\n - complete listing of all services offered by dtGateway", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#creating-applications", 
            "text": "Here are a few links to get you started:    Beginner's Guide  An introductory guide that shows you how to generate a new\n  ready-to-run Apex application using the maven archetype and a good deal of the application\n  landscape at an introductory level.    Operator Development    Operator Library    Demo Videos", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#application-development-tutorials", 
            "text": "Explore Apache Apex application development with one of the tutorial applications below.    Top N Words  is a complete guide to writing your first Apache Apex application using  Java  or  dtAssemble     Sales Dimensions  is an introduction to assembling and visualizing sales analytics application with  dtAssemble", 
            "title": "Application Development Tutorials"
        }, 
        {
            "location": "/create/#advanced-topics", 
            "text": "Application Development  - comprehensive guide to developing Apache Apex applications  Application Packaging  - creating application packages, changing settings, and launching application packages  Operator Development  - creating new operators for Apache Apex applications  dtGateway REST API  - complete listing of all services offered by dtGateway", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/beginner/", 
            "text": "Beginner's Guide to Apache Apex\n\n\nIntroduction\n\n\nApache Apex is a fault-tolerant, high-performance platform and framework for building\ndistributed applications; it is built on Hadoop. This guide is targeted at Java\ndevelopers who are getting started with building Apex applications.\n\n\nQuickstart\n\n\nThose eager to get started right away can download, build, and run a few sample\napplications; just follow these steps:\n\n\n\n\nMake sure you have \nJava JDK\n, \nmaven\n and \ngit\n installed.\n\n\nClone the examples git repo: \nhttps://github.com/DataTorrent/examples\n\n\nSwitch to \ntutorials/fileOutput\n and build it either in your favorite IDE or on the command line with:\n\n\n\n\ncd examples/tutorials/fileOutput\nmvn clean package -DskipTests\n\n\n\n\n\n\nRun the test in your IDE or on the command line with:\n\n\n\n\nmvn test\n\n\n\n\nSome of the applications are discussed in the \nSample Applications\n section below.\nThe rest of this document provides a more leisurely introduction to Apex.\n\n\nPreliminaries\n\n\nBefore beginning development, you'll need to make sure the following prerequisites\nare present; details of setting up your development environment are\n\nhere\n:\n\n\n\n\nRecent versions of the Java JDK, maven, git.\n\n\nA Hadoop cluster where the application can be deployed.\n\n\nA working internet connection.\n\n\n\n\nRunning the maven archetype\n\n\nApex applications use the \nmaven\n build tool. The maven archetype is useful for avoiding\nthe tedious process of creating a suitable \npom.xml\n maven build file by hand; it also\ngenerates a simple default application with two operators that you can build and run\nwithout having to write any code or make any changes. To run it, place the following\nlines in a text file (called, say \nnewapp.sh\n) and run it with the shell\n(e.g. \nbash newapp.sh\n):\n\n\nv=\n3.3.0-incubating\n\nmvn -B archetype:generate \\\n  -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=\n$v\n \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.myapexapp \\\n  -DartifactId=myapexapp \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\nAs new versions are released, you might need to update the version number (the\nthe variable \nv\n above). You can also run the archetype from your Java IDE as described\n\nhere\n.\n\n\nIt should create a new project directory named \nmyapexapp\n with these 3 Java source files:\n\n\nsrc/test/java/com/example/myapexapp/ApplicationTest.java\nsrc/main/java/com/example/myapexapp/Application.java\nsrc/main/java/com/example/myapexapp/RandomNumberGenerator.java\n\n\n\n\nThe project should also contain these properties files:\n\n\nsrc/site/conf/my-app-conf1.xml\nsrc/main/resources/META-INF/properties.xml\n\n\n\n\nYou should now be able to step into the new directory and build the project:\n\n\ncd myapexapp; mvn clean package -DskipTests\n\n\n\n\nThis will create a directory named \ntarget\n and an application package file\nwithin it named \nmyapexapp-1.0-SNAPSHOT.apa\n.\n\n\nThese files are discussed further in the sections below.\n\n\nThe default application\n\n\nWe now discuss the default application generated by the archetype in some detail.\nAdditional, more realistic applications are presented in the section titled\n\nSample Applications\n below.\n\n\nThe default application creates an application in \nApplication.java\n with 2 operators:\n\n\n@ApplicationAnnotation(name=\nMyFirstApplication\n)\npublic class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf) {\n    RandomNumberGenerator randomGenerator = dag.addOperator(\nrandomGenerator\n, RandomNumberGenerator.class);\n    randomGenerator.setNumTuples(500);\n    ConsoleOutputOperator cons = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\n    dag.addStream(\nrandomData\n, randomGenerator.out, cons.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}\n\n\n\n\nThe application is named \nMyFirstApplication\n via the annotation\n\n@ApplicationAnnotation\n; this name will be displayed in the UI console and must be\nunique among the applications running on a cluster. It can also be changed at launch time.\n\n\nThe \npopulateDAG\n method is the only one that you'll need to implement. Its contents\nfall into three categories: operator creation, operator configuration, and stream\ncreation.\n\n\nTwo operators are created: \nrandomGenerator\n and \nconsole\n. The first is\ndefined in \nRandomNumberGenerator.java\n; it generates random\nfloating point values and emits them on its output port. The second is an instance\nof \nConsoleOutputOperator\n class defined in \nMalhar\n \n the library of pre-built\noperators. The first argument to \naddOperator()\n is the name of this operator instance\n(there can be multiple instances of the same class, so we need a unique name to\ndistinguish them); the second can be either a class object that needs to be instantiated\nas shown for \nrandomGenerator\n or an actual instance of that class as shown for \nconsole\n.\n\n\nThe operators can be configured by calling setter methods on them; the call to\n\nsetNumTuples\n is an example. However, operators are typically configured via XML\nproperties files as discussed in later sections below.\n\n\nFinally, the \naddStream\n call creates a stream named \nrandomData\n connecting the\noutput port of first operator to the input port of the second.\n\n\nRunning the application and the unit test\n\n\nThe file \nApplicationTest.java\n contains a unit test that can be run from an IDE\nby highlighting the \ntestApplication\n method and selecting the appropriate\noption from the dropdown; it can also be run the maven command line:\n\n\nmvn -Dtest=ApplicationTest#testApplication test\n\n\n\n\nIt runs for about 10 seconds printing each random number with a prefix of \nhello world:\n.\nThe first argument explicitly selects the test to run (\ntestApplication\n)\nfrom the named class (\nApplicationTest\n); you can omit it and just run \nmvn test\n to\nrun all of the unit tests.\n\n\nIt is important to note that this particular test is actually a test of the entire\napplication rather than a single class or a method within a class. It uses a class\ncalled \nLocalMode\n to essentially simulate a cluster. It is an extremely useful\ntechnique for testing your application without the need for a cluster. It can be used\nin more elaborate ways to test complex applications as discussed in the section\nentitled \nLocal Mode Testing\n below.\n\n\nTo run the application, you need access to a cluster with Hadoop installed; there are\nmultiple options here:\n\n\n\n\nDownload and setup the sandbox as described\n  \nhere\n. This is\n  the simplest option for experimenting with Apex since it has all the necessary pieces\n  installed.\n\n\nDownload and install the DataTorrent Community or Enterprise Edition downloadable from\n  \nhere\n.\n\n\nUse an existing DataTorrent RTS licensed installation.\n\n\nClone the Apex source code on a cluster with Hadoop already installed, build it and\n  use the \napexcli\n command line tool (previously named \ndtcli\n) from there to run your\n  application as described in \nthis video\n.\n\n\n\n\nWith the first 3 methods, you have a browser-based GUI console and you can simply\nnavigate to \nDevelop\n \n \nUpload Package\n and upload the \n.apa\n file built earlier;\nthen run it using the \nlaunch\n button. If using the command line tool, run that tool\nthen use the command \nlaunch myapexapp-1.0-SNAPSHOT.apa\n to launch the application. You\ncan also specify a particular XML configuration file that is packaged with the\napplication to use during launch, for example:\n\nlaunch -apconf my-app-conf1.xml myapexapp-1.0-SNAPSHOT.apa\n\nFor an exhaustive list of commands available with this tool, please see\n\nhere\n.\n\n\nMore details about configuration files are provided in the next section.\n\n\nConfiguring your application\n\n\nApplication configuration is typically done via XML properties files though it can also\nbe done with code as shown above. The \nproperties.xml\n file mentioned earlier has some\nexamples:\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.randomGenerator.prop.numTuples\n/name\n\n  \nvalue\n1000\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.console.prop.stringFormat\n/name\n\n  \nvalue\nhello world: %s\n/value\n\n\n/property\n\n\n\n\n\nThe number of tuples output per window (windows are discussed in greater detail below)\nis set both in the code and in this file; in such cases, values in this properties\nfile override those set in the code.\n\n\nConfiguration values can also be placed in XML\nfiles under \nsite/conf\n and the file \nmy-app-conf1.xml\n mentioned above is an example.\nThese files are not processed automatically; they need to be explicitly selected at\nlaunch time. To do this in the GUI, select the \nUse saved configuration\n checkbox in\nthe launch dialog and choose the desired file from the dropdown. When a property is\nspecified in multiple files, precedence rules determine the final value; those rules\nare discussed\n\nhere\n.\n\n\nAttributes and Properties\n\n\nProperties are simply public accessor methods in the operator classes and govern the\nfunctionality of the operator. Attributes on the other hand are pre-defined and affect\nhow the operator or application behaves with respect to its environment. We have already\nseen a couple of examples of properties, namely, the number of tuples emitted per window\nby the random number generator (\nnumTuples\n) and the prefix string appended to each\nvalue before it is output by the console operator (\nstringFormat\n).\n\n\nOperator properties that are more complex objects than the primitive types can also\nbe initialized from XML files. For example, if we have properties declared as\n\nint[] counts;\n and \nString[] paths;\n in an operator named \nfoo\n, we can initialize\nthem with:\n\n\nproperty\n\n  \nname\ndt.operator.foo.prop.counts\n/name\n\n  \nvalue\n10, 20, 30\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.operator.foo.prop.paths\n/name\n\n  \nvalue\n/tmp/path1\n, \n/tmp/path3\n, \n/tmp/path3\n/value\n\n\n/property\n\n\n\n\n\nAn example of an attribute is the amount of memory allocated to the Buffer Server (see\nsection below entitled \nBuffer Server\n); it is named \nBUFFER_MEMORY_MB\n can be set\nlike this:\n\n\nproperty\n\n  \nname\ndt.application.{appName}.operator.{opName}.port.{portName}.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n128\n/value\n\n\n/property\n\n\n\n\n\nHere \n{appName}\n, \n{opName}\n, \n{portName}\n are appropriate application, operator\nand port names respectively; they can also be replaced with asterisks (wildcards). The\ndefault value is 512MB.\n\n\nSome additional attributes include:\n\n\n\n\n\n\n\n\nAttribute name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPARTITIONER\n\n\ncustom partitioner class associated with an operator\n\n\n\n\n\n\nPARTITION_PARALLEL\n\n\nif true, triggers parallel partitioning of a downstream operator when an upstream operator is partitioned.\n\n\n\n\n\n\nSTREAM_CODEC\n\n\ncontrols serialization/deserialization as well as destination partitions\n\n\n\n\n\n\nRECOVERY_ATTEMPTS\n\n\nmaximum restart attempts of a failed operator\n\n\n\n\n\n\n\n\nCurrently available attribute names are in \nContext\n class of the \napi\n module in\nApex source code.\n\n\nFor additional examples of initialization of properties, including list and map\nobjects, please look\n\nhere\n.\n\n\nLocal Mode Testing\n\n\nAs noted above, the \nLocalMode\n class is used for testing the application locally in\nyour development environment. A common, though suboptimal, use looks like this:\n\n\ntry {\n  LocalMode lma = LocalMode.newInstance();\n  Configuration conf = new Configuration(false);\n  conf.addResource(this.getClass().getResourceAsStream(\n/META-INF/properties.xml\n));\n  lma.prepareDAG(new Application(), conf);\n  LocalMode.Controller lc = lma.getController();\n  lc.run(10000); // runs for 10 seconds and quits\n} catch (ConstraintViolationException e) {\n  Assert.fail(\nconstraint violations: \n + e.getConstraintViolations());\n}\n\n\n\n\nHere, a \nConfiguration\n object containing all the appropriate settings of properties and\nattributes for the application is created by parsing the default \nproperties.xml\n file,\na new \nApplication\n object is created and configured and finally a controller used for\ntimed execution of the application. This approach, though occasionally useful to uncover\nshallow bugs, has one glaring deficiency \n it does not check the results in any way\nas most unit tests do. We strongly recommend avoiding this usage pattern.\n\n\nA far better (and recommended) approach is to write data to some storage system\nthat can then be queried for verification. An example of this approach is\n\nhere\n and looks like this:\n\n\ntry {\n  LocalMode lma = LocalMode.newInstance();\n  lma.prepareDAG(new Application(), getConfig());\n  LocalMode.Controller lc = lma.getController();\n  lc.runAsync();\n\n  // wait for output files to show up\n  while ( ! check() ) {\n    System.out.println(\nSleeping ....\n);\n    Thread.sleep(1000);\n  }\n} catch (ConstraintViolationException e) {\n  Assert.fail(\nconstraint violations: \n + e.getConstraintViolations());\n}\n\n\n\n\n\nHere, we invoke \nrunAsync\n on the controller to fork off a separate thread to run the\napplication while the main thread enters a loop where it looks for the presence of the\nexpected output files. A pair of functions can be defined to setup prerequisites such\nas starting external servers, creating directories, etc. and perform the corresponding\nteardown upon test termination; these functions are annotated with \n@Before\n and \n@After\n.\n\n\nChecking logs\n\n\nLogs for the Application Master and the various containers can be retrieved and viewed\non the UI console by navigating to the \nPhysical\n tab, clicking on the specific container\nin question, clicking on the blue \nlogs\n button and then selecting the appropriate file\nfrom the dropdown. If you don't have access to the UI, you'll need to log in to the\nappropriate node on the cluster and check the logs there.\n\n\nA good starting point is the\nYARN log file which is usually present at \n/var/log/hadoop-yarn\n or \n/var/log/hadoop\n or\nsimilar locations and named \nyarn-{user}-resourcemanager-{host}.log\n or\n\nhadoop-cmf-yarn-RESOURCEMANAGER-{host}.log.out\n (where \n{user}\n and \n{host}\n have\nappropriate values) or something similar. This file will\nindicate what containers were allocated for each application, whether the allocation\nsucceeded or failed and where each container is running. Typically, application and\ncontainer ids will have the respective forms \napplication_1448033276100_0001\n and\n\ncontainer_1462948052533_0001_01_022468\n.\n\n\nIf the application failed \nduring\n launch,\nthe YARN logs will usually have enough information to diagnose the root cause -- for\nexample, a container requiring more memory than is available. If the application launch\nwas successful, you'll see the containers transition through various states: \nNEW\n,\n\nALLOCATED\n, \nACQUIRED\n, \nRUNNING\n.\n\n\nIf it failed \nafter\n launch, the logs of the particular container that failed will shed\nmore light on the issue. Those logs are located on the appropriate node under a\ndirectory with the same name as the container id which itself is under a directory\nnamed with the application id, for example:\n\n\ndtadmin@dtbox:/sfw/hadoop/shared/logs$ cd nodemanager/application_1465857463845_0001/\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls\ncontainer_1465857463845_0001_01_000001  container_1465857463845_0001_01_000003\ncontainer_1465857463845_0001_01_000002  container_1465857463845_0001_01_000004\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *1\nAppMaster.stderr  AppMaster.stdout  dt.log\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *2\ndt.log  stderr  stdout\n\n\n\n\nThe Application Master container id always has the suffix \n_000001\n.\n\n\nOperators, Ports and Streams\n\n\nAn \noperator\n is, simply put, a class that\nimplements the \nOperator\n interface. Though the class could be written to directly\nimplement that interface, a more common and easier method is to extend \nBaseOperator\n\nsince it provides default empty implementations of all the required methods.\nWe've already seen an example above, namely \nRandomNumberGenerator\n.\n\n\nA \nport\n is a class that can either emit (output port) or ingest (input port) data.\nInput and output ports implement the \nInputPort\n and \nOutputPort\n interfaces\nrespectively. More commonly, output ports are simply defined as instances of\n\nDefaultOutputPort\n, for example:\n\n\npublic final transient DefaultOutputPort\nDouble\n out = new DefaultOutputPort\nDouble\n();\n\n\n\n\nand input ports are defined as anonymous inner classes that extend \nDefaultInputPort\n:\n\n\npublic final transient DefaultInputPort\nDouble\n input = new DefaultInputPort\nDouble\n()\n{\n  @Override\n  public void process(Double v) {\n    out.emit(v);\n  }\n}\n\n\n\n\nA \nstream\n is the set of links connecting a single port of an upstream operator to one or\nmore input ports of downstream operators. We've already seen an example of a stream above,\nnamely \nrandomData\n. If the upstream operator is not partitioned, tuples are delivered to\nthe input ports of a stream in the same order in which they were written to the output\nport; this guarantee may change in the future. For a more detailed explanation of these\nconcepts, please\nlook \nhere\n.\n\n\nAnnotations\n\n\nAnnotations are an important tool for expressing desired guarantees which are then\nverified in a validation phase before running the application. Some examples:\n\n\n@Min(1)\nint index;\n\n@NotNull\nString name\n\n@NotNull\n@Size(min=1, max=128)\nprivate String[] path;\n\n@NotNull\n@Size(min = 1)\nprivate final Set\nString\n files;\n\n@Valid\nFooBar object;\n\n\n\n\nThe \n@Min\n and \n@Max\n annotations check lower and upper bounds; \n@Size\n checks the\nsize of a collection or array against minimum and maximum limits; \n@Valid\n requests\nrecursive validation checks on the object.\n\n\nThere are also a few Apex-specific annotations; we've seen one example above, namely\n\n@ApplicationAnnotation\n used to set the name of the application. A couple of others\nare useful to declare that a port within an operator need not be connected:\n\n\n@InputPortFieldAnnotation(optional = true)\npublic final transient InputPort\nObject\n inportWithCodec = new DefaultInputPort\n();\n\n@OutputPortFieldAnnotation(optional = true)\npublic final transient DefaultOutputPort\nMap\nString, Object\n outBindings\n   = new DefaultOutputPort\n();\n\n\n\n\nFor additional information about annotations, please see:\n\nhere\n and\n\nhere\n\n\nPartitioners, Unifiers and StreamCodecs\n\n\nPartitioning is a mechanism for load balancing; it involves replicating one or more\noperators so that the load can be shared by all the replicas. Partitioning is\naccomplished by the \ndefinePartitions\n method of the \nPartitioner\n interface.\nA couple of implementations are available: \nStatelessPartitioner\n (static partitioning)\nand \nStatelessThroughputBasedPartitioner\n (dynamic partitioning, see below).\nThese can be used by setting the \nPARTITIONER\n\nattribute on the operator by including a stanza like the following in your\nconfiguration file (where \n{appName}\n and \n{opName}\n are the\nappropriate application and operator names):\n\n\nproperty\n\n  \nname\ndt.application.{appName}.operator.{opName}.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\n\nThe number after the colon specifies the number of desired partitions. This can be\ndone for any operator that is not a connector (i.e. input or output operator) and is not\nannotated with \n@OperatorAnnotation(partitionable = false)\n. No code changes are necessary.\nIncoming tuples entering input ports of the operator are automatically distributed among the\npartitions based on their hash code by default. You can get greater control of how tuples\nare distributed to partitions by using a \nStreamCodec\n; further discussion of stream\ncodecs is deferred to the Advanced Guide [\ncoming soon\n].\nA small sample program illustrating use of stream codecs is\n\nhere\n.\n\n\nConnectors need special care since they interact with external systems. Many connectors\n(e.g. Kafka input, file input and output operators) implement the \nPartitioner\n\ninterface and support partitioning using custom implementations of \ndefinePartitions\n\n\nwithin\n the operator.\nDocumentation and/or source code of the individual connectors should be consulted for\ndetails.\n\n\nSometimes there is need to replicate an entire linear segment of the DAG; this is\nknown as \nParallel Partitioning\n and is achieved by setting the \nPARTITION_PARALLEL\n\nattribute on the input port of each downstream operator that is part of the linear\nsegment. Both of these mechanisms are described in greater detail in the\n\nAdvanced Features\n section of the\n\nTop N Words tutorial\n\n\nAs mentioned above, the \nStatelessPartitioner\n is used for \nstatic\n\npartitioning since it occurs once before the application starts. Dynamic partitioning\nwhile the application is running is also possible using the\n\nStatelessThroughputBasedPartitioner\n or a custom partitioner. Implementing such\na partitioner needs special care, especially if the operator to be partitioned\nhas some accumulated state since this state typically needs to be redistributed among\nthe newly created partitions. An example of a custom partitioner that does dynamic\npartitioning is \nhere\n.\n\n\nUnifiers are the flip side of the partitioning coin: When data that was intended to be\nprocessed by a single instance is now processed by multiple partitions, each instance\ncomputes partial results since it processes only part of the stream; these partial\nresults need to be combined to form the final result; this is the function of a unifier.\n\n\nFor example, suppose an operator is processing numbers and computes the sum of all the\nvalues seen in a window. If it is partitioned into N replicas, each replica is computing\na partial sum and we would need a unifier that computes the overall sum from\nthese N partial sums. A sample application that shows how to define and use a unifier is\navailable \nhere\n.\n\n\nA unifier for an operator is provided by a suitable override of the \ngetUnifier()\n method\nof the output port, for example:\n\n\n  public final transient DefaultOutputPort\nHighLow\nInteger\n out\n    = new DefaultOutputPort\nHighLow\nInteger\n() {\n    @Override\n    public Unifier\nHighLow\nInteger\n getUnifier() {\n      return new UnifierRange\nInteger\n()\n    }\n  };\n\n\n\n\nIf no unifier is supplied for a partitioned operator, the platform will supply a default\npass-through unifier.\n\n\nWhen the number of partitions is large and the unifier involves non-trivial computations\nthere is a risk that it can become a bottleneck; in such cases, the \nUNIFIER_LIMIT\n\nattribute can be set on the appropriate output port. The platform will then automatically\ngenerate the required number of parallel unifiers, cascading into multiple levels if\nnecessary, to ensure that the number of input streams at each unifier does not exceed\nthis limit.\n\n\nBuffer Server\n\n\nThe \nBuffer Server\n is a separate service within a container which implements\na publish-subscribe model. It is present whenever the container hosts an operator\nwith an output port connected to another operator outside the container.\n\n\nThe output port is the publisher and the connected input ports of downstream operators\nare the subscribers. It buffers tuples so that they can be replayed when a\ndownstream operator fails and is restarted. As described earlier, the memory allocated\nto a buffer server is user configurable via an attribute named \nBUFFER_MEMORY_MB\n and\ndefaults to 512MB.\n\n\nThe total memory required by a container that hosts many such operators may climb rapidly;\nreducing the value of this attribute is advisable in such cases in a memory constrained\nenvironment.\n\n\nAllocating Operator Memory\n\n\nA container is a JVM process; the maximum memory each such container can consume is\nuser configurable via the \nMEMORY_MB\n attribute whose default value is 1GB.\nIf it is too large, container allocation may fail before the application even\nbegins; if it is too small, the application may fail at run time as it tries to\nallocate memory and runs up against this limit.\nAn example of setting this limit:\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.randomGenerator.attr.MEMORY_MB\n/name\n\n  \nvalue\n300\n/value\n\n\n/property\n\n\n\n\n\nAs before, wildcards can be used to set the same value for all operators by replacing\nthe operator name \nrandomGenerator\n with an asterisk. The application name can also be\nomitted, so a shorter version is:\n\n\nproperty\n\n  \nname\ndt.operator.*.attr.MEMORY_MB\n/name\n\n  \nvalue\n300\n/value\n\n\n/property\n\n\n\n\n\nThe Application Master (aka \nStrAM\n) is a supervisory Java process associated with each\napplication; memory allocation for it is specified slightly differently:\n\n\nproperty\n\n  \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n  \nvalue\n700\n/value\n\n\n/property\n\n\n\n\n\nFor small applications, a value as low as 700 may be adequate; for larger applications,\na value 2000 or more may be needed. If this value is too small, the application typically\nfails at startup with no user-visible diagnostics; YARN logs need to be examined in such\ncases.\n\n\nSample Applications\n\n\nThis section briefly discusses some sample applications using commonly used connectors.\nThe applications themselves are available at\n\nExamples\n.\nWe briefly describe a few of them below.\n\n\nEach example has a brief \nREADME.md\n file (in markdown format) describing what the\napplication does. In most cases, the unit tests function as full application tests that\ncan be run locally in your development environment without the need for a cluster as\ndescribed above.\n\n\nApplication \n file copy\n\n\nThe \nfileIO-simple\n application copies all data verbatim from files added to an input\ndirectory to rolling files in an output directory. The input and output directories,\nthe output file base name and maximum size are configured in \nMETA_INF/properties.xml\n:\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.input.prop.directory\n/name\n\n  \nvalue\n/tmp/SimpleFileIO/input-dir\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.filePath\n/name\n\n  \nvalue\n/tmp/SimpleFileIO/output-dir\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.fileName\n/name\n\n  \nvalue\nmyfile\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.maxLength\n/name\n\n  \nvalue\n1000000\n/value\n\n\n/property\n\n\n\n\n\nThe application DAG is created in \nApplication.java\n:\n\n\n@Override\npublic void populateDAG(DAG dag, Configuration conf)\n{\n  // create operators\n  FileLineInputOperator in = dag.addOperator(\ninput\n, new FileLineInputOperator());\n  FileOutputOperator out = dag.addOperator(\noutput\n, new FileOutputOperator());\n\n  // create streams\n  dag.addStream(\ndata\n, in.output, out.input);\n}\n\n\n\n\nThe \nFileLineInputOperator\n is part of Malhar and is a concrete class that extends\n\nAbstractFileInputOperator\n. The \nFileOutputOperator\n is defined locally\nand extends the \nAbstractFileOutputOperator\n and overrides 3 methods:\n+ \ngetFileName\n which simply returns the current file name\n+ \ngetBytesForTuple\n which appends a newline to the argument string, converts it to an array of bytes and returns it.\n+ \nsetup\n which creates the actual file name by appending the operator id to the configured base name (this last step is necessary when partitioning is involved to ensure that multiple partitions do not write to the same file).\n\n\nOutput files are created with temporary names like \nmyfile_p2.0.1465929407447.tmp\n and\nrenamed to \nmyfile_p2.0\n when they reach the maximum configured size.\n\n\nApplication \n database to file\n\n\nThe \njdbcIngest\n application reads rows from a table in \nMySQL\n, creates Java objects\n(_POJO_s) and writes them to a file in the user specified directory in HDFS.\n\n\nApplication configuration values are specified in 2 files: \n\nMETA_INF/properties.xml\n and \nsrc/site/conf/example.xml\n. The former uses the in-memory\ndatabase \nHSQLDB\n and is used by the unit test in JdbcInputAppTest; this test can be\nrun, as described earlier, either in your IDE or using maven on the command line.\nThe latter uses \nMySql\n and is intended for use on a cluster. To run on a cluster you'll\nneed a couple of preparatory steps:\n\n\n\n\nMake sure \nMySql\n is installed on the cluster.\n\n\nChange \nexample.xml\n to reflect proper values for \ndatabaseUrl\n, \nuserName\n, \npassword\n and \nfilePath\n.\n\n\nCreate the required table and rows by running the SQL queries in the file \nsrc/test/resources/example.sql\n.\n\n\nCreate the HDFS output directory if necessary.\n\n\nBuild the project to create the \n.apa\n package\n\n\nLaunch the application, selecting \nexample.xml\n as the configuration file during launch.\n\n\nVerify that the expected output file is present.\n\n\n\n\nFurther details on these steps are in the project \nREADME.md\n file.\n\n\nThe application uses two operators: The first is \nFileLineOutputOperator\n which extends\n\nAbstractFileOutputOperator\n and provides implementations for two methods:\n\ngetFileName\n and \ngetBytesForTuple\n. The former creates a file name using the operator\nid \n this is important if this operator has multiple partitions and prevents the\npartitions from writing to the same file (which could cause garbled data). The latter\nsimply converts the incoming object to an array of bytes and returns it.\n\n\nThe second is \nJdbcPOJOInputOperator\n which comes from Malhar; it reads records from\na table and outputs them on the output port; they type of object that is emitted is\nspecified by the value of the  \nTUPLE_CLASS\n attribute in the configuration file\nnamely \nPojoEvent\n in this case. This operator also needs a couple of additional\nproperties: (a) a list of \nFieldInfo\n objects that describe the mapping from table\ncolumns to fields of the Pojo; and (b) a \nstore\n object that deals with the details\nof establishing a connection to the database server.\n\n\nThe application itself is then created in the usual way in \nJdbcHDFSApp.populateDAG\n:\n\n\nJdbcPOJOInputOperator jdbcInputOperator = dag.addOperator(\nJdbcInput\n, new JdbcPOJOInputOperator());\njdbcInputOperator.setFieldInfos(addFieldInfos());\n\njdbcInputOperator.setStore(new JdbcStore());\n\nFileLineOutputOperator fileOutput = dag.addOperator(\nFileOutputOperator\n, new FileLineOutputOperator());\n\ndag.addStream(\nPOJO's\n, jdbcInputOperator.outputPort, fileOutput.input).setLocality(Locality.CONTAINER_LOCAL);\n\n\n\n\nAppHub\n\n\nAppHub\n is a source of application templates. There are many more applications for various use cases to help jump start development effort. \n\n\nAdditional Resources\n\n\n\n\n\n\nDevelopment environment setup\n\n\n\n\n\n\nTroubleshooting guide", 
            "title": "Beginner's Guide"
        }, 
        {
            "location": "/beginner/#beginners-guide-to-apache-apex", 
            "text": "", 
            "title": "Beginner's Guide to Apache Apex"
        }, 
        {
            "location": "/beginner/#introduction", 
            "text": "Apache Apex is a fault-tolerant, high-performance platform and framework for building\ndistributed applications; it is built on Hadoop. This guide is targeted at Java\ndevelopers who are getting started with building Apex applications.", 
            "title": "Introduction"
        }, 
        {
            "location": "/beginner/#quickstart", 
            "text": "Those eager to get started right away can download, build, and run a few sample\napplications; just follow these steps:   Make sure you have  Java JDK ,  maven  and  git  installed.  Clone the examples git repo:  https://github.com/DataTorrent/examples  Switch to  tutorials/fileOutput  and build it either in your favorite IDE or on the command line with:   cd examples/tutorials/fileOutput\nmvn clean package -DskipTests   Run the test in your IDE or on the command line with:   mvn test  Some of the applications are discussed in the  Sample Applications  section below.\nThe rest of this document provides a more leisurely introduction to Apex.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/beginner/#preliminaries", 
            "text": "Before beginning development, you'll need to make sure the following prerequisites\nare present; details of setting up your development environment are here :   Recent versions of the Java JDK, maven, git.  A Hadoop cluster where the application can be deployed.  A working internet connection.", 
            "title": "Preliminaries"
        }, 
        {
            "location": "/beginner/#running-the-maven-archetype", 
            "text": "Apex applications use the  maven  build tool. The maven archetype is useful for avoiding\nthe tedious process of creating a suitable  pom.xml  maven build file by hand; it also\ngenerates a simple default application with two operators that you can build and run\nwithout having to write any code or make any changes. To run it, place the following\nlines in a text file (called, say  newapp.sh ) and run it with the shell\n(e.g.  bash newapp.sh ):  v= 3.3.0-incubating \nmvn -B archetype:generate \\\n  -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion= $v  \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.myapexapp \\\n  -DartifactId=myapexapp \\\n  -Dversion=1.0-SNAPSHOT  As new versions are released, you might need to update the version number (the\nthe variable  v  above). You can also run the archetype from your Java IDE as described here .  It should create a new project directory named  myapexapp  with these 3 Java source files:  src/test/java/com/example/myapexapp/ApplicationTest.java\nsrc/main/java/com/example/myapexapp/Application.java\nsrc/main/java/com/example/myapexapp/RandomNumberGenerator.java  The project should also contain these properties files:  src/site/conf/my-app-conf1.xml\nsrc/main/resources/META-INF/properties.xml  You should now be able to step into the new directory and build the project:  cd myapexapp; mvn clean package -DskipTests  This will create a directory named  target  and an application package file\nwithin it named  myapexapp-1.0-SNAPSHOT.apa .  These files are discussed further in the sections below.", 
            "title": "Running the maven archetype"
        }, 
        {
            "location": "/beginner/#the-default-application", 
            "text": "We now discuss the default application generated by the archetype in some detail.\nAdditional, more realistic applications are presented in the section titled Sample Applications  below.  The default application creates an application in  Application.java  with 2 operators:  @ApplicationAnnotation(name= MyFirstApplication )\npublic class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf) {\n    RandomNumberGenerator randomGenerator = dag.addOperator( randomGenerator , RandomNumberGenerator.class);\n    randomGenerator.setNumTuples(500);\n    ConsoleOutputOperator cons = dag.addOperator( console , new ConsoleOutputOperator());\n    dag.addStream( randomData , randomGenerator.out, cons.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}  The application is named  MyFirstApplication  via the annotation @ApplicationAnnotation ; this name will be displayed in the UI console and must be\nunique among the applications running on a cluster. It can also be changed at launch time.  The  populateDAG  method is the only one that you'll need to implement. Its contents\nfall into three categories: operator creation, operator configuration, and stream\ncreation.  Two operators are created:  randomGenerator  and  console . The first is\ndefined in  RandomNumberGenerator.java ; it generates random\nfloating point values and emits them on its output port. The second is an instance\nof  ConsoleOutputOperator  class defined in  Malhar    the library of pre-built\noperators. The first argument to  addOperator()  is the name of this operator instance\n(there can be multiple instances of the same class, so we need a unique name to\ndistinguish them); the second can be either a class object that needs to be instantiated\nas shown for  randomGenerator  or an actual instance of that class as shown for  console .  The operators can be configured by calling setter methods on them; the call to setNumTuples  is an example. However, operators are typically configured via XML\nproperties files as discussed in later sections below.  Finally, the  addStream  call creates a stream named  randomData  connecting the\noutput port of first operator to the input port of the second.", 
            "title": "The default application"
        }, 
        {
            "location": "/beginner/#running-the-application-and-the-unit-test", 
            "text": "The file  ApplicationTest.java  contains a unit test that can be run from an IDE\nby highlighting the  testApplication  method and selecting the appropriate\noption from the dropdown; it can also be run the maven command line:  mvn -Dtest=ApplicationTest#testApplication test  It runs for about 10 seconds printing each random number with a prefix of  hello world: .\nThe first argument explicitly selects the test to run ( testApplication )\nfrom the named class ( ApplicationTest ); you can omit it and just run  mvn test  to\nrun all of the unit tests.  It is important to note that this particular test is actually a test of the entire\napplication rather than a single class or a method within a class. It uses a class\ncalled  LocalMode  to essentially simulate a cluster. It is an extremely useful\ntechnique for testing your application without the need for a cluster. It can be used\nin more elaborate ways to test complex applications as discussed in the section\nentitled  Local Mode Testing  below.  To run the application, you need access to a cluster with Hadoop installed; there are\nmultiple options here:   Download and setup the sandbox as described\n   here . This is\n  the simplest option for experimenting with Apex since it has all the necessary pieces\n  installed.  Download and install the DataTorrent Community or Enterprise Edition downloadable from\n   here .  Use an existing DataTorrent RTS licensed installation.  Clone the Apex source code on a cluster with Hadoop already installed, build it and\n  use the  apexcli  command line tool (previously named  dtcli ) from there to run your\n  application as described in  this video .   With the first 3 methods, you have a browser-based GUI console and you can simply\nnavigate to  Develop     Upload Package  and upload the  .apa  file built earlier;\nthen run it using the  launch  button. If using the command line tool, run that tool\nthen use the command  launch myapexapp-1.0-SNAPSHOT.apa  to launch the application. You\ncan also specify a particular XML configuration file that is packaged with the\napplication to use during launch, for example: launch -apconf my-app-conf1.xml myapexapp-1.0-SNAPSHOT.apa \nFor an exhaustive list of commands available with this tool, please see here .  More details about configuration files are provided in the next section.", 
            "title": "Running the application and the unit test"
        }, 
        {
            "location": "/beginner/#configuring-your-application", 
            "text": "Application configuration is typically done via XML properties files though it can also\nbe done with code as shown above. The  properties.xml  file mentioned earlier has some\nexamples:  property \n   name dt.application.MyFirstApplication.operator.randomGenerator.prop.numTuples /name \n   value 1000 /value  /property  property \n   name dt.application.MyFirstApplication.operator.console.prop.stringFormat /name \n   value hello world: %s /value  /property   The number of tuples output per window (windows are discussed in greater detail below)\nis set both in the code and in this file; in such cases, values in this properties\nfile override those set in the code.  Configuration values can also be placed in XML\nfiles under  site/conf  and the file  my-app-conf1.xml  mentioned above is an example.\nThese files are not processed automatically; they need to be explicitly selected at\nlaunch time. To do this in the GUI, select the  Use saved configuration  checkbox in\nthe launch dialog and choose the desired file from the dropdown. When a property is\nspecified in multiple files, precedence rules determine the final value; those rules\nare discussed here .", 
            "title": "Configuring your application"
        }, 
        {
            "location": "/beginner/#attributes-and-properties", 
            "text": "Properties are simply public accessor methods in the operator classes and govern the\nfunctionality of the operator. Attributes on the other hand are pre-defined and affect\nhow the operator or application behaves with respect to its environment. We have already\nseen a couple of examples of properties, namely, the number of tuples emitted per window\nby the random number generator ( numTuples ) and the prefix string appended to each\nvalue before it is output by the console operator ( stringFormat ).  Operator properties that are more complex objects than the primitive types can also\nbe initialized from XML files. For example, if we have properties declared as int[] counts;  and  String[] paths;  in an operator named  foo , we can initialize\nthem with:  property \n   name dt.operator.foo.prop.counts /name \n   value 10, 20, 30 /value  /property  property \n   name dt.operator.foo.prop.paths /name \n   value /tmp/path1 ,  /tmp/path3 ,  /tmp/path3 /value  /property   An example of an attribute is the amount of memory allocated to the Buffer Server (see\nsection below entitled  Buffer Server ); it is named  BUFFER_MEMORY_MB  can be set\nlike this:  property \n   name dt.application.{appName}.operator.{opName}.port.{portName}.attr.BUFFER_MEMORY_MB /name \n   value 128 /value  /property   Here  {appName} ,  {opName} ,  {portName}  are appropriate application, operator\nand port names respectively; they can also be replaced with asterisks (wildcards). The\ndefault value is 512MB.  Some additional attributes include:     Attribute name  Description      PARTITIONER  custom partitioner class associated with an operator    PARTITION_PARALLEL  if true, triggers parallel partitioning of a downstream operator when an upstream operator is partitioned.    STREAM_CODEC  controls serialization/deserialization as well as destination partitions    RECOVERY_ATTEMPTS  maximum restart attempts of a failed operator     Currently available attribute names are in  Context  class of the  api  module in\nApex source code.  For additional examples of initialization of properties, including list and map\nobjects, please look here .", 
            "title": "Attributes and Properties"
        }, 
        {
            "location": "/beginner/#local-mode-testing", 
            "text": "As noted above, the  LocalMode  class is used for testing the application locally in\nyour development environment. A common, though suboptimal, use looks like this:  try {\n  LocalMode lma = LocalMode.newInstance();\n  Configuration conf = new Configuration(false);\n  conf.addResource(this.getClass().getResourceAsStream( /META-INF/properties.xml ));\n  lma.prepareDAG(new Application(), conf);\n  LocalMode.Controller lc = lma.getController();\n  lc.run(10000); // runs for 10 seconds and quits\n} catch (ConstraintViolationException e) {\n  Assert.fail( constraint violations:   + e.getConstraintViolations());\n}  Here, a  Configuration  object containing all the appropriate settings of properties and\nattributes for the application is created by parsing the default  properties.xml  file,\na new  Application  object is created and configured and finally a controller used for\ntimed execution of the application. This approach, though occasionally useful to uncover\nshallow bugs, has one glaring deficiency   it does not check the results in any way\nas most unit tests do. We strongly recommend avoiding this usage pattern.  A far better (and recommended) approach is to write data to some storage system\nthat can then be queried for verification. An example of this approach is here  and looks like this:  try {\n  LocalMode lma = LocalMode.newInstance();\n  lma.prepareDAG(new Application(), getConfig());\n  LocalMode.Controller lc = lma.getController();\n  lc.runAsync();\n\n  // wait for output files to show up\n  while ( ! check() ) {\n    System.out.println( Sleeping .... );\n    Thread.sleep(1000);\n  }\n} catch (ConstraintViolationException e) {\n  Assert.fail( constraint violations:   + e.getConstraintViolations());\n}  Here, we invoke  runAsync  on the controller to fork off a separate thread to run the\napplication while the main thread enters a loop where it looks for the presence of the\nexpected output files. A pair of functions can be defined to setup prerequisites such\nas starting external servers, creating directories, etc. and perform the corresponding\nteardown upon test termination; these functions are annotated with  @Before  and  @After .", 
            "title": "Local Mode Testing"
        }, 
        {
            "location": "/beginner/#checking-logs", 
            "text": "Logs for the Application Master and the various containers can be retrieved and viewed\non the UI console by navigating to the  Physical  tab, clicking on the specific container\nin question, clicking on the blue  logs  button and then selecting the appropriate file\nfrom the dropdown. If you don't have access to the UI, you'll need to log in to the\nappropriate node on the cluster and check the logs there.  A good starting point is the\nYARN log file which is usually present at  /var/log/hadoop-yarn  or  /var/log/hadoop  or\nsimilar locations and named  yarn-{user}-resourcemanager-{host}.log  or hadoop-cmf-yarn-RESOURCEMANAGER-{host}.log.out  (where  {user}  and  {host}  have\nappropriate values) or something similar. This file will\nindicate what containers were allocated for each application, whether the allocation\nsucceeded or failed and where each container is running. Typically, application and\ncontainer ids will have the respective forms  application_1448033276100_0001  and container_1462948052533_0001_01_022468 .  If the application failed  during  launch,\nthe YARN logs will usually have enough information to diagnose the root cause -- for\nexample, a container requiring more memory than is available. If the application launch\nwas successful, you'll see the containers transition through various states:  NEW , ALLOCATED ,  ACQUIRED ,  RUNNING .  If it failed  after  launch, the logs of the particular container that failed will shed\nmore light on the issue. Those logs are located on the appropriate node under a\ndirectory with the same name as the container id which itself is under a directory\nnamed with the application id, for example:  dtadmin@dtbox:/sfw/hadoop/shared/logs$ cd nodemanager/application_1465857463845_0001/\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls\ncontainer_1465857463845_0001_01_000001  container_1465857463845_0001_01_000003\ncontainer_1465857463845_0001_01_000002  container_1465857463845_0001_01_000004\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *1\nAppMaster.stderr  AppMaster.stdout  dt.log\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *2\ndt.log  stderr  stdout  The Application Master container id always has the suffix  _000001 .", 
            "title": "Checking logs"
        }, 
        {
            "location": "/beginner/#operators-ports-and-streams", 
            "text": "An  operator  is, simply put, a class that\nimplements the  Operator  interface. Though the class could be written to directly\nimplement that interface, a more common and easier method is to extend  BaseOperator \nsince it provides default empty implementations of all the required methods.\nWe've already seen an example above, namely  RandomNumberGenerator .  A  port  is a class that can either emit (output port) or ingest (input port) data.\nInput and output ports implement the  InputPort  and  OutputPort  interfaces\nrespectively. More commonly, output ports are simply defined as instances of DefaultOutputPort , for example:  public final transient DefaultOutputPort Double  out = new DefaultOutputPort Double ();  and input ports are defined as anonymous inner classes that extend  DefaultInputPort :  public final transient DefaultInputPort Double  input = new DefaultInputPort Double ()\n{\n  @Override\n  public void process(Double v) {\n    out.emit(v);\n  }\n}  A  stream  is the set of links connecting a single port of an upstream operator to one or\nmore input ports of downstream operators. We've already seen an example of a stream above,\nnamely  randomData . If the upstream operator is not partitioned, tuples are delivered to\nthe input ports of a stream in the same order in which they were written to the output\nport; this guarantee may change in the future. For a more detailed explanation of these\nconcepts, please\nlook  here .", 
            "title": "Operators, Ports and Streams"
        }, 
        {
            "location": "/beginner/#annotations", 
            "text": "Annotations are an important tool for expressing desired guarantees which are then\nverified in a validation phase before running the application. Some examples:  @Min(1)\nint index;\n\n@NotNull\nString name\n\n@NotNull\n@Size(min=1, max=128)\nprivate String[] path;\n\n@NotNull\n@Size(min = 1)\nprivate final Set String  files;\n\n@Valid\nFooBar object;  The  @Min  and  @Max  annotations check lower and upper bounds;  @Size  checks the\nsize of a collection or array against minimum and maximum limits;  @Valid  requests\nrecursive validation checks on the object.  There are also a few Apex-specific annotations; we've seen one example above, namely @ApplicationAnnotation  used to set the name of the application. A couple of others\nare useful to declare that a port within an operator need not be connected:  @InputPortFieldAnnotation(optional = true)\npublic final transient InputPort Object  inportWithCodec = new DefaultInputPort ();\n\n@OutputPortFieldAnnotation(optional = true)\npublic final transient DefaultOutputPort Map String, Object  outBindings\n   = new DefaultOutputPort ();  For additional information about annotations, please see: here  and here", 
            "title": "Annotations"
        }, 
        {
            "location": "/beginner/#partitioners-unifiers-and-streamcodecs", 
            "text": "Partitioning is a mechanism for load balancing; it involves replicating one or more\noperators so that the load can be shared by all the replicas. Partitioning is\naccomplished by the  definePartitions  method of the  Partitioner  interface.\nA couple of implementations are available:  StatelessPartitioner  (static partitioning)\nand  StatelessThroughputBasedPartitioner  (dynamic partitioning, see below).\nThese can be used by setting the  PARTITIONER \nattribute on the operator by including a stanza like the following in your\nconfiguration file (where  {appName}  and  {opName}  are the\nappropriate application and operator names):  property \n   name dt.application.{appName}.operator.{opName}.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   The number after the colon specifies the number of desired partitions. This can be\ndone for any operator that is not a connector (i.e. input or output operator) and is not\nannotated with  @OperatorAnnotation(partitionable = false) . No code changes are necessary.\nIncoming tuples entering input ports of the operator are automatically distributed among the\npartitions based on their hash code by default. You can get greater control of how tuples\nare distributed to partitions by using a  StreamCodec ; further discussion of stream\ncodecs is deferred to the Advanced Guide [ coming soon ].\nA small sample program illustrating use of stream codecs is here .  Connectors need special care since they interact with external systems. Many connectors\n(e.g. Kafka input, file input and output operators) implement the  Partitioner \ninterface and support partitioning using custom implementations of  definePartitions  within  the operator.\nDocumentation and/or source code of the individual connectors should be consulted for\ndetails.  Sometimes there is need to replicate an entire linear segment of the DAG; this is\nknown as  Parallel Partitioning  and is achieved by setting the  PARTITION_PARALLEL \nattribute on the input port of each downstream operator that is part of the linear\nsegment. Both of these mechanisms are described in greater detail in the Advanced Features  section of the Top N Words tutorial  As mentioned above, the  StatelessPartitioner  is used for  static \npartitioning since it occurs once before the application starts. Dynamic partitioning\nwhile the application is running is also possible using the StatelessThroughputBasedPartitioner  or a custom partitioner. Implementing such\na partitioner needs special care, especially if the operator to be partitioned\nhas some accumulated state since this state typically needs to be redistributed among\nthe newly created partitions. An example of a custom partitioner that does dynamic\npartitioning is  here .  Unifiers are the flip side of the partitioning coin: When data that was intended to be\nprocessed by a single instance is now processed by multiple partitions, each instance\ncomputes partial results since it processes only part of the stream; these partial\nresults need to be combined to form the final result; this is the function of a unifier.  For example, suppose an operator is processing numbers and computes the sum of all the\nvalues seen in a window. If it is partitioned into N replicas, each replica is computing\na partial sum and we would need a unifier that computes the overall sum from\nthese N partial sums. A sample application that shows how to define and use a unifier is\navailable  here .  A unifier for an operator is provided by a suitable override of the  getUnifier()  method\nof the output port, for example:    public final transient DefaultOutputPort HighLow Integer  out\n    = new DefaultOutputPort HighLow Integer () {\n    @Override\n    public Unifier HighLow Integer  getUnifier() {\n      return new UnifierRange Integer ()\n    }\n  };  If no unifier is supplied for a partitioned operator, the platform will supply a default\npass-through unifier.  When the number of partitions is large and the unifier involves non-trivial computations\nthere is a risk that it can become a bottleneck; in such cases, the  UNIFIER_LIMIT \nattribute can be set on the appropriate output port. The platform will then automatically\ngenerate the required number of parallel unifiers, cascading into multiple levels if\nnecessary, to ensure that the number of input streams at each unifier does not exceed\nthis limit.", 
            "title": "Partitioners, Unifiers and StreamCodecs"
        }, 
        {
            "location": "/beginner/#buffer-server", 
            "text": "The  Buffer Server  is a separate service within a container which implements\na publish-subscribe model. It is present whenever the container hosts an operator\nwith an output port connected to another operator outside the container.  The output port is the publisher and the connected input ports of downstream operators\nare the subscribers. It buffers tuples so that they can be replayed when a\ndownstream operator fails and is restarted. As described earlier, the memory allocated\nto a buffer server is user configurable via an attribute named  BUFFER_MEMORY_MB  and\ndefaults to 512MB.  The total memory required by a container that hosts many such operators may climb rapidly;\nreducing the value of this attribute is advisable in such cases in a memory constrained\nenvironment.", 
            "title": "Buffer Server"
        }, 
        {
            "location": "/beginner/#allocating-operator-memory", 
            "text": "A container is a JVM process; the maximum memory each such container can consume is\nuser configurable via the  MEMORY_MB  attribute whose default value is 1GB.\nIf it is too large, container allocation may fail before the application even\nbegins; if it is too small, the application may fail at run time as it tries to\nallocate memory and runs up against this limit.\nAn example of setting this limit:  property \n   name dt.application.MyFirstApplication.operator.randomGenerator.attr.MEMORY_MB /name \n   value 300 /value  /property   As before, wildcards can be used to set the same value for all operators by replacing\nthe operator name  randomGenerator  with an asterisk. The application name can also be\nomitted, so a shorter version is:  property \n   name dt.operator.*.attr.MEMORY_MB /name \n   value 300 /value  /property   The Application Master (aka  StrAM ) is a supervisory Java process associated with each\napplication; memory allocation for it is specified slightly differently:  property \n   name dt.attr.MASTER_MEMORY_MB /name \n   value 700 /value  /property   For small applications, a value as low as 700 may be adequate; for larger applications,\na value 2000 or more may be needed. If this value is too small, the application typically\nfails at startup with no user-visible diagnostics; YARN logs need to be examined in such\ncases.", 
            "title": "Allocating Operator Memory"
        }, 
        {
            "location": "/beginner/#sample-applications", 
            "text": "This section briefly discusses some sample applications using commonly used connectors.\nThe applications themselves are available at Examples .\nWe briefly describe a few of them below.  Each example has a brief  README.md  file (in markdown format) describing what the\napplication does. In most cases, the unit tests function as full application tests that\ncan be run locally in your development environment without the need for a cluster as\ndescribed above.", 
            "title": "Sample Applications"
        }, 
        {
            "location": "/beginner/#application-file-copy", 
            "text": "The  fileIO-simple  application copies all data verbatim from files added to an input\ndirectory to rolling files in an output directory. The input and output directories,\nthe output file base name and maximum size are configured in  META_INF/properties.xml :  property \n   name dt.application.SimpleFileIO.operator.input.prop.directory /name \n   value /tmp/SimpleFileIO/input-dir /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.filePath /name \n   value /tmp/SimpleFileIO/output-dir /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.fileName /name \n   value myfile /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.maxLength /name \n   value 1000000 /value  /property   The application DAG is created in  Application.java :  @Override\npublic void populateDAG(DAG dag, Configuration conf)\n{\n  // create operators\n  FileLineInputOperator in = dag.addOperator( input , new FileLineInputOperator());\n  FileOutputOperator out = dag.addOperator( output , new FileOutputOperator());\n\n  // create streams\n  dag.addStream( data , in.output, out.input);\n}  The  FileLineInputOperator  is part of Malhar and is a concrete class that extends AbstractFileInputOperator . The  FileOutputOperator  is defined locally\nand extends the  AbstractFileOutputOperator  and overrides 3 methods:\n+  getFileName  which simply returns the current file name\n+  getBytesForTuple  which appends a newline to the argument string, converts it to an array of bytes and returns it.\n+  setup  which creates the actual file name by appending the operator id to the configured base name (this last step is necessary when partitioning is involved to ensure that multiple partitions do not write to the same file).  Output files are created with temporary names like  myfile_p2.0.1465929407447.tmp  and\nrenamed to  myfile_p2.0  when they reach the maximum configured size.", 
            "title": "Application &mdash; file copy"
        }, 
        {
            "location": "/beginner/#application-database-to-file", 
            "text": "The  jdbcIngest  application reads rows from a table in  MySQL , creates Java objects\n(_POJO_s) and writes them to a file in the user specified directory in HDFS.  Application configuration values are specified in 2 files:  META_INF/properties.xml  and  src/site/conf/example.xml . The former uses the in-memory\ndatabase  HSQLDB  and is used by the unit test in JdbcInputAppTest; this test can be\nrun, as described earlier, either in your IDE or using maven on the command line.\nThe latter uses  MySql  and is intended for use on a cluster. To run on a cluster you'll\nneed a couple of preparatory steps:   Make sure  MySql  is installed on the cluster.  Change  example.xml  to reflect proper values for  databaseUrl ,  userName ,  password  and  filePath .  Create the required table and rows by running the SQL queries in the file  src/test/resources/example.sql .  Create the HDFS output directory if necessary.  Build the project to create the  .apa  package  Launch the application, selecting  example.xml  as the configuration file during launch.  Verify that the expected output file is present.   Further details on these steps are in the project  README.md  file.  The application uses two operators: The first is  FileLineOutputOperator  which extends AbstractFileOutputOperator  and provides implementations for two methods: getFileName  and  getBytesForTuple . The former creates a file name using the operator\nid   this is important if this operator has multiple partitions and prevents the\npartitions from writing to the same file (which could cause garbled data). The latter\nsimply converts the incoming object to an array of bytes and returns it.  The second is  JdbcPOJOInputOperator  which comes from Malhar; it reads records from\na table and outputs them on the output port; they type of object that is emitted is\nspecified by the value of the   TUPLE_CLASS  attribute in the configuration file\nnamely  PojoEvent  in this case. This operator also needs a couple of additional\nproperties: (a) a list of  FieldInfo  objects that describe the mapping from table\ncolumns to fields of the Pojo; and (b) a  store  object that deals with the details\nof establishing a connection to the database server.  The application itself is then created in the usual way in  JdbcHDFSApp.populateDAG :  JdbcPOJOInputOperator jdbcInputOperator = dag.addOperator( JdbcInput , new JdbcPOJOInputOperator());\njdbcInputOperator.setFieldInfos(addFieldInfos());\n\njdbcInputOperator.setStore(new JdbcStore());\n\nFileLineOutputOperator fileOutput = dag.addOperator( FileOutputOperator , new FileLineOutputOperator());\n\ndag.addStream( POJO's , jdbcInputOperator.outputPort, fileOutput.input).setLocality(Locality.CONTAINER_LOCAL);", 
            "title": "Application &mdash; database to file"
        }, 
        {
            "location": "/beginner/#apphub", 
            "text": "AppHub  is a source of application templates. There are many more applications for various use cases to help jump start development effort.", 
            "title": "AppHub"
        }, 
        {
            "location": "/beginner/#additional-resources", 
            "text": "Development environment setup    Troubleshooting guide", 
            "title": "Additional Resources"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nThe following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance. \n\n\nApplication Builder\n\n\n\n\n\nDimensional Computing\n\n\n\n\n\nElastic Scalability\n\n\n\n\n\nFault Tolerance", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "The following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance.", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demo_videos/#application-builder", 
            "text": "", 
            "title": "Application Builder"
        }, 
        {
            "location": "/demo_videos/#dimensional-computing", 
            "text": "", 
            "title": "Dimensional Computing"
        }, 
        {
            "location": "/demo_videos/#elastic-scalability", 
            "text": "", 
            "title": "Elastic Scalability"
        }, 
        {
            "location": "/demo_videos/#fault-tolerance", 
            "text": "", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/tutorials/topnwords/", 
            "text": "Top N Words Application\n\n\nThe Top N words application is a tutorial on building a word counting application using:\n\n\n\n\nApache Apex platform\n\n\nApache Apex Malhar, an associated library of operators\n\n\nOther related tools\n\n\n\n\nNote: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.\n\n\nThe Top N Words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words and computes word frequencies for both that specific file\nas well as across all files processed so far. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on \ndtDashboard\n, the browser-based dashboard of DataTorrent RTS.\n\n\nA simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:\n\n\n\n\nThe Apex platform\n\n\nThe operator library\n\n\nThe tools required for developing and deploying\n    applications on a cluster\n\n\napex\n \n the command-line tool for managing\n    application packages and the constituent applications\n\n\ndtManage\n \n for monitoring the applications\n\n\ndtDashboard\n \n for visualizing the output\n\n\ndtAssemble\n \n for  visual application assembly\n\n\n\n\nIn the context of such an application, a number of questions arise:\n\n\n\n\nWhat operators do we need ?\n\n\nHow many are present in the Malhar library ?\n\n\nHow many need to be written from scratch ?\n\n\nHow are operators wired together ?\n\n\nHow do we monitor the running application ?\n\n\nHow do we display the output data in an aesthetically pleasing way ?\n\n\n\n\nThe answers to these and other questions are explored in the sections below.\n\n\nFor this tutorial, use the DataTorrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and the latest version of DataTorrent RTS configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/topnwords/#top-n-words-application", 
            "text": "The Top N words application is a tutorial on building a word counting application using:   Apache Apex platform  Apache Apex Malhar, an associated library of operators  Other related tools   Note: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.  The Top N Words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words and computes word frequencies for both that specific file\nas well as across all files processed so far. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on  dtDashboard , the browser-based dashboard of DataTorrent RTS.  A simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:   The Apex platform  The operator library  The tools required for developing and deploying\n    applications on a cluster  apex    the command-line tool for managing\n    application packages and the constituent applications  dtManage    for monitoring the applications  dtDashboard    for visualizing the output  dtAssemble    for  visual application assembly   In the context of such an application, a number of questions arise:   What operators do we need ?  How many are present in the Malhar library ?  How many need to be written from scratch ?  How are operators wired together ?  How do we monitor the running application ?  How do we display the output data in an aesthetically pleasing way ?   The answers to these and other questions are explored in the sections below.  For this tutorial, use the DataTorrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and the latest version of DataTorrent RTS configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Top N Words Application"
        }, 
        {
            "location": "/tutorials/topnwords-c1/", 
            "text": "Setting up your development environment\n\n\nTo begin with, please follow the steps outlined in:\n\nApache Apex Development Environment Setup\n\nto setup your development environment; you can skip the sandbox download\nand installation if you already have a Hadoop cluster with Datatorrent\nRTS installed where you can deploy applications.\n\n\nSample input files\n\n\nFor this tutorial, you need some sample text files to use as input to the application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  \nWqgi\n).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  \ndiv\n, \ntd\n and \np\n dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:\n\n\nOpen a terminal and run the following commands to create a directory named\n\ndata\n under your home directory and download 3 files there:\n\n\ncd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Development Environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#setting-up-your-development-environment", 
            "text": "To begin with, please follow the steps outlined in: Apache Apex Development Environment Setup \nto setup your development environment; you can skip the sandbox download\nand installation if you already have a Hadoop cluster with Datatorrent\nRTS installed where you can deploy applications.", 
            "title": "Setting up your development environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#sample-input-files", 
            "text": "For this tutorial, you need some sample text files to use as input to the application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,   Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as   div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:  Open a terminal and run the following commands to create a directory named data  under your home directory and download 3 files there:  cd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Sample input files"
        }, 
        {
            "location": "/tutorials/topnwords-c2/", 
            "text": "Building top N words using JAVA\n\n\nThis chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on a cluster (or sandbox). We will use the \ndtManage\n GUI tool to launch the\napplication.\n\n\nStep I: Clone the Apex Malhar repository\n\n\nClone the Malhar repository (we will use some of these source files in a later\nsection):\n\n\n\n\n\n\nOpen a terminal window and create a new directory where you want the code\n    to reside, for example: \ncd ~/src; mkdir dt; cd dt\n\n\n\n\n\n\nDownload the code for Malhar:\n\n\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\nYou should now see a directory named \nincubator-apex-malhar\n.\n\n\n\n\n\n\nStep II: Create a new application project\n\n\nCreate a new application project as described in (you can use either an IDE\nor the command line):\n\nApache Apex Development Environment Setup\n\n\nStep III: Copy application files to the new project\n\n\nWe will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.\n\n\n\n\nDelete files \nApplication.java\n and \nRandomNumberGenerator.java\n\n   under \nsrc/main/java/com/example/topnwordcount\n.\n\n\nDelete file \nApplicationTest.java\n file under\n   \nsrc/test/java/com/example/topnwordcount\n.\n\n\n\n\nCopy the following files from:\n\n\nincubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/\n\n\nto\n\n\nsrc/main/java/com/example/topnwordcount\n\n\n\n\nApplicationWithQuerySupport.java\n\n\nFileWordCount.java\n\n\nLineReader.java\n\n\nWCPair.java\n\n\nWindowWordCount.java\n\n\nWordCountWriter.java\n\n\nWordReader.java\n\n\n\n\n\n\n\n\nCopy the file \nWordDataSchema.json\n from\n\n\nincubator-apex-malhar/demos/wordcount/src/main/resources/\n\n\nto\n\n\nsrc/main/resources/\n\n\nin the new project.\n\n\nNote\n: This file defines the format of data sent to the visualization widgets within \ndtDashboard\n.\n\n\n\n\n\n\nStep IV: Configure the application and operators\n\n\nNext, we need to configure application properties. These\nproperties accomplish the following aims:\n\n\n\n\nLimit the amount of memory used by most operators so that more memory can\n  be allocated for \nfileWordCount\n which maintains the frequency counts.\n\n\nSet the locality of a couple of streams to \nCONTAINER_LOCAL\n to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).\n\n\nDefine the regular expression for matching the non-word string that\n  delimits words.\n\n\nDefine number of top (word, frequency) pairs we want output.\n\n\nDefine the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.\n\n\nDefine the topics for sending queries and retrieving data for visualization.\n\n\n\n\nTo do this:\n\n\nOpen the file \nsrc/main/resources/META-INF/properties.xml\n, and replace its\ncontent with the following:\n\n\nconfiguration\n\n \nproperty\n\n   \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n   \nvalue\n500\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.*.operator.*.attr.MEMORY_MB\n/name\n\n   \nvalue\n200\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB\n/name\n\n   \nvalue\n512\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.lineReader.directory\n/name\n\n   \nvalue\n/tmp/test/input-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n/name\n\n   \nvalue\n[\\p{Punct}\\s]+\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wcWriter.filePath\n/name\n\n   \nvalue\n/tmp/test/output-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n/name\n\n   \nvalue\n10\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryFileStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFile\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFileResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobal\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobalResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TwitterDemo.operator.wsResult.numRetries\n/name\n\n   \nvalue\n2147483647\n/value\n\n \n/property\n\n\n/configuration\n\n\n\n\n\nNote\n:\nThe package name within the Java files we just copied currently reflects the\npackage from which they were copied. This may be flagged as an error by your IDE\nbut the application should build with no errors when built with maven on the\ncommand line. You can fix the errors in the IDE by changing the relevant line\nwithin each file from:\n\n\npackage com.datatorrent.demos.wordcount;\n\n\n\nto reflect the current location of the file, for example:\n\n\npackage com.example.topnwordcount;\n\n\n\nStep V: Build the top N words count application\n\n\nFrom your IDE build the application in the usual way\n\n\nFrom the command line build it with:\n\n\ncd topNwordcount; mvn clean package -DskipTests\n\n\n\nIn either case, if the build is successful, it should have created the\napplication package file\n\ntopNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa\n.\n\n\nStep VI: Upload the top N words application package\n\n\nTo upload the top N words application package\n\n\n\n\nLog on to the DataTorrent Console using the default username and password\n   (both are \ndtadmin\n).\n\n\nOn the top navigation bar, click \nDevelop\n.\n\n\nClick \nApplication Packages\n.\n\n\nUnder \nApplications\n, click the \nupload package\n button.\n  \n\n\nNavigate to the location of the \ntopNwordcount-1.0-SNAPSHOT.apa\n\n   application package file is stored.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep VII: Launch the top N words application\n\n\nNote\n: If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nIn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the top N word count application, and click\n   \nLaunch Application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n. To specify individual properties, select \nSpecify\n    custom properties\n.\n\n\nClick Launch.\n\n\n\n\nA message indicating success of the launch operation should appear along with\nthe application ID.\n\n\nNote\n: After a successful launch, monitor the top N words application following\ninstructions in the chapter \nMonitoring with dtManage\n.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#building-top-n-words-using-java", 
            "text": "This chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on a cluster (or sandbox). We will use the  dtManage  GUI tool to launch the\napplication.", 
            "title": "Building top N words using JAVA"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-i-clone-the-apex-malhar-repository", 
            "text": "Clone the Malhar repository (we will use some of these source files in a later\nsection):    Open a terminal window and create a new directory where you want the code\n    to reside, for example:  cd ~/src; mkdir dt; cd dt    Download the code for Malhar:  git clone https://github.com/apache/incubator-apex-malhar  You should now see a directory named  incubator-apex-malhar .", 
            "title": "Step I: Clone the Apex Malhar repository"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-create-a-new-application-project", 
            "text": "Create a new application project as described in (you can use either an IDE\nor the command line): Apache Apex Development Environment Setup", 
            "title": "Step II: Create a new application project"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iii-copy-application-files-to-the-new-project", 
            "text": "We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.   Delete files  Application.java  and  RandomNumberGenerator.java \n   under  src/main/java/com/example/topnwordcount .  Delete file  ApplicationTest.java  file under\n    src/test/java/com/example/topnwordcount .   Copy the following files from:  incubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/  to  src/main/java/com/example/topnwordcount   ApplicationWithQuerySupport.java  FileWordCount.java  LineReader.java  WCPair.java  WindowWordCount.java  WordCountWriter.java  WordReader.java     Copy the file  WordDataSchema.json  from  incubator-apex-malhar/demos/wordcount/src/main/resources/  to  src/main/resources/  in the new project.  Note : This file defines the format of data sent to the visualization widgets within  dtDashboard .", 
            "title": "Step III: Copy application files to the new project"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iv-configure-the-application-and-operators", 
            "text": "Next, we need to configure application properties. These\nproperties accomplish the following aims:   Limit the amount of memory used by most operators so that more memory can\n  be allocated for  fileWordCount  which maintains the frequency counts.  Set the locality of a couple of streams to  CONTAINER_LOCAL  to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).  Define the regular expression for matching the non-word string that\n  delimits words.  Define number of top (word, frequency) pairs we want output.  Define the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.  Define the topics for sending queries and retrieving data for visualization.   To do this:  Open the file  src/main/resources/META-INF/properties.xml , and replace its\ncontent with the following:  configuration \n  property \n    name dt.attr.MASTER_MEMORY_MB /name \n    value 500 /value \n  /property   property \n    name dt.application.*.operator.*.attr.MEMORY_MB /name \n    value 200 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB /name \n    value 512 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.lineReader.directory /name \n    value /tmp/test/input-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr /name \n    value [\\p{Punct}\\s]+ /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wcWriter.filePath /name \n    value /tmp/test/output-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.topN /name \n    value 10 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryFileStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryFile.topic /name \n    value TopNWordsQueryFile /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultFile.topic /name \n    value TopNWordsQueryFileResult /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryGlobal.topic /name \n    value TopNWordsQueryGlobal /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic /name \n    value TopNWordsQueryGlobalResult /value \n  /property   property \n    name dt.application.TwitterDemo.operator.wsResult.numRetries /name \n    value 2147483647 /value \n  /property  /configuration   Note :\nThe package name within the Java files we just copied currently reflects the\npackage from which they were copied. This may be flagged as an error by your IDE\nbut the application should build with no errors when built with maven on the\ncommand line. You can fix the errors in the IDE by changing the relevant line\nwithin each file from:  package com.datatorrent.demos.wordcount;  to reflect the current location of the file, for example:  package com.example.topnwordcount;", 
            "title": "Step IV: Configure the application and operators"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-v-build-the-top-n-words-count-application", 
            "text": "From your IDE build the application in the usual way  From the command line build it with:  cd topNwordcount; mvn clean package -DskipTests  In either case, if the build is successful, it should have created the\napplication package file topNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa .", 
            "title": "Step V: Build the top N words count application"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vi-upload-the-top-n-words-application-package", 
            "text": "To upload the top N words application package   Log on to the DataTorrent Console using the default username and password\n   (both are  dtadmin ).  On the top navigation bar, click  Develop .  Click  Application Packages .  Under  Applications , click the  upload package  button.\n    Navigate to the location of the  topNwordcount-1.0-SNAPSHOT.apa \n   application package file is stored.  Wait till the package is successfully uploaded.", 
            "title": "Step VI: Upload the top N words application package"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vii-launch-the-top-n-words-application", 
            "text": "Note : If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.   Log on to the DataTorrent Console (the default username and password are\n   both  dtadmin ).  In the top navigation bar, click  Develop .  Under  App Packages , locate the top N word count application, and click\n    Launch Application .  (Optional) To configure the application using a configuration file, select\n     Use a config file . To specify individual properties, select  Specify\n    custom properties .  Click Launch.   A message indicating success of the launch operation should appear along with\nthe application ID.  Note : After a successful launch, monitor the top N words application following\ninstructions in the chapter  Monitoring with dtManage .", 
            "title": "Step VII: Launch the top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c3/", 
            "text": "Building top N words using dtAssemble\n\n\nYou can build top N words using dtAssemble \n the graphical drag-and-drop\napplication builder.\n\n\nNote\n: This tool is not available with the Community edition license.\n\n\nUsing the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.\n\n\nNote\n: You cannot undo the auto-layout or zoom-to-fit operations.\n\n\nPrerequisites\n\n\nTo use \ndtAssemble\n, you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.\n\n\nTo ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the \nMonitor\n tab, and under \nDataTorrent Applications\n, kill any\nrunning applications.\n\n\nStep I: Create top N words using dtAssemble\n\n\n\n\nLog on to the DataTorrent RTS console (default username and password are\n   both \ndtadmin\n).\n\n\nOn the DataTorrent RTS console, click \nDevelop\n \n \nApp Packages\n.\n\n\nMake sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.\n\n\nClick \nTopNWordCount\n in the name column to see the application details.\n  \n\n\nClick \ncreate new application\n button.\n  \n\n\nType a name for your application, for example, \nTopNWordsJson\n, and click\n   \nCreate\n. The \nApplication Canvas\n should open.\n  \n\n\nClick the breadcrumbs link to the Application page, in this case, \nTopNWordsJson\n.\n\n\n\n\nThe existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For JSON applications, there is an\nextra \nedit\n button on the application page which brings you to dtAssemble.\nThe \nedit\n option is not available for JAVA applications.\n\n\n\n\n\n\nClick the \nedit\n button to go back to dtAssemble.\n\n\n\n\nStep II: Drag operators to the application canvas\n\n\n\n\nWait till the Application Canvas opens.\n  \n\n\nFrom the Operator Library list on the left, locate the desired\n   operators by either:\n\n\n\n\nExploring the categories.\n\n\n-or-\n\n\n\n\n\n\nUsing the \nquick find\n feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type \nfile\n  into the search box to see a list of\n   matching operators. For example, the first operator is \nLineReader\n.\n    \n\n\n\n\nDrag the operator onto the canvas.\n\n\nRepeat this process for all the operators described in Appendix entitled\n    \nOperators in Top N words application\n, and arrange them on the canvas.\n\n\nTo magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.\n\n\n\n\nStep III: Connect the operators\n\n\nObserve that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.\n\n\nConnect the operators as shown in the diagram below. Note the following points about\nthe \nFileWordCount\n operator which has the largest number of connections:\n\n\n\n\nThe control port is connected to the control port of \nLineReader\n.\n\n\nThe input port is connected to the output port of \nWindowWordCount\n.\n\n\nThe \nfileOutput\n port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of \nWordCountWriter\n.\n\n\nThe \noutputGlobal\n port emits the global top N pairs and so is connected to\n  the input port of \nAppDataSnapshotMapServer\n.\n\n\nThe \noutputPerFile\n port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to \nConsoleOutput\n as\n  well as to \nAppDataSnapshotMapServer\n.\n\n\n\n\nNote\n: As you make changes, you may wish to save your progress by clicking\nthe \nsave\n button on the top right.\n\n\nAfter you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.\n\n\n\n\nStep IV: Configure the operator properties\n\n\nThe last step before running this application is to configure\nproperties of the operators.\n\n\n\n\nClick the first operator (\nLineReader\n) in the canvas to see the list of\n   configurable properties in the right panel.\n\n\n\n\nLocate the property labelled \nDirectory\n and enter the path to the input\n   directory: \n/tmp/test/input-dir\n:\n\n\n\n\n\n\n\n\nConfigure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n5\n\n\nFile Path\n\n\n/tmp/test/output-dir\n\n\n\n\n\n\n2\n\n\nNon Word Str\n\n\n[\\p{Punct}\\s]+\n\n\n\n\n\n\n9\n\n\nTopic\n\n\nTopNWordsQueryFile\n\n\n\n\n\n\n10\n\n\nTopic\n\n\nTopNWordsQueryGlobal\n\n\n\n\n\n\n11\n\n\nTopic\n\n\nTopNWordsQueryFileResult\n\n\n\n\n\n\n12\n\n\nTopic\n\n\nTopNWordsQueryGlobalResult\n\n\n\n\n\n\n7, 8\n\n\nSnapshot Schema JSON\n\n\n{ \nvalues\n: [{\nname\n: \nword\n, \ntype\n: \nstring\n},\n\n\n{\nname\n: \ncount\n, \ntype\n: \ninteger\n}] }\n\n\n\n\n\n\n4\n\n\nTop N\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\nClick Stream 8 and Stream 9, and change \nStream Locality\n from\n  \nAUTOMATIC\n to \nCONTAINER_LOCAL\n to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.\n\n\n\n\n\n\nClick the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to \nMaster Memory Mb\n, and\n   change its value to 500.\n\n\n\n\n\n\nClick each operator, navigate to the \nMemory Mb\n attribute in the\n   \nAttributes\n section, and change the value to 200 except for \nOperator\n    4\n for which the value is 512.\n\n\n\n\n\n\nClick \nsave\n in the top-right corner. You can click the \nActivity\n tab in the\n   Inspector panel on the bottom to monitor the validation and save process.\n\n\n\n\n\n\nClick \nlaunch\n in the top-right corner. \nNote\n: Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n  \n\n\n\n\n\n\nOn the launch application dialog window, type a name for your application.\n  \n\n\n\n\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n checkbox. To specify individual properties, select\n    \nSpecify custom properties\n checkbox.\n\n\n\n\nClick \nLaunch\n.\n\n\n\n\nA transient pop-up at the top-right indicating that the launch was successful\nshould appear.\n\n\n\nAfter a successful launch, monitor the application following\ninstructions in the Chapter entitled \nMonitoring with dtManage\n.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#building-top-n-words-using-dtassemble", 
            "text": "You can build top N words using dtAssemble   the graphical drag-and-drop\napplication builder.  Note : This tool is not available with the Community edition license.  Using the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.  Note : You cannot undo the auto-layout or zoom-to-fit operations.  Prerequisites  To use  dtAssemble , you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.  To ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the  Monitor  tab, and under  DataTorrent Applications , kill any\nrunning applications.", 
            "title": "Building top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-i-create-top-n-words-using-dtassemble", 
            "text": "Log on to the DataTorrent RTS console (default username and password are\n   both  dtadmin ).  On the DataTorrent RTS console, click  Develop     App Packages .  Make sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.  Click  TopNWordCount  in the name column to see the application details.\n    Click  create new application  button.\n    Type a name for your application, for example,  TopNWordsJson , and click\n    Create . The  Application Canvas  should open.\n    Click the breadcrumbs link to the Application page, in this case,  TopNWordsJson .   The existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For JSON applications, there is an\nextra  edit  button on the application page which brings you to dtAssemble.\nThe  edit  option is not available for JAVA applications.    Click the  edit  button to go back to dtAssemble.", 
            "title": "Step I: Create top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-ii-drag-operators-to-the-application-canvas", 
            "text": "Wait till the Application Canvas opens.\n    From the Operator Library list on the left, locate the desired\n   operators by either:   Exploring the categories.  -or-    Using the  quick find  feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type  file   into the search box to see a list of\n   matching operators. For example, the first operator is  LineReader .\n       Drag the operator onto the canvas.  Repeat this process for all the operators described in Appendix entitled\n     Operators in Top N words application , and arrange them on the canvas.  To magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.", 
            "title": "Step II: Drag operators to the application canvas"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iii-connect-the-operators", 
            "text": "Observe that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.  Connect the operators as shown in the diagram below. Note the following points about\nthe  FileWordCount  operator which has the largest number of connections:   The control port is connected to the control port of  LineReader .  The input port is connected to the output port of  WindowWordCount .  The  fileOutput  port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of  WordCountWriter .  The  outputGlobal  port emits the global top N pairs and so is connected to\n  the input port of  AppDataSnapshotMapServer .  The  outputPerFile  port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to  ConsoleOutput  as\n  well as to  AppDataSnapshotMapServer .   Note : As you make changes, you may wish to save your progress by clicking\nthe  save  button on the top right.  After you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.", 
            "title": "Step III: Connect the operators"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iv-configure-the-operator-properties", 
            "text": "The last step before running this application is to configure\nproperties of the operators.   Click the first operator ( LineReader ) in the canvas to see the list of\n   configurable properties in the right panel.   Locate the property labelled  Directory  and enter the path to the input\n   directory:  /tmp/test/input-dir :     Configure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.          Operator  Property Name  Value    5  File Path  /tmp/test/output-dir    2  Non Word Str  [\\p{Punct}\\s]+    9  Topic  TopNWordsQueryFile    10  Topic  TopNWordsQueryGlobal    11  Topic  TopNWordsQueryFileResult    12  Topic  TopNWordsQueryGlobalResult    7, 8  Snapshot Schema JSON  {  values : [{ name :  word ,  type :  string },  { name :  count ,  type :  integer }] }    4  Top N  10       Click Stream 8 and Stream 9, and change  Stream Locality  from\n   AUTOMATIC  to  CONTAINER_LOCAL  to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.    Click the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to  Master Memory Mb , and\n   change its value to 500.    Click each operator, navigate to the  Memory Mb  attribute in the\n    Attributes  section, and change the value to 200 except for  Operator\n    4  for which the value is 512.    Click  save  in the top-right corner. You can click the  Activity  tab in the\n   Inspector panel on the bottom to monitor the validation and save process.    Click  launch  in the top-right corner.  Note : Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n      On the launch application dialog window, type a name for your application.\n      (Optional) To configure the application using a configuration file, select\n     Use a config file  checkbox. To specify individual properties, select\n     Specify custom properties  checkbox.   Click  Launch .   A transient pop-up at the top-right indicating that the launch was successful\nshould appear.  After a successful launch, monitor the application following\ninstructions in the Chapter entitled  Monitoring with dtManage .", 
            "title": "Step IV: Configure the operator properties"
        }, 
        {
            "location": "/tutorials/topnwords-c4/", 
            "text": "Monitoring top N words using dtManage\n\n\ndtManage\n is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.\n\n\nMonitor the Application\n\n\nTo monitor the top N words application\n\n\n\n\nLog on to the Datatorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nUnder \nDatatorrent Applications\n, check if the application started.\n\n\nWait till the state entry changes to \nRUNNING\n.\n\n\nClick \nTopNWordsWithQueries\n to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and \nmetric-view\n.\n\n\nUnder \nStramEvents\n, ensure that all the operators have started.\n    \n\n\n\n\nDAGs and widgets\n\n\nWhen monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets: \nStram Events\n, \nApplication\nOverview\n, \nLogical DAG\n, \nLogical Operators\n, \nStreams\n, and \nMetrics Chart\n.\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).\n\n\nLogical view and associated widgets (panels)\n\n\nThis section describes the widgets that you see when you select the logical\ntab.\n\n\nStram Events\n\n\nAs shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled \ndetails\n\nappears next to the event; click on it for additional details about the\nfailure.\n\n\nApplication Overview\n\n\nThis panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The \nvisualize\n button allows you to\ncreate one or more custom dashboards to visualize the application output.\n\n\n\nLogical DAG\n\n\nThe logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.\n\n\nTo customize these properties\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator, click the Top list and\n   select a metric.\n\n\nTo display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.\n\n\n\n\n\n\nLogical Operators\n\n\nThis panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.\n\n\n\n\nStreams\n\n\nThis panel displays details of each stream such as the name, locality, source,\nand sinks.\n\n\n\n\nMetrics Chart\n\n\nThis panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:\n\n\n\n\nPhysical view and associated widgets\n\n\nThe physical tab displays the \nApplication Overview\n and \nMetrics Chart\n\ndiscussed above along with additional panels: \nPhysical Operators\n and\n\nContainers\n. The \nPhysical Operators\n table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.\n\n\nPhysical Operators\n\n\n\n\nContainers\n\n\nFor each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the \nContainers\n table.\n\n\n\n\nIf the state of all the physical operators and containers is \nACTIVE\n\nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.\n\n\nThe physical-dag-view\n\n\nThe \nphysical-dag-view\n tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.\n\n\nThe metric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nView application logs\n\n\nWhen debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines. \ndtManage\n simplifies\nthis task by making all relevant logs accessible from the console.\n\n\nFor example, to examine logs for the \nFileWordCount\n operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.\n\n\nThe numeric values in the \ncontainer\n column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the \nContainer Overview\n panel, you should see a blue \nlogs\n dropdown\nbutton; click on it to see a menu containing three entries: \ndt.log\n, \nstderr\n,\nand \nstdout\n.\n\n\n\n\nAll messages output using \nlog4j\n classes will appear in \ndt.log\n\nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitoring-top-n-words-using-dtmanage", 
            "text": "dtManage  is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.", 
            "title": "Monitoring top N words using dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitor-the-application", 
            "text": "To monitor the top N words application   Log on to the Datatorrent Console (the default username and password\n   are both  dtadmin ).  On the top navigation bar, click  Monitor .  Under  Datatorrent Applications , check if the application started.  Wait till the state entry changes to  RUNNING .  Click  TopNWordsWithQueries  to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and  metric-view .  Under  StramEvents , ensure that all the operators have started.", 
            "title": "Monitor the Application"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#dags-and-widgets", 
            "text": "When monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets:  Stram Events ,  Application\nOverview ,  Logical DAG ,  Logical Operators ,  Streams , and  Metrics Chart .\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).  Logical view and associated widgets (panels)  This section describes the widgets that you see when you select the logical\ntab.  Stram Events  As shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled  details \nappears next to the event; click on it for additional details about the\nfailure.  Application Overview  This panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The  visualize  button allows you to\ncreate one or more custom dashboards to visualize the application output.  Logical DAG  The logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.  To customize these properties   Click an operator for which you want to display additional details.  To display a detail on the top of this operator, click the Top list and\n   select a metric.  To display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.    Logical Operators  This panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.   Streams  This panel displays details of each stream such as the name, locality, source,\nand sinks.   Metrics Chart  This panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:   Physical view and associated widgets  The physical tab displays the  Application Overview  and  Metrics Chart \ndiscussed above along with additional panels:  Physical Operators  and Containers . The  Physical Operators  table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.  Physical Operators   Containers  For each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the  Containers  table.   If the state of all the physical operators and containers is  ACTIVE \nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.  The physical-dag-view  The  physical-dag-view  tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.  The metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "DAGs and widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#view-application-logs", 
            "text": "When debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines.  dtManage  simplifies\nthis task by making all relevant logs accessible from the console.  For example, to examine logs for the  FileWordCount  operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.  The numeric values in the  container  column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the  Container Overview  panel, you should see a blue  logs  dropdown\nbutton; click on it to see a menu containing three entries:  dt.log ,  stderr ,\nand  stdout .   All messages output using  log4j  classes will appear in  dt.log \nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "View application logs"
        }, 
        {
            "location": "/tutorials/topnwords-c5/", 
            "text": "Visualizing the application output using dtDashboard\n\n\nThis chapter covers how to add input files to the monitored input directory and\nvisualize the output.\n\n\nWhen adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled \nFurther Explorations\n.\n\n\n\n\nNote: If you are have trouble with any of the following steps, or have not\ncomplete the preceding sections in this tutorial, you can import \nWord Count Demo\n\nfrom AppHub. The \nWord Count Demo\n Application Package contains the\nTopNWordsWithQueries Application.\n\n\n\n\nStep I: Add files to the monitored directory\n\n\nTo add the files to the monitored input directory\n\n\n\n\nLog on to the Datatorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nClick TopNWordsWithQueries to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and  \nmetric-view\n.\n\n\nClick the \nlogical\n tab and make sure that the DAG is visible.\n\n\nCreate the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands:\nhdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir\n\n\n\n\n\n\n\nYou should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.\n\n\nYou can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:\n\n\nhdfs dfs -cat /tmp/test/output-dir/rfc4844.txt\n\n\n\nFor operating on these input and output directories, you may find the following\nshell aliases and functions useful:\n\n\nin=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}\n\n\n\nPut them in a file called, say, \naliases\n and read them into your shell with:\n\nsource aliases\n.\n\n\nThereafter, you can list contents of the input and output directories with\n\nls-input\n and \nls-output\n, remove all files from them with \nclean-input\n and\n\nclean-output\n, drop an input file \nfoo.txt\n into the input directory with\n\nput-file foo.txt\n and finally, retrieve the corresponding output file with\n\nget-file foo.txt\n.\n\n\nNote\n: When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.\n\n\nStep II: Visualize the results by generating dashboards\n\n\nTo generate dashboards\n\n\n\n\nPerform step I above.\n\n\nMake sure that the logical tab is selected and the \nApplication Overview\n\n  panel is visible.\n\n\n\n\nClick \nvisualize\n to see a dropdown containing previously created dashboards\n (if any), as well as the \ngenerate new dashboard\n entry.\n\n\n\n\n\n\nSelect the \ngenerate new dashboard\n entry.\n\n\nYou should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.\n\n\n\n\n\n\n\nAdd more files, one at a time, to the input directory as described in\n  step I above.\n\n\n\n\nObserve the charts changing to reflect the new data.\n\n\n\n\nYou can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.\n\n\nStep III: Add widgets\n\n\nTo derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets: \nbar chart\n,\n\npie chart\n, \nhorizontal bar chart\n, \ntable\n, and \nnote\n.\n\n\nTo add a widget\n\n\n\n\nGenerate a dashboard by following instructions of Step II above.\n\n\nClick the \nadd widget\n button below the name of the dashboard.\n\n\nIn the \nData Source\n list, select a data source for your widget.\n\n\n\n\nSelect a widget type under \nAvailable Widgets\n.\n\n\n\n\n\n\n\n\nClick \nadd widget\n.\n\n\n\n\n\n\nThe widget is added to your dashboard.\n\n\nStep IV: Configure a widget\n\n\nAfter you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo configure a widget\n\n\n\n\nTo change the size of the widget, click the border of the widget, and\n  resize it.\n\n\nTo move the widget around, click the widget, and drag it to the desired\n  position.\n\n\nTo change the title and other properties, click the \nedit\n button in the\n  top-right corner of the widget.\n    \n\n  You can now enter a new title in the \nTitle\n box or configure the rest of the\n  options in any suitable way.\n\n\nClick \nOK\n.\n\n\nTo remove a widget, click the delete (x) button in the top-right corner of\n  the widget.\n\n\n\n\nPerform additional tasks on dashboards\n\n\nAt any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.\n\n\nTo perform additional tasks\n\n\n\n\nEnsure that you generated a dashboard as described in Step II above and\n   select it.\n\n\nClick \nsettings\n button (next to buttons named \nadd widget\n,\n   \nauto generate\n, and \nsave settings\n), below the name of the dashboard to see the \nDashboard Settings\n dialog:\n    \n\n\nType a new name for the dashboard in the \nName of dashboard\n box.\n\n\nType a suitable description in the box below.\n\n\nMake sure that \nTopNWordsWithQueries\n is selected under \nChoose apps to\n    visualize\n.\n\n\nClick \nSave\n.\n\n\n\n\nDelete a dashboard\n\n\nYou can delete a dashboard at any time.\n\n\n\n\nLog on to the DataTorrent Console (default username and password are both\n  \ndtadmin\n)\n\n\nOn the top navigation bar, click \nVisualize\n.\n\n\n\n\nSelect a dashboard.\n\n\n\n\n\n\n\n\nClick delete.\n\n\n\n\n\n\n\n\nNote: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#visualizing-the-application-output-using-dtdashboard", 
            "text": "This chapter covers how to add input files to the monitored input directory and\nvisualize the output.  When adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled  Further Explorations .   Note: If you are have trouble with any of the following steps, or have not\ncomplete the preceding sections in this tutorial, you can import  Word Count Demo \nfrom AppHub. The  Word Count Demo  Application Package contains the\nTopNWordsWithQueries Application.", 
            "title": "Visualizing the application output using dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-i-add-files-to-the-monitored-directory", 
            "text": "To add the files to the monitored input directory   Log on to the Datatorrent Console (the default username and password are\n   both  dtadmin ).  On the top navigation bar, click  Monitor .  Click TopNWordsWithQueries to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and   metric-view .  Click the  logical  tab and make sure that the DAG is visible.  Create the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands: hdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir    You should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.  You can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:  hdfs dfs -cat /tmp/test/output-dir/rfc4844.txt  For operating on these input and output directories, you may find the following\nshell aliases and functions useful:  in=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}  Put them in a file called, say,  aliases  and read them into your shell with: source aliases .  Thereafter, you can list contents of the input and output directories with ls-input  and  ls-output , remove all files from them with  clean-input  and clean-output , drop an input file  foo.txt  into the input directory with put-file foo.txt  and finally, retrieve the corresponding output file with get-file foo.txt .  Note : When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.", 
            "title": "Step I: Add files to the monitored directory"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-ii-visualize-the-results-by-generating-dashboards", 
            "text": "To generate dashboards   Perform step I above.  Make sure that the logical tab is selected and the  Application Overview \n  panel is visible.   Click  visualize  to see a dropdown containing previously created dashboards\n (if any), as well as the  generate new dashboard  entry.    Select the  generate new dashboard  entry.  You should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.    Add more files, one at a time, to the input directory as described in\n  step I above.   Observe the charts changing to reflect the new data.   You can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.", 
            "title": "Step II: Visualize the results by generating dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iii-add-widgets", 
            "text": "To derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets:  bar chart , pie chart ,  horizontal bar chart ,  table , and  note .  To add a widget   Generate a dashboard by following instructions of Step II above.  Click the  add widget  button below the name of the dashboard.  In the  Data Source  list, select a data source for your widget.   Select a widget type under  Available Widgets .     Click  add widget .    The widget is added to your dashboard.", 
            "title": "Step III: Add widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iv-configure-a-widget", 
            "text": "After you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To configure a widget   To change the size of the widget, click the border of the widget, and\n  resize it.  To move the widget around, click the widget, and drag it to the desired\n  position.  To change the title and other properties, click the  edit  button in the\n  top-right corner of the widget.\n     \n  You can now enter a new title in the  Title  box or configure the rest of the\n  options in any suitable way.  Click  OK .  To remove a widget, click the delete (x) button in the top-right corner of\n  the widget.", 
            "title": "Step IV: Configure a widget"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#perform-additional-tasks-on-dashboards", 
            "text": "At any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.  To perform additional tasks   Ensure that you generated a dashboard as described in Step II above and\n   select it.  Click  settings  button (next to buttons named  add widget ,\n    auto generate , and  save settings ), below the name of the dashboard to see the  Dashboard Settings  dialog:\n      Type a new name for the dashboard in the  Name of dashboard  box.  Type a suitable description in the box below.  Make sure that  TopNWordsWithQueries  is selected under  Choose apps to\n    visualize .  Click  Save .", 
            "title": "Perform additional tasks on dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#delete-a-dashboard", 
            "text": "You can delete a dashboard at any time.   Log on to the DataTorrent Console (default username and password are both\n   dtadmin )  On the top navigation bar, click  Visualize .   Select a dashboard.     Click delete.     Note: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Delete a dashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c7/", 
            "text": "Top N Words (Advanced)\n\n\nThis section touches on some advanced features of the RTS platform in the context of the\n\nTop N Words\n application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.\n\n\n\n\n\nThe first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.\n\n\nManaging Memory Allocation for Containers\n\n\nIn this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.\n\n\nRecall the following facts from the earlier sections:\n\n\n\n\nA container (JVM process) can host multiple operators.\n\n\nThe memory requirements for an operator can be specified via a properties file.\n\n\n\n\nFor reference, here is the application DAG:\n\n\n\nIf we look at the information displayed in the physical tab of \ndtManage\n for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContainer Id\n\n\nAllocated Memory\n\n\nHosted Operators\n\n\n\n\n\n\n1\n\n\n1 GB\n\n\nNone (AppMaster)\n\n\n\n\n\n\n2\n\n\n768 MB\n\n\nsnapshotServerGlobal, QueryGlobal\n\n\n\n\n\n\n3\n\n\n768 MB\n\n\nsnapshotServerFile, QueryFile\n\n\n\n\n\n\n4\n\n\n128 MB\n\n\nwsResultGlobal\n\n\n\n\n\n\n5\n\n\n640 MB\n\n\nwindowWordCount\n\n\n\n\n\n\n6\n\n\n128 MB\n\n\nConsole\n\n\n\n\n\n\n7\n\n\n128 MB\n\n\nwsResultFile\n\n\n\n\n\n\n8\n\n\n640 MB\n\n\nwordReader\n\n\n\n\n\n\n9\n\n\n128 MB\n\n\nwcWriter\n\n\n\n\n\n\n10\n\n\n1.4 GB\n\n\nlineReader\n\n\n\n\n\n\n11\n\n\n1.9 GB\n\n\nfileWordCount\n\n\n\n\n\n\n\n\n\nIf we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the \nproperties.xml\n file for\n\ndt.attr.MASTER_MEMORY_MB\n. The discrepancy is due to the fact that the file\n\n.dt/dt-site.xml\n in the home directory of user \ndtadmin\n has a value of 1024 for\nthis key which overrides the application specified value.\n\n\nLooking now at container 2, we notice that it hosts 2 operators: \nsnapshotServerGlobal\n\nand \nQueryGlobal\n and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator\n\nconnected to another operator outside the container\n\nalso has an associated \nbuffer-server\n which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in\n\nApplication Development\n.\nThe space allocated to the buffer server is governed by properties of the form:\n\n\ndt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB\n\n\n\nwhere \napp-name\n, \nop-name\n and \nport-name\n can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard (\n*\n). The\ndefault value is 512 MB. Of the two operators, only one (\nsnapshotServerGlobal\n)\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).\n\n\nThe values for \nwsResultGlobal\n, \nConsole\n, \nwsResultFile\n, \nwcWriter\n are, as\nexpected, 128 MB \n since no output ports are involved, there is no buffer-server.\nThe values for \nwindowWordCount\n and \nwordReader\n are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor \nfileWordCount\n is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for \nlineReader\n can be computed\nsimilarly.\n\n\nThe total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say, \nlow-mem.xml\n at \nsrc/site/conf/\n with this content:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n    \nvalue\n512\n/value\n\n  \n/property\n \nproperty\n\n    \nname\ndt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n    \nvalue\n128\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nWe will use this file at launch time.\n\n\nThe BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).\n\n\nThe MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in \nMETA-INF/properties.xml\n\nis actually overridden by the setting of 1024 MB for this parameter in\n\n~dtadmin/.dt/dt-site.xml\n; however, if we use a launch-time configuration file,\nvalues in it override those in \ndt-site.xml\n. A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see \nDebugging\n\nsection below):\n\n\njava.lang.OutOfMemoryError: GC overhead limit exceeded.\n\n\n\nRebuild the application, upload the package and use this file at launch time:\n\n\n\nThe allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.\n\n\nDebugging\n\n\nOn the sandbox, various log files generated by YARN and Hadoop are located at\n\n/sfw/hadoop/shared/logs\n; the \nnodemanager\n directory has application specific\ndirectories with names like this: \napplication_1448033276100_0001\n within\nwhich there are container specific directories with names like\n\ncontainer_1448033276100_0001_01_000001\n. The App Master container has the \n000001\n\nsuffix and the corresponding directory will have these files:\n\n\nAppMaster.stderr  AppMaster.stdout  dt.log\n\n\n\nThe remaining container directories will have files:\n\n\ndt.log  stderr  stdout\n\n\n\nWhen problems occur, all these log files should be carefully examined. For example, the\n\ndt.log\n file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:\n\n\n2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=\n com.datatorrent.stram.engine.StreamingContainer 1\n/stdout 2\n/stderr\n\n\nYou can provide your own \nlog4j\n configuration file called, say, \nlog4j.properties\n and place\nit in the directory \nsrc/main/resources\n as described in the\n\nconfiguration\n\npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say \nINFO\n to \nDEBUG\n while the application is running, you can click on the blue\n\nset logging level\n button in the \nApplication Overview\n panel of \ndtManage\n. It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:\n\n\n\n\nNormally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the \nlogs\n dropdown but sometimes using the commandline\nfrom a terminal window may be easier.\n\n\nPartitioning\n\n\nPartitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.\n\n\nWithout partitioning, the DAG shown in the \nlogical\n and \nphysical-dag-view\n tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a \nunifier\n. The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.\n\n\nFor our word counting example, we illustrate the technique by partitioning the \nwordReader\n\noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe \nStatelessPartitioner\n which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the \nlow-mem.xml\n configuration file\nwe created above\nto a new file named \nsimple-partition.xml\n in the same directory and add this stanza to it:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\nWhen you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:\n\n\n\n\nNotice the two copies of \nwordReader\n and the generated unifier. The \nphysical\n tab will\nalso show the containers for these additional operators and their characteristics as well.\n\n\nA slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators: \nwordReader\n and the next operator\n\nwindowWordCount\n. To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nIt enables the PARTITION_PARALLEL attribute on the \ninput port\n of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:\n\n\n\n\nNotice that both operators have been replicated and the unifier added.\n\n\nStreaming Windows and Application Windows\n\n\nOperators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a \nstreaming window\n. Its boundaries are marked by calls to \nbeginWindow\n and\n\nendWindow\n within which the platform repeatedly invokes either \nemitTuples\n (for input\nadapters) or \nprocess\n on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the\n\nOperatorGuide\n.\n\n\nFor flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:\n\n\nproperty\n\n  \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n  \nvalue\n5000\n/value\n\n\n/property\n\n\n\n\nThis is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.\n\n\nA second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before, \napp-name\n and \nop-name\n should be\nreplaced by either wildcards or names of a specific application and/or operator):\n\n\nproperty\n\n  \nname\ndt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n5\n/value\n\n\n/property\n\n\n\n\nBy default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the \nbeginWindow\n and \nendWindow\n are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.", 
            "title": "Advanced Features"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#top-n-words-advanced", 
            "text": "This section touches on some advanced features of the RTS platform in the context of the Top N Words  application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.   The first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.", 
            "title": "Top N Words (Advanced)"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#managing-memory-allocation-for-containers", 
            "text": "In this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.  Recall the following facts from the earlier sections:   A container (JVM process) can host multiple operators.  The memory requirements for an operator can be specified via a properties file.   For reference, here is the application DAG:  If we look at the information displayed in the physical tab of  dtManage  for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):          Container Id  Allocated Memory  Hosted Operators    1  1 GB  None (AppMaster)    2  768 MB  snapshotServerGlobal, QueryGlobal    3  768 MB  snapshotServerFile, QueryFile    4  128 MB  wsResultGlobal    5  640 MB  windowWordCount    6  128 MB  Console    7  128 MB  wsResultFile    8  640 MB  wordReader    9  128 MB  wcWriter    10  1.4 GB  lineReader    11  1.9 GB  fileWordCount     If we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the  properties.xml  file for dt.attr.MASTER_MEMORY_MB . The discrepancy is due to the fact that the file .dt/dt-site.xml  in the home directory of user  dtadmin  has a value of 1024 for\nthis key which overrides the application specified value.  Looking now at container 2, we notice that it hosts 2 operators:  snapshotServerGlobal \nand  QueryGlobal  and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator connected to another operator outside the container \nalso has an associated  buffer-server  which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in Application Development .\nThe space allocated to the buffer server is governed by properties of the form:  dt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB  where  app-name ,  op-name  and  port-name  can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard ( * ). The\ndefault value is 512 MB. Of the two operators, only one ( snapshotServerGlobal )\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).  The values for  wsResultGlobal ,  Console ,  wsResultFile ,  wcWriter  are, as\nexpected, 128 MB   since no output ports are involved, there is no buffer-server.\nThe values for  windowWordCount  and  wordReader  are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor  fileWordCount  is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for  lineReader  can be computed\nsimilarly.  The total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say,  low-mem.xml  at  src/site/conf/  with this content:  configuration \n   property \n     name dt.attr.MASTER_MEMORY_MB /name \n     value 512 /value \n   /property   property \n     name dt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n     value 128 /value \n   /property  /configuration   We will use this file at launch time.  The BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).  The MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in  META-INF/properties.xml \nis actually overridden by the setting of 1024 MB for this parameter in ~dtadmin/.dt/dt-site.xml ; however, if we use a launch-time configuration file,\nvalues in it override those in  dt-site.xml . A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see  Debugging \nsection below):  java.lang.OutOfMemoryError: GC overhead limit exceeded.  Rebuild the application, upload the package and use this file at launch time:  The allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.", 
            "title": "Managing Memory Allocation for Containers"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#debugging", 
            "text": "On the sandbox, various log files generated by YARN and Hadoop are located at /sfw/hadoop/shared/logs ; the  nodemanager  directory has application specific\ndirectories with names like this:  application_1448033276100_0001  within\nwhich there are container specific directories with names like container_1448033276100_0001_01_000001 . The App Master container has the  000001 \nsuffix and the corresponding directory will have these files:  AppMaster.stderr  AppMaster.stdout  dt.log  The remaining container directories will have files:  dt.log  stderr  stdout  When problems occur, all these log files should be carefully examined. For example, the dt.log  file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:  2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=  com.datatorrent.stram.engine.StreamingContainer 1 /stdout 2 /stderr  You can provide your own  log4j  configuration file called, say,  log4j.properties  and place\nit in the directory  src/main/resources  as described in the configuration \npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say  INFO  to  DEBUG  while the application is running, you can click on the blue set logging level  button in the  Application Overview  panel of  dtManage . It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:   Normally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the  logs  dropdown but sometimes using the commandline\nfrom a terminal window may be easier.", 
            "title": "Debugging"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#partitioning", 
            "text": "Partitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.  Without partitioning, the DAG shown in the  logical  and  physical-dag-view  tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a  unifier . The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.  For our word counting example, we illustrate the technique by partitioning the  wordReader \noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe  StatelessPartitioner  which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the  low-mem.xml  configuration file\nwe created above\nto a new file named  simple-partition.xml  in the same directory and add this stanza to it:  property \n   name dt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   When you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:   Notice the two copies of  wordReader  and the generated unifier. The  physical  tab will\nalso show the containers for these additional operators and their characteristics as well.  A slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators:  wordReader  and the next operator windowWordCount . To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:  property \n   name dt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL /name \n   value true /value  /property   It enables the PARTITION_PARALLEL attribute on the  input port  of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:   Notice that both operators have been replicated and the unifier added.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#streaming-windows-and-application-windows", 
            "text": "Operators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a  streaming window . Its boundaries are marked by calls to  beginWindow  and endWindow  within which the platform repeatedly invokes either  emitTuples  (for input\nadapters) or  process  on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the OperatorGuide .  For flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:  property \n   name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n   value 5000 /value  /property   This is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.  A second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before,  app-name  and  op-name  should be\nreplaced by either wildcards or names of a specific application and/or operator):  property \n   name dt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT /name \n   value 5 /value  /property   By default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the  beginWindow  and  endWindow  are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.", 
            "title": "Streaming Windows and Application Windows"
        }, 
        {
            "location": "/tutorials/topnwords-c6/", 
            "text": "Appendix\n\n\nOperators in Top N words application\n\n\nThis section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nImplementing class\n\n\nDescription\n\n\n\n\n\n\nlineReader\n\n\nLineReader\n\n\nReads lines from input files.\n\n\n\n\n\n\nwordReader\n\n\nWordReader\n\n\nSplits a line into words.\n\n\n\n\n\n\nwindowWordCount\n\n\nWindowWordCount\n\n\nComputes word frequencies for a single window.\n\n\n\n\n\n\nfileWordCount\n\n\nFileWordCount\n\n\nMaintains per-file and global word frequencies.\n\n\n\n\n\n\nwcWriter\n\n\nWcWriter\n\n\nWrites top N words and their frequencies to output files.\n\n\n\n\n\n\nconsole\n\n\nConsoleOutputOperator\n\n\nWrites received tuples to console.\n\n\n\n\n\n\nsnapshotServerFile\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last data set for the current file, and returns it in response to queries.\n\n\n\n\n\n\nsnapshotServerGlobal\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last global data set, and returns it in response to queries.\n\n\n\n\n\n\nQueryFile\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for per-file data.\n\n\n\n\n\n\nQueryGlobal\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for global data.\n\n\n\n\n\n\nwsResultFile\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for per-file queries.\n\n\n\n\n\n\nwsResultGlobal\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for global queries.\n\n\n\n\n\n\n\n\n\nWe now describe the process of wiring these operators together in the\n\npopulateDAG()\n method of the main application class\n\nApplicationWithQuerySupport\n. First, the operators are created and added to\nthe DAG via the \naddOperator\n method:\n\n\nLineReader lineReader = dag.addOperator(\nlineReader\n,new LineReader());\n\n\n\n\nThe first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.\n\n\nNext, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the \naddStream\n function, for example:\n\n\ndag.addStream(\nlines\n, lineReader.output, wordReader.input);\n...\ndag.addStream(\nWordCountsFile\n, fileWordCount.outputPerFile, snapshotServerFile.input, console.input);\n\n\n\n\nNotice that the stream from \nfileWordCount.outputPerFile\n (which consists of\nthe top N words for the current file as the file is being read) goes to\n\nsnapshotServerFile.input\n (where it will be saved to respond to queries) and to\n\nconsole.input\n (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.\n\n\nThis section provides detailed information about each operator.\n\n\nLineReader\n\n\nThis class extends \nAbstractFileInputOperator\nString\n to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.\n\n\nThe base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods: \nopenFile\n, \ncloseFile\n, \nreadEntity\n, and \nemit\n. Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the \nFileWordCount\n operator.\n\n\nWordReader\n\n\nThis operator receives lines from \nLineReader\n on the input port and emits\nwords on the output port. It has a configurable property called \nnonWordStr\n\nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property\n\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n.\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe \nprocess\n method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.\n\n\nWindowWordCount\n\n\nThis operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when\n\nendWindow\n is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the\n\nendWindow\n call, but rather emit output tuples as each input tuple is\nprocessed.\n\n\nFileWordCount\n\n\nThis operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen \nLineReader\n reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next \nendWindow\n. The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we \ndo\n\nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing\n\nbeginWindow\n and \nendWindow\n calls by the upstream operator.\n\n\nThis operator also has three output ports: the \noutputPerFile\n port for the top\nN pairs for the current file as it is being read; the \noutputGlobal\n port for\nthe global top N pairs, and the \nfileOutput\n port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.\n\n\nFileWordCount\n also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.\n\n\nFileWordCount\n has a configurable property \ntopN\n for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name: \ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n\n\nIn the \nendWindow\n call, both maps are passed to the \ngetTopNList\n function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.\n\n\nWordCountWriter\n\n\nThis operator extends \nAbstractFileOutputOperator\nMap\nString,Object\n, and\nsimply writes the final top N pairs to the output file. As with \nLineReader\n,\nmost of the complexity of \nWordCountWriter\n is hidden in the base class. You must\nprovide implementations for 3 methods: \nendWindow\n, \ngetFileName\n, and\n\ngetBytesForTuple\n. The first method calls the base class method \nrequestFinalize\n.\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The \ngetFileName\n\nmethod retrieves the file name from the tuple, and the \ngetBytesForTuple\n\nmethod converts the list of pairs to a string in the desired format.\n\n\nConsoleOutputOperator\n\n\nThis is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.\n\n\nAppDataSnapshotServerMap\n\n\nThis operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots \n  one for a per-file top N snapshot and one for a\nglobal snapshot.\n\n\nPubSubWebSocketAppDataQuery\n\n\nThis is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:\n\n\nws://gateway-host:port/pubsub\n\n\n\n\nwhere \ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message to the URL where the value of the \ndata\n key\nis the desired message content. The JSON might look like this:\n\n\n{\ntype\n:\npublish\n, \ntopic\n:\nfoobar\n, \ndata\n: ...}\n\n\n\n\nCorrespondingly, subscribers send messages like this to retrieve published\nmessage data:\n\n\n{\ntype\n:\nsubscribe\n, \ntopic\n:\nfoobar\n}\n\n\n\n\nTopic names need not be pre-registered anywhere but the same topic\nname (for example, \nfoobar\n in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.\n\n\nFor this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values \nTopNWordsQueryFile\n and \nTopNWordsQueryGlobal\n under the\nrespective names:\n\n\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n\n\n\n\nPubSubWebSocketAppDataResult\n\n\nAnalogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values\n\nTopNWordsQueryFileResult\n and \nTopNWordsQueryGlobalResult\n corresponding to\nthe respective names:\n\n\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n\n\n\n\nFurther Exploration\n\n\nIn this tutorial, the property values in the \nproperties.xml\n file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.\n\n\nAnother aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the \nFileWordCount\n operator gets an EOF on the\ncontrol port, it waits for an \nendWindow\n call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.\n\n\nDataTorrent terminology\n\n\nOperators\n\n\nOperators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.\n\n\nStreams\n\n\nA stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.\n\n\nPorts\n\n\nPorts are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.\n\n\nDirected Acyclic Graph (DAG)\n\n\nA DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.\n\n\nLogical Plan or DAG\n\n\nLogical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.\n\n\nPhysical Plan or DAG\n\n\nA physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.\n\n\nData Tuples Processed\n\n\nThis is the number of data objects processed by real-time stream processing\napplications.\n\n\nData Tuples Emitted\n\n\nThis is the number of data objects emitted after real-time stream processing\napplications complete processing operations.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.\n\n\nStreaming Window\n\n\nA streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.\n\n\nSliding Application Window\n\n\nSliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.\n\n\nDemo Applications\n\n\nThe real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.\n\n\nCommand-line Interface\n\n\nCommand line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.\n\n\nWeb services\n\n\nDataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#operators-in-top-n-words-application", 
            "text": "This section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.          Operator  Implementing class  Description    lineReader  LineReader  Reads lines from input files.    wordReader  WordReader  Splits a line into words.    windowWordCount  WindowWordCount  Computes word frequencies for a single window.    fileWordCount  FileWordCount  Maintains per-file and global word frequencies.    wcWriter  WcWriter  Writes top N words and their frequencies to output files.    console  ConsoleOutputOperator  Writes received tuples to console.    snapshotServerFile  AppDataSnapshotServerMap  Caches the last data set for the current file, and returns it in response to queries.    snapshotServerGlobal  AppDataSnapshotServerMap  Caches the last global data set, and returns it in response to queries.    QueryFile  PubSubWebSocketAppDataQuery  Receives queries for per-file data.    QueryGlobal  PubSubWebSocketAppDataQuery  Receives queries for global data.    wsResultFile  PubSubWebSocketAppDataResult  Returns results for per-file queries.    wsResultGlobal  PubSubWebSocketAppDataResult  Returns results for global queries.     We now describe the process of wiring these operators together in the populateDAG()  method of the main application class ApplicationWithQuerySupport . First, the operators are created and added to\nthe DAG via the  addOperator  method:  LineReader lineReader = dag.addOperator( lineReader ,new LineReader());  The first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.  Next, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the  addStream  function, for example:  dag.addStream( lines , lineReader.output, wordReader.input);\n...\ndag.addStream( WordCountsFile , fileWordCount.outputPerFile, snapshotServerFile.input, console.input);  Notice that the stream from  fileWordCount.outputPerFile  (which consists of\nthe top N words for the current file as the file is being read) goes to snapshotServerFile.input  (where it will be saved to respond to queries) and to console.input  (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.  This section provides detailed information about each operator.  LineReader  This class extends  AbstractFileInputOperator String  to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.  The base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods:  openFile ,  closeFile ,  readEntity , and  emit . Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the  FileWordCount  operator.  WordReader  This operator receives lines from  LineReader  on the input port and emits\nwords on the output port. It has a configurable property called  nonWordStr \nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr .\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe  process  method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.  WindowWordCount  This operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when endWindow  is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the endWindow  call, but rather emit output tuples as each input tuple is\nprocessed.  FileWordCount  This operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen  LineReader  reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next  endWindow . The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we  do \nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing beginWindow  and  endWindow  calls by the upstream operator.  This operator also has three output ports: the  outputPerFile  port for the top\nN pairs for the current file as it is being read; the  outputGlobal  port for\nthe global top N pairs, and the  fileOutput  port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.  FileWordCount  also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.  FileWordCount  has a configurable property  topN  for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name:  dt.application.TopNWordsWithQueries.operator.fileWordCount.topN  In the  endWindow  call, both maps are passed to the  getTopNList  function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.  WordCountWriter  This operator extends  AbstractFileOutputOperator Map String,Object , and\nsimply writes the final top N pairs to the output file. As with  LineReader ,\nmost of the complexity of  WordCountWriter  is hidden in the base class. You must\nprovide implementations for 3 methods:  endWindow ,  getFileName , and getBytesForTuple . The first method calls the base class method  requestFinalize .\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The  getFileName \nmethod retrieves the file name from the tuple, and the  getBytesForTuple \nmethod converts the list of pairs to a string in the desired format.  ConsoleOutputOperator  This is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.  AppDataSnapshotServerMap  This operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots    one for a per-file top N snapshot and one for a\nglobal snapshot.  PubSubWebSocketAppDataQuery  This is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:  ws://gateway-host:port/pubsub  where  gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message to the URL where the value of the  data  key\nis the desired message content. The JSON might look like this:  { type : publish ,  topic : foobar ,  data : ...}  Correspondingly, subscribers send messages like this to retrieve published\nmessage data:  { type : subscribe ,  topic : foobar }  Topic names need not be pre-registered anywhere but the same topic\nname (for example,  foobar  in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.  For this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values  TopNWordsQueryFile  and  TopNWordsQueryGlobal  under the\nrespective names:  dt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic  PubSubWebSocketAppDataResult  Analogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values TopNWordsQueryFileResult  and  TopNWordsQueryGlobalResult  corresponding to\nthe respective names:  dt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic", 
            "title": "Operators in Top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#further-exploration", 
            "text": "In this tutorial, the property values in the  properties.xml  file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.  Another aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the  FileWordCount  operator gets an EOF on the\ncontrol port, it waits for an  endWindow  call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.", 
            "title": "Further Exploration"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#datatorrent-terminology", 
            "text": "Operators  Operators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.  Streams  A stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.  Ports  Ports are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.  Directed Acyclic Graph (DAG)  A DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.  Logical Plan or DAG  Logical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.  Physical Plan or DAG  A physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.  Data Tuples Processed  This is the number of data objects processed by real-time stream processing\napplications.  Data Tuples Emitted  This is the number of data objects emitted after real-time stream processing\napplications complete processing operations.  Streaming Application Manager (STRAM)  Streaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.  Streaming Window  A streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.  Sliding Application Window  Sliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.  Demo Applications  The real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.  Command-line Interface  Command line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.  Web services  DataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "DataTorrent terminology"
        }, 
        {
            "location": "/tutorials/salesdimensions/", 
            "text": "Building the Sales Dimension application in JAVA\n\n\nThe Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.\n\n\nExample scenario\n\n\nA large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.\n\n\nIn order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.\n\n\nThe application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.\n\n\nThe application setup for this retailer requires:\n\n\n\n\nInput \n For receiving individual sales transactions\n\n\nTransform \n For converting incoming records into a consumable format\n\n\nEnrich \n For providing additional information for each record by\n    performing additional lookups\n\n\nCompute \n For performing aggregate computations on all possible\n    key field combinations\n\n\nStore \n For storing computed results for further\n    analysis and visualizations\n\n\nAnalyze, Alert \n Visualize \n For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.\n\n\n\n\nStep I: Build the Sales Dimension application\n\n\nTo save time we will use some source and data files that are available online.\nWe will create a new maven project using the maven archetype, add the source\nand data files to the project, modify them suitable and finally build and\ndeploy application.\n\n\nTo build an application\n\n\n\n\n\n\nCreate a new application project named, say \nsalesapp\n, as described in:\n   \nApache Apex Development Environment Setup\n\n\n\n\n\n\nDelete the following generated JAVA files: \nApplication.java\n and\n    \nRandomNumberGenerator.java\n under \nsrc/main/java/com/example/salesapp\n\n    and \nApplicationTest.java\n under \nsrc/test/java/com/example/salesapp\n.\n\n\n\n\n\n\nCheckout the \nexamples\n git repository in a suitable location, for example:\n\n\ncd; git checkout https://github.com/datatorrent/examples\n\n\n\n\n\n\n\nCopy the following files from that repository at\n    \nexamples/dt-demo/dimensions/src/main/java/com/datatorrent/demos/dimensions/sales/generic\n\n    to the main source directory of the new project at \nsrc/main/java/com/example/salesapp\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrichmentOperator.java\n\n\nJsonSalesGenerator.java\n\n\n\n\n\n\nJsonToMapConverter.java\n\n\nRandomWeightedMovableGenerator.java\n\n\n\n\n\n\nSalesDemo.java\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso copy these text files from the examples repository at\n    \nexamples/dt-demo/dimensions/src/main/resources\n:\n    \nsalesGenericDataSchema.json\n, \nsalesGenericEventSchema.json\n,\n    \nproducts.txt\n to the new project at \nsrc/main/resources\n. The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.\n\n\n\n\n\n\nChange the package location in each Java file to reflect\n    its current location by changing the line\n\n\npackage com.datatorrent.demos.dimensions.sales.generic;\n\n\n\nto\n\n\npackage com.example.salesapp;\n\n\n\n\n\n\n\nAdd a new file called \nInputGenerator.java\n to the same location\n    containing this block of code:\n\n\npackage com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator\nT\n extends InputOperator {\n    public OutputPort\nT\n getOutputPort();\n}\n\n\n\n\n\n\n\nRemove these lines from \nJsonSalesGenerator.java\n (the first is\n    unused, while the second is now package local):\n\n\nimport com.datatorrent.demos.dimensions.InputGenerator;\nimport com.datatorrent.demos.dimensions.ads.AdInfo;\n\n\n\nAlso remove the first import from \nSalesDemo.java\n.\n\n\n\n\n\n\nAdd the following two lines to \nSalesDemo.java\n (if it does not exist already).\n\n\nPubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\nwsIn.setTopic(\"SalesDimensionsQuery\");      // 1. Add this line\nstore.setEmbeddableQueryInfoProvider(wsIn);\n\nPubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\nwsOut.setTopic(\"SalesDimensionResult\");     // 2. Add this line\n\ndag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n...\n\n\n\n\n\n\n\nMake the following changes to pom.xml:\n\n\n\n\n\n\nChange the artifactId to something that is likely to be unique to\n   this application, for example: \nartifactId\nsalesapp\n/artifactId\n.\n   This step is optional but is recommended since uploading a second\n   package with the same artifact id will overwrite the first. Similarly,\n   change the \nname\n and \ndescription\n elements to something meaningful\n   for this application.\n\n\n\n\n\n\nAdd the following \nrepositories\n element at the top level (i.e. as a\n   child of the \nproject\n element):\n\n\n!-- repository to provide the DataTorrent artifacts --\n\n\nrepositories\n\n  \nrepository\n\n    \nid\ndatatorrent\n/id\n\n    \nname\nDataTorrent Release Repository\n/name\n\n    \nurl\nhttps://www.datatorrent.com/maven/content/repositories/releases/\n/url\n\n    \nsnapshots\n\n      \nenabled\nfalse\n/enabled\n\n    \n/snapshots\n\n  \n/repository\n\n\n/repositories\n\n\n\n\n\n\n\n\nAdd these lines to the dependencies section at the end of the \npom.xml\n\nfile (the version number might need to change as new releases come out):\n\n\ndependency\n\n  \ngroupId\ncom.datatorrent\n/groupId\n\n  \nartifactId\ndt-contrib\n/artifactId\n\n  \nversion\n3.5.0\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\n*\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\ndependency\n\n  \ngroupId\ncom.datatorrent\n/groupId\n\n  \nartifactId\ndt-library\n/artifactId\n\n  \nversion\n3.5.0\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\n*\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\n\n\n\n\n\n\nFinally change \napex.version\n to \n3.6.0-SNAPSHOT\n. To recapitulate, we are\n   using versions \n3.5.0\n for \ndt-contrib\n and \ndt-library\n, \n3.6.0\n\n   for \nmalhar-library\n and \n3.6.0-SNAPSHOT\n for Apex.\n\n\n\n\n\n\n\n\n\n\nBuild the project as usual:\n\n\nmvn clean package -DskipTests\n\n\n\n\n\n\n\nAssuming the build is successful, you should see the package file named\n\nsalesApp-1.0-SNAPSHOT.jar\n under the target directory. The next step\nshows you how to use the \ndtManage\n GUI to upload the package and launch the\napplication from there.\n\n\nStep II: Upload the Sales Dimension application package\n\n\nTo upload the Sales Dimension application package\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n    both \ndtadmin\n).\n\n\nOn the menu bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, click on \nupload a package\n.\n    \n\n\nNavigate to the location of \nsalesApp-1.0-SNAPSHOT.apa\n and select it.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep III: Launch the Sales Dimension application\n\n\nNote\n: If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.\n\n\n\n\nIn the menu bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the Sales Dimension application, and click\n   \nlaunch application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n   \nUse configuration file\n. To specify individual properties, select \nSpecify Launch Properties\n.\n\n\nClick Launch.\n\n\n\n\nIf the launch is successful, a notification will appear on the top-right corner with the application ID and a hyperlink to monitor the running application.\n\n\nOperator base classes and interfaces\n\n\nThis section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.\n\n\nOperators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The \nOperator\n interface extends the \nComponent\n\ninterface:\n\n\npublic interface Component \nCONTEXT extends Context\n {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}\n\n\n\nThe \nOperator\n interface defines \nPort\n, \nInputPort\n, and \nOutputPort\n as inner interfaces with\n\nInputPort\n, and \nOutputPort\n extending \nPort\n.\n\n\npublic interface Operator extends Component\nContext.OperatorContext\n {\n\n  public static interface Port extends Component\nContext.PortContext\n {}\n\n  public static interface InputPort\nT extends Object\n extends Port {\n    public Sink\nT\n getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec\nT\n getStreamCodec();\n  }\n\n  public static interface OutputPort\nT extends Object\n extends Port {\n    public void setSink(Sink\nObject\n sink);\n    public Unifier\nT\n getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}\n\n\n\nOperators typically extend the \nBaseOperator\n class which simply\ndefines empty methods for \nsetup\n, \nteardown\n, \nbeginWindow\n, and\n\nendWindow\n. Derived classes only need to define those functions for\nwhich they want to perform an action. For example the\n\nConsoleOutputOperator\n class, which is often used during testing and\ndebugging, does not override any of these methods.\n\n\nInput operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).\n\n\nInput ports must implement the \nInputOperator\n interface.\n\n\npublic interface InputOperator extends Operator {\n  public void emitTuples();\n}\n\n\n\nThe \nemitTuples\n method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named \nRandomNumberGenerator\n,\nwhich is defined like this:\n\n\npublic class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort\nDouble\n out = new DefaultOutputPort\nDouble\n();\n\n  public void emitTuples()  {\n    if (count++ \n 100) {\n      out.emit(Math.random());\n    }\n  }\n}\n\n\n\nFinally, the \nDefaultInputPort\n and \nDefaultOutputPort\n classes are\nvery useful as base classes that can be extended when defining ports\nin operators.\n\n\npublic abstract class DefaultInputPort\nT\n implements InputPort\nT\n, Sink\nT\n {\n  private int count;\n\n  public Sink\nT\n getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort\nT\n implements Operator.OutputPort\nT\n {\n  private transient Sink\nObject\n sink;\n\n  final public void setSink(Sink\nObject\n s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}\n\n\n\nThe \nDefaultInputPort\n class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract \nprocess\n method needs to be\nimplemented by any concrete derived class; it will be invoked via the\n\nSink.put\n override.\n\n\nThe \nDefaultOutputPort\n class also supports a sink and forwards calls\nto \nemit\n to the sink. The \nsetSink\n method is called by the \nStrAM\n\nexecution engine to inject a suitable sink at deployment time.\n\n\nOutput operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending \nBaseOperator\n for\nconvenience. For example, the \nConsoleOutputOperator\n mentioned earlier\nis defined like this:\n\n\npublic class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort\nObject\n input = new DefaultInputPort\nObject\n() {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}\n\n\n\nNotice that the implementation of the abstract method\n\nDefaultInputPort.process\n simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).\n\n\nOperators in the Sales Dimensions application\n\n\nThe application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the\n\ndag.addoperator\n call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.\n\n\nThis diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.\n\n\n\nJsonSalesGenerator (InputGenerator)\n\n\nThis class (new operator) is an input operator that generates a single\nsales event defined by a class like this:\n\n\nclass SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}\n\n\n\nJsonToMapConverter (Converter)\n\n\nThis operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:\n\n\npublic class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort\nbyte\\[\\]\n input = new DefaultInputPort\nbyte[]\n() {\n    public void process(byte\\[\\] message) {\n      Map\nString, Object\n tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort\nMap\nString, Object\n outputMap\n     = new DefaultOutputPort\nMap\nString, Object\n();\n\n}\n\n\n\nEnrichmentOperator (Enrichment)\n\n\nThis operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file \nproducts.txt\n that\nwe encountered earlier while building the application. It contains\ndata like this:\n\n\n{\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}\n\n\n\nThe core functionality of this operator is in the \nprocess\n function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.\n\n\npublic class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort\nMap\nString, Object\n\n    outputPort = new DefaultOutputPort\nMap\nString, Object\n();\n\n  public transient DefaultInputPort\nMap\nString, Object\n\n    inputPort = new DefaultInputPort\nMap\nString, Object\n() {\n\n    public void process(Map\nString, Object\n tuple) {\n      ...\n    }\n  }\n}\n\n\n\nDimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)\n\n\nThis operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.\n\n\nAppDataSingleDimensionStoreHDHT (Store)\n\n\nThis operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.\n\n\nPubSubWebSocketAppDataQuery (Query)\n\n\nThis is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike \nws://\ngateway-host\n:\nport\n/pubsub\n where\n\ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message that looks like this to the URL\nwhere the value of the \ndata\n key is the desired message content:\n\n\n{\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}\n\n\n\nCorrespondingly, subscribers send messages like this\nto retrieve published message data:\n\n\n{\"type\":\"subscribe\", \"topic\":\"foobar\"}\n\n\n\nTopic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g. \nfoobar\n in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.\n\n\nThis query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:\n\n\npublic class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}\n\n\n\nThe important method here is \nconvertMessage\n to convert the input\nstring to a JSON object, get the value of the \ndata\n key from the object\nand return it. The base classes look like this:\n\n\npublic class PubSubWebSocketInputOperator\nT\n extends WebSocketInputOperator\nT\n {\n  ...\n}\n\n\n\nThis class simply converts a JSON event into Java maps via the\n\nconvertMessage\n method.\n\n\npublic class WebSocketInputOperator\nT\n extends\nSimpleSinglePortInputOperator\nT\n implements Runnable {\n  ...\n}\n\n\n\nThis code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.\n\n\npublic abstract class SimpleSinglePortInputOperator\nT\n extends BaseOperator\nimplements InputOperator, Operator.ActivationListener\nOperatorContext\n {\n\n  final public transient BufferingOutputPort\nT\n outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort\nT\n extends DefaultOutputPort\nT\n {\n    public void flush(int count) { ... }\n  }\n\n}\n\n\n\nThe class starts a separate thread which retrieves source events and\ninvokes the \nemit\n method of the output port; the output port buffers\nevents until the \nflush\n method is called at which point all buffered\nevents are emitted.\n\n\nPubSubWebSocketAppDataResult (QueryResult)\n\n\nThis is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:\n\n\npublic class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n}\n\n\n\nThis class merely overrides the generic \nconvertMapToMessage\n method of the\nbase class to generate the required JSON publish message.\n\n\npublic class PubSubWebSocketOutputOperator\nT\n extends WebSocketOutputOperator\nT\n {\n  ...\n}\n\n\n\nThis class, similarly, doesn't do much \n the \nconvertMapToMessage\n\nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.\n\n\npublic class WebSocketOutputOperator\nT\n extends BaseOperator {\n  public final transient DefaultInputPort\nT\n input = new DefaultInputPort\nT\n() {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}\n\n\n\nThe key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is \nprocess\n\nwhich converts the incoming event to a JSON message and sends it\nacross the connection.\n\n\nConnecting the operators\n\n\nNow that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the\n\nStreamingApplication\n interface:\n\n\npublic class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    ...\n\n    PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\n    wsIn.setTopic(\"SalesDimensionsQuery\");\n    store.setEmbeddableQueryInfoProvider(wsIn);\n\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setTopic(\"SalesDimensionsResult\");\n\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, wsOut.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}\n\n\n\nThe key method to implement in an application is \npopulateDAG\n; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the \nDAG.StreamMeta\n\ninterface and is created via \nDAG.addStream()\n. The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the\n\npopulateDAG\n function.\n\n\nThese two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/salesdimensions/#building-the-sales-dimension-application-in-java", 
            "text": "The Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.", 
            "title": "Building the Sales Dimension application in JAVA"
        }, 
        {
            "location": "/tutorials/salesdimensions/#example-scenario", 
            "text": "A large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.  In order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.  The application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.  The application setup for this retailer requires:   Input   For receiving individual sales transactions  Transform   For converting incoming records into a consumable format  Enrich   For providing additional information for each record by\n    performing additional lookups  Compute   For performing aggregate computations on all possible\n    key field combinations  Store   For storing computed results for further\n    analysis and visualizations  Analyze, Alert   Visualize   For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.", 
            "title": "Example scenario"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-i-build-the-sales-dimension-application", 
            "text": "To save time we will use some source and data files that are available online.\nWe will create a new maven project using the maven archetype, add the source\nand data files to the project, modify them suitable and finally build and\ndeploy application.  To build an application    Create a new application project named, say  salesapp , as described in:\n    Apache Apex Development Environment Setup    Delete the following generated JAVA files:  Application.java  and\n     RandomNumberGenerator.java  under  src/main/java/com/example/salesapp \n    and  ApplicationTest.java  under  src/test/java/com/example/salesapp .    Checkout the  examples  git repository in a suitable location, for example:  cd; git checkout https://github.com/datatorrent/examples    Copy the following files from that repository at\n     examples/dt-demo/dimensions/src/main/java/com/datatorrent/demos/dimensions/sales/generic \n    to the main source directory of the new project at  src/main/java/com/example/salesapp .         EnrichmentOperator.java  JsonSalesGenerator.java    JsonToMapConverter.java  RandomWeightedMovableGenerator.java    SalesDemo.java        Also copy these text files from the examples repository at\n     examples/dt-demo/dimensions/src/main/resources :\n     salesGenericDataSchema.json ,  salesGenericEventSchema.json ,\n     products.txt  to the new project at  src/main/resources . The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.    Change the package location in each Java file to reflect\n    its current location by changing the line  package com.datatorrent.demos.dimensions.sales.generic;  to  package com.example.salesapp;    Add a new file called  InputGenerator.java  to the same location\n    containing this block of code:  package com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator T  extends InputOperator {\n    public OutputPort T  getOutputPort();\n}    Remove these lines from  JsonSalesGenerator.java  (the first is\n    unused, while the second is now package local):  import com.datatorrent.demos.dimensions.InputGenerator;\nimport com.datatorrent.demos.dimensions.ads.AdInfo;  Also remove the first import from  SalesDemo.java .    Add the following two lines to  SalesDemo.java  (if it does not exist already).  PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\nwsIn.setTopic(\"SalesDimensionsQuery\");      // 1. Add this line\nstore.setEmbeddableQueryInfoProvider(wsIn);\n\nPubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\nwsOut.setTopic(\"SalesDimensionResult\");     // 2. Add this line\n\ndag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n...    Make the following changes to pom.xml:    Change the artifactId to something that is likely to be unique to\n   this application, for example:  artifactId salesapp /artifactId .\n   This step is optional but is recommended since uploading a second\n   package with the same artifact id will overwrite the first. Similarly,\n   change the  name  and  description  elements to something meaningful\n   for this application.    Add the following  repositories  element at the top level (i.e. as a\n   child of the  project  element):  !-- repository to provide the DataTorrent artifacts --  repositories \n   repository \n     id datatorrent /id \n     name DataTorrent Release Repository /name \n     url https://www.datatorrent.com/maven/content/repositories/releases/ /url \n     snapshots \n       enabled false /enabled \n     /snapshots \n   /repository  /repositories     Add these lines to the dependencies section at the end of the  pom.xml \nfile (the version number might need to change as new releases come out):  dependency \n   groupId com.datatorrent /groupId \n   artifactId dt-contrib /artifactId \n   version 3.5.0 /version \n   exclusions \n     exclusion \n       groupId * /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency  dependency \n   groupId com.datatorrent /groupId \n   artifactId dt-library /artifactId \n   version 3.5.0 /version \n   exclusions \n     exclusion \n       groupId * /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency     Finally change  apex.version  to  3.6.0-SNAPSHOT . To recapitulate, we are\n   using versions  3.5.0  for  dt-contrib  and  dt-library ,  3.6.0 \n   for  malhar-library  and  3.6.0-SNAPSHOT  for Apex.      Build the project as usual:  mvn clean package -DskipTests    Assuming the build is successful, you should see the package file named salesApp-1.0-SNAPSHOT.jar  under the target directory. The next step\nshows you how to use the  dtManage  GUI to upload the package and launch the\napplication from there.", 
            "title": "Step I: Build the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-ii-upload-the-sales-dimension-application-package", 
            "text": "To upload the Sales Dimension application package   Log on to the DataTorrent Console (the default username and password are\n    both  dtadmin ).  On the menu bar, click  Develop .  Under  App Packages , click on  upload a package .\n      Navigate to the location of  salesApp-1.0-SNAPSHOT.apa  and select it.  Wait till the package is successfully uploaded.", 
            "title": "Step II: Upload the Sales Dimension application package"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-iii-launch-the-sales-dimension-application", 
            "text": "Note : If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.   In the menu bar, click  Develop .  Under  App Packages , locate the Sales Dimension application, and click\n    launch application .  (Optional) To configure the application using a configuration file, select\n    Use configuration file . To specify individual properties, select  Specify Launch Properties .  Click Launch.   If the launch is successful, a notification will appear on the top-right corner with the application ID and a hyperlink to monitor the running application.", 
            "title": "Step III: Launch the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#operator-base-classes-and-interfaces", 
            "text": "This section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.  Operators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The  Operator  interface extends the  Component \ninterface:  public interface Component  CONTEXT extends Context  {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}  The  Operator  interface defines  Port ,  InputPort , and  OutputPort  as inner interfaces with InputPort , and  OutputPort  extending  Port .  public interface Operator extends Component Context.OperatorContext  {\n\n  public static interface Port extends Component Context.PortContext  {}\n\n  public static interface InputPort T extends Object  extends Port {\n    public Sink T  getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec T  getStreamCodec();\n  }\n\n  public static interface OutputPort T extends Object  extends Port {\n    public void setSink(Sink Object  sink);\n    public Unifier T  getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}  Operators typically extend the  BaseOperator  class which simply\ndefines empty methods for  setup ,  teardown ,  beginWindow , and endWindow . Derived classes only need to define those functions for\nwhich they want to perform an action. For example the ConsoleOutputOperator  class, which is often used during testing and\ndebugging, does not override any of these methods.  Input operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).  Input ports must implement the  InputOperator  interface.  public interface InputOperator extends Operator {\n  public void emitTuples();\n}  The  emitTuples  method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named  RandomNumberGenerator ,\nwhich is defined like this:  public class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort Double  out = new DefaultOutputPort Double ();\n\n  public void emitTuples()  {\n    if (count++   100) {\n      out.emit(Math.random());\n    }\n  }\n}  Finally, the  DefaultInputPort  and  DefaultOutputPort  classes are\nvery useful as base classes that can be extended when defining ports\nin operators.  public abstract class DefaultInputPort T  implements InputPort T , Sink T  {\n  private int count;\n\n  public Sink T  getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort T  implements Operator.OutputPort T  {\n  private transient Sink Object  sink;\n\n  final public void setSink(Sink Object  s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}  The  DefaultInputPort  class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract  process  method needs to be\nimplemented by any concrete derived class; it will be invoked via the Sink.put  override.  The  DefaultOutputPort  class also supports a sink and forwards calls\nto  emit  to the sink. The  setSink  method is called by the  StrAM \nexecution engine to inject a suitable sink at deployment time.  Output operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending  BaseOperator  for\nconvenience. For example, the  ConsoleOutputOperator  mentioned earlier\nis defined like this:  public class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort Object  input = new DefaultInputPort Object () {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}  Notice that the implementation of the abstract method DefaultInputPort.process  simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).", 
            "title": "Operator base classes and interfaces"
        }, 
        {
            "location": "/tutorials/salesdimensions/#operators-in-the-sales-dimensions-application", 
            "text": "The application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the dag.addoperator  call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.  This diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.  JsonSalesGenerator (InputGenerator)  This class (new operator) is an input operator that generates a single\nsales event defined by a class like this:  class SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}  JsonToMapConverter (Converter)  This operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:  public class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort byte\\[\\]  input = new DefaultInputPort byte[] () {\n    public void process(byte\\[\\] message) {\n      Map String, Object  tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort Map String, Object  outputMap\n     = new DefaultOutputPort Map String, Object ();\n\n}  EnrichmentOperator (Enrichment)  This operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file  products.txt  that\nwe encountered earlier while building the application. It contains\ndata like this:  {\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}  The core functionality of this operator is in the  process  function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.  public class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort Map String, Object \n    outputPort = new DefaultOutputPort Map String, Object ();\n\n  public transient DefaultInputPort Map String, Object \n    inputPort = new DefaultInputPort Map String, Object () {\n\n    public void process(Map String, Object  tuple) {\n      ...\n    }\n  }\n}  DimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)  This operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.  AppDataSingleDimensionStoreHDHT (Store)  This operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.  PubSubWebSocketAppDataQuery (Query)  This is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike  ws:// gateway-host : port /pubsub  where gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message that looks like this to the URL\nwhere the value of the  data  key is the desired message content:  {\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}  Correspondingly, subscribers send messages like this\nto retrieve published message data:  {\"type\":\"subscribe\", \"topic\":\"foobar\"}  Topic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g.  foobar  in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.  This query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:  public class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}  The important method here is  convertMessage  to convert the input\nstring to a JSON object, get the value of the  data  key from the object\nand return it. The base classes look like this:  public class PubSubWebSocketInputOperator T  extends WebSocketInputOperator T  {\n  ...\n}  This class simply converts a JSON event into Java maps via the convertMessage  method.  public class WebSocketInputOperator T  extends\nSimpleSinglePortInputOperator T  implements Runnable {\n  ...\n}  This code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.  public abstract class SimpleSinglePortInputOperator T  extends BaseOperator\nimplements InputOperator, Operator.ActivationListener OperatorContext  {\n\n  final public transient BufferingOutputPort T  outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort T  extends DefaultOutputPort T  {\n    public void flush(int count) { ... }\n  }\n\n}  The class starts a separate thread which retrieves source events and\ninvokes the  emit  method of the output port; the output port buffers\nevents until the  flush  method is called at which point all buffered\nevents are emitted.  PubSubWebSocketAppDataResult (QueryResult)  This is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:  public class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n}  This class merely overrides the generic  convertMapToMessage  method of the\nbase class to generate the required JSON publish message.  public class PubSubWebSocketOutputOperator T  extends WebSocketOutputOperator T  {\n  ...\n}  This class, similarly, doesn't do much   the  convertMapToMessage \nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.  public class WebSocketOutputOperator T  extends BaseOperator {\n  public final transient DefaultInputPort T  input = new DefaultInputPort T () {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}  The key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is  process \nwhich converts the incoming event to a JSON message and sends it\nacross the connection.", 
            "title": "Operators in the Sales Dimensions application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#connecting-the-operators", 
            "text": "Now that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the StreamingApplication  interface:  public class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    ...\n\n    PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\n    wsIn.setTopic(\"SalesDimensionsQuery\");\n    store.setEmbeddableQueryInfoProvider(wsIn);\n\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setTopic(\"SalesDimensionsResult\");\n\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, wsOut.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}  The key method to implement in an application is  populateDAG ; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the  DAG.StreamMeta \ninterface and is created via  DAG.addStream() . The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the populateDAG  function.  These two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.", 
            "title": "Connecting the operators"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/", 
            "text": "Building the Sales Dimensions application using dtAssemble\n\n\nThe DataTorrent RTS platform supports building new applications using \ndtAssemble\n, the Graphical\nApplication Builder which we will use for the Sales Dimensions application. \ndtAssemble\n\nis an easy and intuitive tool for constructing applications,\nwhile providing a great visualization of the logical operator connectivity and the\napplication data flow.\n\n\nNote\n: You can also find these instructions in the UI console. Click \nLearn\n in the menu\nbar, and then click the first link in the left panel: \nTransform, Analyze, Alert\n.\n\n\nStep 1: Open the Application Builder interface\n\n\n\n\nOn the DataTorrent RTS console, navigate to \nApp Packages\n.\n\n\nMake sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).\n\n\nClick the green \nCreate new application\n button, and name the application\n    Sales Dimensions. The Application Canvas window should open.\n    \n\n\n\n\nStep 2: Add and connect operators\n\n\n\n\n\n\nUnder \nOperator Library\n in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.\n\n\n\n\nJSON Sales Event Generator (Input)\n \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.\n\n\nEnrichment (Enrich)\n \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n \n This operator\n   stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n \n The dashboard connector for\n   visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n \n The dashboard connector for\n   visualization data results.\n\n\n\n\n\n\n\n\nTo connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:\n    \n\n\n\n\n\n\nStep 3: Customize application and operator settings\n\n\nCustomize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the \nOperator Inspector\n panel\non the bottom to edit the operator and stream settings as described in the item:\n\n\n\n\n\n\nCopy this Sales schema below into the \nEvent Schema JSON\n field of \nInput\n\n    operator, and the \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n\n    operators.\n\n\n{\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to\n    \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\nOptional\n: In the \nBuilding with Java\n section, the \nApp Data Pub Sub Query (PubSubWebSocketAppDataQuery)\n operator was not added to the DAG. Instead, it was embedded into the \nstore\n operator to avoid query delays which may happen when the operator is blocked upstream. You can achieve the same results in dtAssemble by filling the \nEmbeddable Query Info Provider\n field of the \nStore\n operator with the properties set in the \nQuery\n operator, and then removing the \nQuery\n operator.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.\n    Set \nBase Path\n value to \nSalesDimensionsDemoStore\n. This sets the HDHT\n    storage path to write dimensions computation results to\n    \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n    \n\n\n\n\nClick the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.\n\n\n\n\nNote\n: Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.\n\n\nStep 4: Launch the application\n\n\nOnce the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the\n\nApplication Canvas\n window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.\n\n\nTo launch the Sales Dimension application\n\n\n\n\nClick the launch button at the top left of the application canvas screen.\n\n\nType a name for the application in the \nName this application\n box.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a configuration file\n checkbox.\n\n\n(Optional) To specify individual properties, select\n    \nSpecify Launch Properties\n checkbox.\n\n\nClick Launch.\n\n\n\n\n\n\nOnce the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled \nMonitoring the Sales\nDimensions Application with dtManage\n.\n\n\n\n\nGo to the Sales Dimensions application operations page under the \nMonitor\n tab.\n\n\nConfirm that the application is launched successfully by validating that\n    the state of the application under the \nApplication Overview\n section\n    is \nRUNNING\n.\n\n\nMake sure that all the operators are successfully started under the\n    \nStramEvents\n widget.\n\n\nNavigate to the \nphysical\n tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.\n    \n\n\n\n\nNote\n: This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#building-the-sales-dimensions-application-using-dtassemble", 
            "text": "The DataTorrent RTS platform supports building new applications using  dtAssemble , the Graphical\nApplication Builder which we will use for the Sales Dimensions application.  dtAssemble \nis an easy and intuitive tool for constructing applications,\nwhile providing a great visualization of the logical operator connectivity and the\napplication data flow.  Note : You can also find these instructions in the UI console. Click  Learn  in the menu\nbar, and then click the first link in the left panel:  Transform, Analyze, Alert .", 
            "title": "Building the Sales Dimensions application using dtAssemble"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-1-open-the-application-builder-interface", 
            "text": "On the DataTorrent RTS console, navigate to  App Packages .  Make sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).  Click the green  Create new application  button, and name the application\n    Sales Dimensions. The Application Canvas window should open.", 
            "title": "Step 1: Open the Application Builder interface"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-2-add-and-connect-operators", 
            "text": "Under  Operator Library  in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.   JSON Sales Event Generator (Input)  \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.  Enrichment (Enrich)  \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.  Simple App Data Dimensions Store (Store)    This operator\n   stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)    The dashboard connector for\n   visualization queries.  App Data Pub Sub Result (Result)    The dashboard connector for\n   visualization data results.     To connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:", 
            "title": "Step 2: Add and connect operators"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-3-customize-application-and-operator-settings", 
            "text": "Customize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the  Operator Inspector  panel\non the bottom to edit the operator and stream settings as described in the item:    Copy this Sales schema below into the  Event Schema JSON  field of  Input \n    operator, and the  Configuration Schema JSON  of the  Compute  and  Store \n    operators.  {\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to\n     SalesDimensionsQuery  and  SalesDimensionsResult  respectively.  Optional : In the  Building with Java  section, the  App Data Pub Sub Query (PubSubWebSocketAppDataQuery)  operator was not added to the DAG. Instead, it was embedded into the  store  operator to avoid query delays which may happen when the operator is blocked upstream. You can achieve the same results in dtAssemble by filling the  Embeddable Query Info Provider  field of the  Store  operator with the properties set in the  Query  operator, and then removing the  Query  operator.    Select the  Store  operator, and edit the  File Store  property.\n    Set  Base Path  value to  SalesDimensionsDemoStore . This sets the HDHT\n    storage path to write dimensions computation results to\n     /user/ username /SalesDimensionsDemoStore  on HDFS.\n       Click the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.   Note : Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.", 
            "title": "Step 3: Customize application and operator settings"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-4-launch-the-application", 
            "text": "Once the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the Application Canvas  window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.  To launch the Sales Dimension application   Click the launch button at the top left of the application canvas screen.  Type a name for the application in the  Name this application  box.  (Optional) To configure the application using a configuration file, select\n     Use a configuration file  checkbox.  (Optional) To specify individual properties, select\n     Specify Launch Properties  checkbox.  Click Launch.    Once the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled  Monitoring the Sales\nDimensions Application with dtManage .   Go to the Sales Dimensions application operations page under the  Monitor  tab.  Confirm that the application is launched successfully by validating that\n    the state of the application under the  Application Overview  section\n    is  RUNNING .  Make sure that all the operators are successfully started under the\n     StramEvents  widget.  Navigate to the  physical  tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.\n       Note : This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.", 
            "title": "Step 4: Launch the application"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/", 
            "text": "Visualizing data from the Sales Dimension application using dtDashboard\n\n\nDataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled \ndtDashboard\n- Application Data Visualization\n at \nhttps://docs.datatorrent.com\n.\n\n\nAfter the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.\n\n\nGenerate dashboards\n\n\n\n\nIf you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.\n\n\nAfter the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:\n\n\n\n\nAdding widgets\n\n\nTo derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets: \nbar chart\n, \npie chart\n, \nhorizontal bar chart\n, \ntable\n, and\n\nnote\n.\n\n\nTo add a widget\n\n\n\n\nClick the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n    \n\n\nIn the Data Source list, click a data source for your widget.\n\n\nSelect a widget type under \nAvailable Widgets\n.\n    \n\n\nClick \nadd widget\n button.\n\n\n\n\nThe widget is added to your dashboard.\n\n\nEdit a widget\n\n\nAfter you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo edit a widget\n\n\n\n\n\n\nChange the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.\n\n\n\n\n\n\nEdit the widget:\n    a.  In the top-right corner of the widget, click \nedit\n.\n    b.  Type a new title in the \nTitle\n box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click \nOK\n.\n    \n\n\n\n\n\n\nTo remove a widget, in the top-right corner, click the \ndelete\n button.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#visualizing-data-from-the-sales-dimension-application-using-dtdashboard", 
            "text": "DataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled  dtDashboard\n- Application Data Visualization  at  https://docs.datatorrent.com .  After the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.", 
            "title": "Visualizing data from the Sales Dimension application using dtDashboard"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#generate-dashboards", 
            "text": "If you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.  After the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:", 
            "title": "Generate dashboards"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#adding-widgets", 
            "text": "To derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets:  bar chart ,  pie chart ,  horizontal bar chart ,  table , and note .  To add a widget   Click the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n      In the Data Source list, click a data source for your widget.  Select a widget type under  Available Widgets .\n      Click  add widget  button.   The widget is added to your dashboard.", 
            "title": "Adding widgets"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#edit-a-widget", 
            "text": "After you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To edit a widget    Change the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.    Edit the widget:\n    a.  In the top-right corner of the widget, click  edit .\n    b.  Type a new title in the  Title  box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click  OK .\n        To remove a widget, in the top-right corner, click the  delete  button.", 
            "title": "Edit a widget"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/", 
            "text": "Monitoring the Sales Dimension application using dtManage\n\n\nRecall that after the application is built and validated, it can be\nlaunched from the \nApp Packages\n page as described in an earlier chapter;\napplications built with \ndtAssemble\n can also, optionally, be launched\nfrom the \nApplication Canvas\n page as described earlier. This section\ndescribes how you can monitor the running Sales Dimension application\nusing \ndtManage\n.\n\n\nThe Monitor menu option\n\n\nYou can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click \nMonitor\n, you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.\n\n\nlogical\n\n\nThis image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the \n+\n button at the top-left corner and choosing\nfrom the resulting dropdown list.\n\n\n\n\n\n\n\n\nApplication Overview\n\n\nThis widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.\n\n\n\n\n\n\nStramEvents\n\n\nThis widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.\n\n\n\n\n\n\nLogical DAG\n\n\nThis widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.\n\n\nTo include additional details\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator representation,\n    click the Top list, and select a metric.\n\n\nTo display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.\n\n\n\n\n\n\n\n\nLogical Operators\n\n\nThis widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.\n\n\n\n\n\n\nStreams\n\n\nThis operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.\n\n\n\n\n\n\nMetrics Chart\n\n\nThis widget displays moving averages of tuples processed and latencies.\n\n\n\n\n\n\nphysical\n\n\nThe physical tab displays, in addition to \nApplication Overview\n\nand \nMetrics Chart\n, 2 more widgets:\n\n\n\n\n\n\n\n\nPhysical Operators\n\n\nThis widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.\n\n\n\n\n\n\nContainers\n\n\nThis widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.\n\n\n\n\n\n\nphysical-dag-view\n\n\nThe physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:\n\n\n\n\nmetric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nMonitor Sales Dimension using the Monitor menu\n\n\nTo monitor the application\n\n\n\n\nClick \nMonitor\n on the menu bar to open the logical view of the DAG.\n    \n\n\nEnsure that the \nState\n is \nRunning\n, indicating that the application\n    is launched successfully.\n\n\nUnder \nStramEvents\n, ensure that the operators from within the\n    application have started.\n\n\nClick \nphysical\n tab to open the physical view.\n\n\n\n\nEnsure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n    \n\n\nNote: This is because we set the corresponding stream locality to\n\nCONTAINER_LOCAL\n earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.\n\n\n\n\n\n\nCreate additional tabs\n\n\nYou can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the \nApplication\nOverview\n widget.\n\n\nTo create additional tabs\n\n\n\n\nNext to the \nmetric-view\n tab, look for the plus sign (+) button.\n\n\nClick this button to create an additional tab.\n\n\nProvide a name for your tab.\n\n\nAdd widgets to your tab.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#monitoring-the-sales-dimension-application-using-dtmanage", 
            "text": "Recall that after the application is built and validated, it can be\nlaunched from the  App Packages  page as described in an earlier chapter;\napplications built with  dtAssemble  can also, optionally, be launched\nfrom the  Application Canvas  page as described earlier. This section\ndescribes how you can monitor the running Sales Dimension application\nusing  dtManage .", 
            "title": "Monitoring the Sales Dimension application using dtManage"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#the-monitor-menu-option", 
            "text": "You can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click  Monitor , you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.  logical  This image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the  +  button at the top-left corner and choosing\nfrom the resulting dropdown list.     Application Overview  This widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.    StramEvents  This widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.    Logical DAG  This widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.  To include additional details   Click an operator for which you want to display additional details.  To display a detail on the top of this operator representation,\n    click the Top list, and select a metric.  To display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.     Logical Operators  This widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.    Streams  This operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.    Metrics Chart  This widget displays moving averages of tuples processed and latencies.    physical  The physical tab displays, in addition to  Application Overview \nand  Metrics Chart , 2 more widgets:     Physical Operators  This widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.    Containers  This widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.    physical-dag-view  The physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:   metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "The Monitor menu option"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#monitor-sales-dimension-using-the-monitor-menu", 
            "text": "To monitor the application   Click  Monitor  on the menu bar to open the logical view of the DAG.\n      Ensure that the  State  is  Running , indicating that the application\n    is launched successfully.  Under  StramEvents , ensure that the operators from within the\n    application have started.  Click  physical  tab to open the physical view.   Ensure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n      Note: This is because we set the corresponding stream locality to CONTAINER_LOCAL  earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.", 
            "title": "Monitor Sales Dimension using the Monitor menu"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#create-additional-tabs", 
            "text": "You can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the  Application\nOverview  widget.  To create additional tabs   Next to the  metric-view  tab, look for the plus sign (+) button.  Click this button to create an additional tab.  Provide a name for your tab.  Add widgets to your tab.", 
            "title": "Create additional tabs"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.\n\n\nMicrosoft Windows\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:\n\n\n\n\n\n\ngit\n -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nVirtualBox\n -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from \nhttps://www.virtualbox.org/wiki/Downloads\n. It is needed to run the DataTorrent Sandbox.\n\n\n\n\n\n\nDataTorrent Sandbox\n -- The sandbox can be downloaded from \nhttps://www.datatorrent.com/download\n. It is useful for testing simple applications since it contains Apache Hadoop and DataTorrent RTS pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.\n\n\n\n\n\n\n(Optional) If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\Program Files\\Java\\jdk1.7.0\\_80\\bin\n; for \ngit\n it might be \nC:\\Program Files\\Git\\bin\n; and for maven it might be \nC:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\n. Open a console window and enter the command:\n\n\necho %PATH%\n\n\n\nto see the value of the \nPATH\n variable and verify that the above directories are present. If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nNow run the following commands and ensure that the output is something similar to that shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\nMaven home: C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\\..\n\n\nJava version: 1.7.0_80, vendor: Oracle Corporation\n\n\nJava home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre\n\n\nDefault locale: en_US, platform encoding: Cp1252\n\n\nOS name: \nwindows 8\n, version: \n6.2\n, arch: \namd64\n, family: \nwindows\n\n\n\n\n\n\n\n\n\nInstalling the Sandbox\n\n\nThe sandbox includes, as noted above, a complete, stand-alone, instance of the\nDatatorrent RTS Enterprise Edition configured as a single-node Hadoop cluster. Please\nsee \nDataTorrent RTS Sandbox\n for details on setting up the sandbox.\n\n\nYou can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g. \njdk\n, \ngit\n, \nmaven\n) are pre-installed and also the package files created by your project are directly available to the DataTorrent tools such as  \ndtManage\n and \nApex CLI\n. The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.\n\n\nCreating a new Project\n\n\nYou can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example, \nnewapp.cmd\n and run it (the\nvalue for \narchetypeVersion\n can be a more recent version if available):\n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn -B archetype:generate ^\n  -DarchetypeGroupId=org.apache.apex ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.6.0-SNAPSHOT ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\nThe caret (^) at the end of some lines indicates that a continuation line follows.\n\n\nThis command will eventually become outdated, check the \napex-app-archetype README\n\nfor the latest version, which includes the most recent archetypeVersion.\n\n\nYou can also, if you prefer, use an IDE to generate the project as described in\n\nCreating a New Apache Apex Project with your IDE\n.\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the appropriate maven command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa\n. This file can then be uploaded to the DataTorrent GUI tool (\ndtManage\n) on your cluster if you have DataTorrent RTS installed there, or on the sandbox and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string  \nhello world:\n.\n\n\nIf you built this package on the host, you can transfer it to the sandbox using either a shared folder or the \npscp\n tool bundled with \nPuTTY\n mentioned earlier.\n\n\nYou can also run this application from the generated unit test file as described in the\nnext section.\n\n\nRunning Unit Tests\n\n\nTo run unit tests on Linux or OSX, simply run the usual maven command, for example: \nmvn test\n\nto run all tests or \nmvn -Dcom.example.myapexapp.ApplicationTest#testApplication test\n to run\na selected test. For the default application generated from the archetype, it should\nprint output like this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\nOn Windows, an additional file, \nwinutils.exe\n, is required; download it from\n\nhttps://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip\n\nand unpack the archive to, say, \nC:\\hadoop\n; this file should be present under\n\nhadoop-common-2.2.0-bin-master\\bin\n within it.\n\n\nSet the \nHADOOP_HOME\n environment variable system-wide to\n\nc:\\hadoop\\hadoop-common-2.2.0-bin-master\n as described at:\n\nhttps://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true\n. You should now be able to run unit tests normally.\n\n\nIf you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty \nhadoop.home.dir\n:\n\n\nmvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test\n\n\n\nor set the environment variable separately:\n\n\nset HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test\n\n\n\nWithin your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:\n\n\nEnv.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master\n\n\n\nat \nProperties \n Actions \n Run project \n Set Properties\n.\n\n\nSimilarly, in Eclipse (Mars) add it to the\nproject properties at \nProperties\n \n \nRun/Debug Settings\n \n \nApplicationTest\n\n\n \nEnvironment\n tab.\n\n\nBuilding the Sources\n\n\nThe Apache Apex source code is useful to have locally for a variety of reasons:\n- It has more substantial demo applications which can serve as models for new\n  applications in the same or similar domain.\n- When extending a class, it is helpful to refer to the base class implementation\n  of overrideable methods.\n- The maven build file \npom.xml\n can be a useful model when you need to add or delete\n  plugins, dependencies, profiles, etc.\n- Browsing the code is a good way to gain a deeper understanding of the platform.\n\n\nYou can download and build the source repositories by running the script \nbuild-apex.cmd\n\nlocated in the same place in the examples repository described above. Alternatively, if\nyou do not want to use the script, you can follow these simple manual steps:\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\npushd incubator-apex-core\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\nmvn clean install -DskipTests\npopd\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your\nlocal maven repository (typically \n.m2/repository\n under your home directory), and\n\nnot\n to the system directories, so \nAdministrator\n privileges are not required.\nThe  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see application package files in the \ntarget\n\ndirectories under each module; for example, the \nPi Demo\n package file\n\npi-demo-3.2.1-incubating-SNAPSHOT.apa\n should be under\n\nincubator-apex-malhar\\demos\\pi\\target\n.\n\n\nLinux\n\n\nMost of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.\n\n\nThe pre-requisites (such as \ngit\n, \nmaven\n, etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is: \necho $PATH\n).\n\n\nThe maven archetype command is the same except that continuation lines use a backslash (\n\\\n) instead of caret (\n^\n); the script for it is available in the same location and is named \nnewapp\n (without the \n.cmd\n extension). The script to checkout and build the Apache Apex repositories is named \nbuild-apex\n.", 
            "title": "Apex Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#microsoft-windows", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:    git  -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    VirtualBox  -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from  https://www.virtualbox.org/wiki/Downloads . It is needed to run the DataTorrent Sandbox.    DataTorrent Sandbox  -- The sandbox can be downloaded from  https://www.datatorrent.com/download . It is useful for testing simple applications since it contains Apache Hadoop and DataTorrent RTS pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.    (Optional) If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like  java  and  javac , the directory might be something like  C:\\Program Files\\Java\\jdk1.7.0\\_80\\bin ; for  git  it might be  C:\\Program Files\\Git\\bin ; and for maven it might be  C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin . Open a console window and enter the command:  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories are present. If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Now run the following commands and ensure that the output is something similar to that shown in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  Maven home: C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\\..  Java version: 1.7.0_80, vendor: Oracle Corporation  Java home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre  Default locale: en_US, platform encoding: Cp1252  OS name:  windows 8 , version:  6.2 , arch:  amd64 , family:  windows", 
            "title": "Microsoft Windows"
        }, 
        {
            "location": "/apex_development_setup/#installing-the-sandbox", 
            "text": "The sandbox includes, as noted above, a complete, stand-alone, instance of the\nDatatorrent RTS Enterprise Edition configured as a single-node Hadoop cluster. Please\nsee  DataTorrent RTS Sandbox  for details on setting up the sandbox.  You can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g.  jdk ,  git ,  maven ) are pre-installed and also the package files created by your project are directly available to the DataTorrent tools such as   dtManage  and  Apex CLI . The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.", 
            "title": "Installing the Sandbox"
        }, 
        {
            "location": "/apex_development_setup/#creating-a-new-project", 
            "text": "You can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example,  newapp.cmd  and run it (the\nvalue for  archetypeVersion  can be a more recent version if available):  @echo off\n@rem Script for creating a new application\nsetlocal\nmvn -B archetype:generate ^\n  -DarchetypeGroupId=org.apache.apex ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.6.0-SNAPSHOT ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal  The caret (^) at the end of some lines indicates that a continuation line follows.  This command will eventually become outdated, check the  apex-app-archetype README \nfor the latest version, which includes the most recent archetypeVersion.  You can also, if you prefer, use an IDE to generate the project as described in Creating a New Apache Apex Project with your IDE .  When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the appropriate maven command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa . This file can then be uploaded to the DataTorrent GUI tool ( dtManage ) on your cluster if you have DataTorrent RTS installed there, or on the sandbox and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string   hello world: .  If you built this package on the host, you can transfer it to the sandbox using either a shared folder or the  pscp  tool bundled with  PuTTY  mentioned earlier.  You can also run this application from the generated unit test file as described in the\nnext section.", 
            "title": "Creating a new Project"
        }, 
        {
            "location": "/apex_development_setup/#running-unit-tests", 
            "text": "To run unit tests on Linux or OSX, simply run the usual maven command, for example:  mvn test \nto run all tests or  mvn -Dcom.example.myapexapp.ApplicationTest#testApplication test  to run\na selected test. For the default application generated from the archetype, it should\nprint output like this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  On Windows, an additional file,  winutils.exe , is required; download it from https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip \nand unpack the archive to, say,  C:\\hadoop ; this file should be present under hadoop-common-2.2.0-bin-master\\bin  within it.  Set the  HADOOP_HOME  environment variable system-wide to c:\\hadoop\\hadoop-common-2.2.0-bin-master  as described at: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true . You should now be able to run unit tests normally.  If you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty  hadoop.home.dir :  mvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test  or set the environment variable separately:  set HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test  Within your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:  Env.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master  at  Properties   Actions   Run project   Set Properties .  Similarly, in Eclipse (Mars) add it to the\nproject properties at  Properties     Run/Debug Settings     ApplicationTest    Environment  tab.", 
            "title": "Running Unit Tests"
        }, 
        {
            "location": "/apex_development_setup/#building-the-sources", 
            "text": "The Apache Apex source code is useful to have locally for a variety of reasons:\n- It has more substantial demo applications which can serve as models for new\n  applications in the same or similar domain.\n- When extending a class, it is helpful to refer to the base class implementation\n  of overrideable methods.\n- The maven build file  pom.xml  can be a useful model when you need to add or delete\n  plugins, dependencies, profiles, etc.\n- Browsing the code is a good way to gain a deeper understanding of the platform.  You can download and build the source repositories by running the script  build-apex.cmd \nlocated in the same place in the examples repository described above. Alternatively, if\nyou do not want to use the script, you can follow these simple manual steps:    Check out the source code repositories:  git clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar    Switch to the appropriate release branch and build each repository:  pushd incubator-apex-core\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\nmvn clean install -DskipTests\npopd    The  install  argument to the  mvn  command installs resources from each project to your\nlocal maven repository (typically  .m2/repository  under your home directory), and not  to the system directories, so  Administrator  privileges are not required.\nThe   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see application package files in the  target \ndirectories under each module; for example, the  Pi Demo  package file pi-demo-3.2.1-incubating-SNAPSHOT.apa  should be under incubator-apex-malhar\\demos\\pi\\target .", 
            "title": "Building the Sources"
        }, 
        {
            "location": "/apex_development_setup/#linux", 
            "text": "Most of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.  The pre-requisites (such as  git ,  maven , etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is:  echo $PATH ).  The maven archetype command is the same except that continuation lines use a backslash ( \\ ) instead of caret ( ^ ); the script for it is available in the same location and is named  newapp  (without the  .cmd  extension). The script to checkout and build the Apache Apex repositories is named  build-apex .", 
            "title": "Linux"
        }, 
        {
            "location": "/configure_IDE/", 
            "text": "Creating a New Apache Apex Project with your IDE\n\n\nWe describe the process for creating a new Apache Apex project for three\ncommon IDEs: \nIntelliJ IDEA\n, \nEclipse\n and \nNetBeans\n\n\nIntelliJ IDEA\n\n\nThe \nIntelliJ IDEA\n is available at \nhttps://www.jetbrains.com/idea/\n.\n\n\nFirst make sure you have the \nMaven Integration\n plugin enabled in the list at\n\nFile\n \n  Settings \n \nPlugins\n.\n\n\nNow, select \nFile\n \n \nNew\n \n \nProject\n. Choose \nMaven\n in the left pane\nand check \nCreate from archetype\n in the dialog box; at this point, you should be\nable to expand the \norg.apache.apex:apex-app-archetype\n element in the center pane and\nselect a suitable version as shown below:\n\n\n\n\nIf the \norg.apache.apex:apex-app-archetype\n element in not present in the center pane,\nyou can click the \nAdd Archetype...\n button and fill out the \nGroup ID\n, \nArtifact ID\n,\nand \nVersion\n entries, (leave \nRepository\n blank), as shown below:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nGroup ID\n\n\norg.apache.apex\n\n\n\n\n\n\nArtifact ID\n\n\napex-app-archetype\n\n\n\n\n\n\nVersion\n\n\n3.2.0-incubating (or any later version)\n\n\n\n\n\n\n\n\nClick \nOK\n. The archetype will appear in the list, selected. Note that this\n\nAdd Archetype...\n step is only required the first time you use the archetype; thereafter,\nyou can select the archetype directly.\n\n\nClick \nNext\n, and fill out the rest of the required information. For example:\n\n\n\n\nClick \nNext\n, and verify the information shown on the next screen (if you have a more\nrecent version of Maven installed, enter its home directory):\n\n\n\n\nClick \nNext\n, and fill out the project name and location: \n\n\n\n\nClick \nFinish\n, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within the IDEA.\n\n\nEclipse\n\n\nThe \nEclipse\n IDE is downloadable from \nhttps://eclipse.org/downloads/\n.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen Eclipse.\n\n\nSelect \nFile\n \n \nNew\n \n \nProject...\n \n\n    \nMaven\n \n \nMaven Project\n and click \nNext\n.\n    \n\n\nClick \nNext\n on the next dialog as well; you should now see a window\n    where you can configure archetype catalogs:\n    \n\n\nFrom the \nCatalog\n dropdown select a suitable remote catalog if one is present\n    and enter \napex\n in the \nFilter\n input box; you should see one or more entries\n    in the center pane with \nGroup Id\n of \norg.apache.apex\n and an \nArtifact Id\n\n    of \napex-app-archetype\n:\n    \n\n\nIf a suitable remote catalog is not present, you'll need to add it by clicking\n    the \nConfigure\n button to see a new dialog that shows a list of catalogs in\n    the middle pane and a \nAdd Remote Catalog\n button on the right:\n    \n\n\nClick that button to get a dialog where you can enter details of a new\n    catalog and enter \nhttp://repo.maven.apache.org/maven2/archetype-catalog.xml\n\n    for the \nCatalog File\n entry and suitable text (such as \nApache Catalog\n)\n    for the \nDescription\n entry:\n    \n\n\nIn either case, you should now be able to select the \napex-app-archetype\n\n    entry, click \nNext\n to see a window where you can enter details of the new\n    project and enter values similar to those in the table below (you'll need\n    to replace the default value \n${archetypeVersion}\n of \narchetypeVersion\n\n    with a suitable concrete version number like \n3.3.0-incubating\n):\n    \n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \ncom.example\n\n  \n\n  \n\n  \nArtifact ID\n\n  \nTestApex\n\n  \n\n  \n\n  \nVersion\n\n  \n0.0.1-SNAPSHOT\n\n  \n\n  \n\n  \nPackage\n\n  \ncom.example.TestApex\n\n  \n\n  \n\n  \narchetypeVersion\n\n  \n3.3.0-incubating\n\n  \n\n  \n\n  \n\n\n\n\nClick Finish; you should see the new project in your Package Explorer\n\n\n\n\nNetBeans\n\n\nThe \nNetBeans\n IDE is downloadable from \nhttps://netbeans.org/downloads/\n.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen NetBeans.\n\n\nClick \nFile\n \n \nNew Project\n.\n\n\n\n\nFrom the \nCategories\n column select \nMaven\n and from the \nProjects\n column,\n    select \nProject from Archetype\n, and click \nNext\n.\n    \n\n\n\n\n\n\nOn the Maven Archetype window, type \napex\n in the \nSearch\n box, and\n     from the list of \nKnown Archetypes\n, select \napex-app-archetype\n.\n     \n\n\n\n\nMake sure that the values for the fields match the values shown in this\n     table (except that the archetype version may be more recent):\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \norg.apache.apex\n\n  \n\n  \n\n  \nArtifact ID\n\n  \napex-app-archetype\n\n  \n\n  \n\n  \nVersion\n\n  \n3.3.0-incubating\n\n  \n\n  \n\n  \nRepository\n\n  \n/maven/content/repositories/releases\n\n  \n\n  \n\n  \n\n\n\n\nClick Next.\n\n\n\n\nOn the \nName and Location\n window, do the following:\n\n\n\n\nEnter a name for this project in the \nProject Name\n box, for example,\n    \nTestApex\n.\n\n\nEnter a location for this project in the \nProject Location\n box, for\n     example, \n/home/dtadmin/NetBeansProjects\n.\n\n\nEnter an ID in the \nGroup Id\n box, for example, \ncom.example\n.\n\n\nEnter a version for this project in the \nVersion\n box, for example,\n     \n1.0-SNAPSHOT\n.\n\n\nEnter the package name in the \nPackage\n box, for example,\n      \ncom.example.testapex\n.\n\n\n\n\n\n\n\n\n\n\nClick Finish.\n\n\n\n\n\n\nThe project is generated at the specified location and should be visible in\nthe left panel with the name \nMy Apex Application\n. You can right-click the\nproject and choose \nRename\n to provide a more descriptive name if you wish.", 
            "title": "Generate New Project in IDE"
        }, 
        {
            "location": "/configure_IDE/#creating-a-new-apache-apex-project-with-your-ide", 
            "text": "We describe the process for creating a new Apache Apex project for three\ncommon IDEs:  IntelliJ IDEA ,  Eclipse  and  NetBeans", 
            "title": "Creating a New Apache Apex Project with your IDE"
        }, 
        {
            "location": "/configure_IDE/#intellij-idea", 
            "text": "The  IntelliJ IDEA  is available at  https://www.jetbrains.com/idea/ .  First make sure you have the  Maven Integration  plugin enabled in the list at File     Settings    Plugins .  Now, select  File     New     Project . Choose  Maven  in the left pane\nand check  Create from archetype  in the dialog box; at this point, you should be\nable to expand the  org.apache.apex:apex-app-archetype  element in the center pane and\nselect a suitable version as shown below:   If the  org.apache.apex:apex-app-archetype  element in not present in the center pane,\nyou can click the  Add Archetype...  button and fill out the  Group ID ,  Artifact ID ,\nand  Version  entries, (leave  Repository  blank), as shown below:      Field  Value      Group ID  org.apache.apex    Artifact ID  apex-app-archetype    Version  3.2.0-incubating (or any later version)     Click  OK . The archetype will appear in the list, selected. Note that this Add Archetype...  step is only required the first time you use the archetype; thereafter,\nyou can select the archetype directly.  Click  Next , and fill out the rest of the required information. For example:   Click  Next , and verify the information shown on the next screen (if you have a more\nrecent version of Maven installed, enter its home directory):   Click  Next , and fill out the project name and location:    Click  Finish , and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within the IDEA.", 
            "title": "IntelliJ IDEA"
        }, 
        {
            "location": "/configure_IDE/#eclipse", 
            "text": "The  Eclipse  IDE is downloadable from  https://eclipse.org/downloads/ .  Generate a new Maven archetype project as follows:   Open Eclipse.  Select  File     New     Project...   \n     Maven     Maven Project  and click  Next .\n      Click  Next  on the next dialog as well; you should now see a window\n    where you can configure archetype catalogs:\n      From the  Catalog  dropdown select a suitable remote catalog if one is present\n    and enter  apex  in the  Filter  input box; you should see one or more entries\n    in the center pane with  Group Id  of  org.apache.apex  and an  Artifact Id \n    of  apex-app-archetype :\n      If a suitable remote catalog is not present, you'll need to add it by clicking\n    the  Configure  button to see a new dialog that shows a list of catalogs in\n    the middle pane and a  Add Remote Catalog  button on the right:\n      Click that button to get a dialog where you can enter details of a new\n    catalog and enter  http://repo.maven.apache.org/maven2/archetype-catalog.xml \n    for the  Catalog File  entry and suitable text (such as  Apache Catalog )\n    for the  Description  entry:\n      In either case, you should now be able to select the  apex-app-archetype \n    entry, click  Next  to see a window where you can enter details of the new\n    project and enter values similar to those in the table below (you'll need\n    to replace the default value  ${archetypeVersion}  of  archetypeVersion \n    with a suitable concrete version number like  3.3.0-incubating ):\n       \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   com.example \n   \n   \n   Artifact ID \n   TestApex \n   \n   \n   Version \n   0.0.1-SNAPSHOT \n   \n   \n   Package \n   com.example.TestApex \n   \n   \n   archetypeVersion \n   3.3.0-incubating \n   \n   \n     Click Finish; you should see the new project in your Package Explorer", 
            "title": "Eclipse"
        }, 
        {
            "location": "/configure_IDE/#netbeans", 
            "text": "The  NetBeans  IDE is downloadable from  https://netbeans.org/downloads/ .  Generate a new Maven archetype project as follows:   Open NetBeans.  Click  File     New Project .   From the  Categories  column select  Maven  and from the  Projects  column,\n    select  Project from Archetype , and click  Next .\n        On the Maven Archetype window, type  apex  in the  Search  box, and\n     from the list of  Known Archetypes , select  apex-app-archetype .\n        Make sure that the values for the fields match the values shown in this\n     table (except that the archetype version may be more recent):   \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   org.apache.apex \n   \n   \n   Artifact ID \n   apex-app-archetype \n   \n   \n   Version \n   3.3.0-incubating \n   \n   \n   Repository \n   /maven/content/repositories/releases \n   \n   \n     Click Next.   On the  Name and Location  window, do the following:   Enter a name for this project in the  Project Name  box, for example,\n     TestApex .  Enter a location for this project in the  Project Location  box, for\n     example,  /home/dtadmin/NetBeansProjects .  Enter an ID in the  Group Id  box, for example,  com.example .  Enter a version for this project in the  Version  box, for example,\n      1.0-SNAPSHOT .  Enter the package name in the  Package  box, for example,\n       com.example.testapex .      Click Finish.    The project is generated at the specified location and should be visible in\nthe left panel with the name  My Apex Application . You can right-click the\nproject and choose  Rename  to provide a more descriptive name if you wish.", 
            "title": "NetBeans"
        }, 
        {
            "location": "/application_development/", 
            "text": "Application Developer Guide\n\n\nReal-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.\n\n\nThe DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.\n\n\nDataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.\n\n\nIn the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called \nOperators\n interconnected\nby the data-flow edges called  \nStreams\n.\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the \nOperator Development Guide\n.\n\n\nRunning A Test Application\n\n\nThis chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:\n\n\n\n\nOpen up platform files in your IDE (for example NetBeans, or Eclipse)\n\n\nOpen Demos project\n\n\nOpen Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package\n\n\nSee the results in your system console\n\n\n\n\nCongratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.\n\n\n// Generates random numbers\nRandomEventGenerator rand = dag.addOperator(\nrand\n, new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of \nx\n and \ny\n\nRoundRobinHashMap\nString,Object\n rrhm = dag.addOperator(\nrrhm\n, new RoundRobinHashMap\nString, Object\n());\nrrhm.setKeys(new String[] { \nx\n, \ny\n });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator(\npicalc\n, new Script());\ncalc.setPassThru(false);\ncalc.put(\ni\n,0);\ncalc.put(\ncount\n,0);\ncalc.addSetupScript(\nfunction pi() { if (x*x+y*y \n= \n+maxValue*maxValue+\n) { i++; } count++; return i / count * 4; }\n);\ncalc.setInvoke(\npi\n);\ndag.addStream(\nrand_rrhm\n, rand.integer_data, rrhm.data);\ndag.addStream(\nrrhm_calc\n, rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\ndag.addStream(\nrand_console\n,calc.result, console.input);\n\n\n\n\nYou can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.\n\n\nTest Application: Yahoo! Finance Quotes\n\n\nThe PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from  \nYahoo! Finance\n \u00a0and computes the\nfollowing for four tickers, namely \nIBM\n,\n\nGOOG\n, \nYHOO\n.\n\n\n\n\nQuote: Consisting of last trade price, last trade time, and\n    total volume for the day\n\n\nPer-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute\n\n\nSimple Moving Average: trade price over 5 minutes\n\n\n\n\nTotal volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.\n\n\n\n\nThe operator StockTickerInput:\u00a0StockTickerInput\n\u00a0\nis\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:\n\n\n$ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1'\n\nIBM\n,203.966,1513041,\n1:43pm\n\n\nGOOG\n,762.68,1879741,\n1:43pm\n\n\nAAPL\n,444.3385,11738366,\n1:43pm\n\n\nYHOO\n,19.3681,14707163,\n1:43pm\n\n\n\n\n\nAmong all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.\n\n\nHere is the class implementation for StockTickInput:\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap\nString, Long\n lastVolume = new HashMap\nString, Long\n();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Double\n price = new DefaultOutputPort\nKeyValPair\nString, Double\n();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Long\n volume = new DefaultOutputPort\nKeyValPair\nString, Long\n();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, String\n time = new DefaultOutputPort\nKeyValPair\nString, String\n();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str = \nhttp://download.finance.yahoo.com/d/quotes.csv?s=\n;\n    for (int i = 0; i \n symbols.length; i++) {\n      if (i != 0) {\n        str += \n,\n;\n      }\n      str += symbols[i];\n    }\n    str += \nf=sl1vt1\ne=.csv\n;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter(\nhttp.protocol.cookie-policy\n, CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println(\nMethod failed: \n + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List\nString[]\n myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList\nString\n tuple = new ArrayList\nString\n(Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is \nSymbol\n,\nPrice\n,\nVolume\n,\nTime\n\n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol \n 0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair\nString, Double\n(symbol, currentPrice));\n            volume.emit(new KeyValPair\nString, Long\n(symbol, vol));\n            time.emit(new KeyValPair\nString, String\n(symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}\n\n\n\n\nThe operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.\n\n\nImportant: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.\n\n\nThe method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.\n\n\nMethod\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.\n\n\nNote that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.\n\n\nThe operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal\nK,V\n\u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal\nString,Long\n, where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if cumulative was set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.\n\n\nThe operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal\nK\n\u00a0from the\nstream\u00a0package.\n\n\nThe operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal\nK,V\n\u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal\nString,Double\n.\n\n\nThe operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal\nString,Long\n, but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.\n\n\nThe operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.\n\n\nThe operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage\nString,Double\n, which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.\n\n\nThe operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.\n\n\nConnecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo. \np\n\n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal\nString, Long\n getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal\nString, Long\n getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal\nString, Double\n getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal\nString, Double\n oper = dag.addOperator(name, new RangeKeyVal\nString, Double\n());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal\nString,Double,Long,String,?,?\n getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,Double,Long,String,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,HighLow,Long,Object,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage\nString, Double\n getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage\nString, Double\n oper = dag.addOperator(name, new SimpleMovingAverage\nString, Double\n());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort\nObject\n getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix + \n: %s\n);\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n       DefaultPartitionCodec\nString, Double\n codec = new DefaultPartitionCodec\nString, Double\n();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n\n}\n\n\n\n\nNote that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.\n\n\nIn the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the \nInstallation Guide\n.\n\n\nRunning a Test Application\n\n\nWe will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).\n\n\nThe platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.\n\n\nThe instructions below assume that the platform was installed in a\ndirectory \nINSTALL_DIR\n and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\n\nlocal mode\n\u00a0\n(in IDE or from command line) or on a  \nHadoop cluster\n \n.\n\n\nTo start the Apex CLI run\n\n\nINSTALL_DIR\n/bin/apex\n\n\n\nThe command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)\n\n\napex\n launch -local \nINSTALL_DIR\n/yahoo-finance-demo-3.4.0.apa\n\n\n\nTo terminate the application in local mode, enter Ctrl-C\n\n\nTu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)\n\n\napex\n launch \nINSTALL_DIR\n/yahoo-finance-demo-3.4.0.apa\n\n\n\nTo stop the application running in Hadoop, terminate it in the Apex CLI:\n\n\napex\n kill-app\n\n\n\nExecuting the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.\n\n\nLocal Mode\n\n\nIn local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.\n\n\nHadoop Cluster\n\n\nIn this section we discuss various Hadoop cluster setups.\n\n\nSingle Node Cluster\n\n\nIn a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.\n\n\nIn this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.\n\n\nMulti-Node Cluster\n\n\nIn a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.\n\n\nBefore you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.\n\n\n\n\nApache Apex Platform Overview\n\n\nStreaming Computational Model\n\n\nIn this chapter, we describe the the basics of the real-time streaming platform and its computational model.\n\n\nThe platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .\n\n\nApplications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.\n\n\nThe streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.\n\n\nA fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0\n\n\nThis atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.\n\n\nThe platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.\n\n\nNote that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.\n\n\n\n\nAlongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave  \nproperties\n\n\n\u00a0\nthat can be set to specify the\ndesired computation. Those interested in details, should refer to\n\nApex Malhar Operator Library\n\n.\n\n\nThe platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.\n\n\nA streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.\n\n\nAn operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include\n\n\n\n\n\n\nRunning the Application\n\n\n\n\nRead the\u00a0logical plan\u00a0of the application (DAG) submitted by the client\n\n\nValidate the logical plan\n\n\nTranslate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.\n\n\nRequest resources (Hadoop containers) from Resource Manager,\n    per physical plan\n\n\nBased on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.\n\n\nExecutes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.\n\n\n\n\n\n\n\n\nContinually monitoring the application via heartbeats from each StreamingContainer\n\n\n\n\nCollecting Application System Statistics and Logs\n\n\nLogging all application-wide decisions taken\n\n\nProviding system data on the state of the application via a  Web Service.\n\n\n\n\nSupporting \nFault Tolerance\n\n\na.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper\n\n\n\n\n\n\nSupporting \nDynamic\n    Partitioning\n:\n\u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).\n\n\n\n\nEnabling \nSecurity\n:\n\u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.\n\n\nEnabling \nDynamic  modification\n\u00a0\nof\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.\n\n\n\n\nAn example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.\n\n\n\n\nAn example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.\n\n\n\n\nHadoop Components\n\n\nIn this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.\n\n\nA streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future\n\n\nAll investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.\n\n\nYARN\n\n\nYARN\n is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through YARN's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).\n\n\nResource Manager (RM)\n\n\nResourceManager\n(RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.\n\n\nApplication Master (AM)\n\n\nThe AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.\n\n\nNode Managers (NM)\n\n\nThere is one \nNodeManager\n(NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.\n\n\nRPC Protocol\n\n\nCommunication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.\n\n\nHDFS\n\n\nHadoop includes a highly fault tolerant, high throughput\ndistributed file system (\nHDFS\n).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.\n\n\nDeveloping An Application\n\n\nIn this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.\n\n\nDevelopment Process\n\n\nWhile the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.\n\n\nDesign\n\n\n\n\nIdentify common, reusable operators. Use a library\n    if possible.\n\n\nIdentify scalability and performance requirements before\n    designing the DAG.\n\n\nLeverage attributes that the platform supports for scalability\n    and performance.\n\n\nUse operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.\n\n\nUse THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completely. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.\n\n\nThe overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.\n\n\nDo not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.\n\n\nPersist key information to HDFS if possible; it may be useful\n    for debugging later.\n\n\nDecide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.\n\n\n\n\nCreating New Project\n\n\nPlease refer to the \nApex Application Packages\n\u00a0for\nthe basic steps for creating a new project.\n\n\nWriting the application code\n\n\nPreferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.\n\n\nTesting\n\n\nWrite test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.\n\n\nGood test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)\n\n\nRunning an application\n\n\nThe platform provides a command line tool called Apex CLI (apex)\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.\n\n\nApex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.\n\n\nFor more details on CLI please refer to the \nApex CLI Guide\n.\n\n\nApplication API\n\n\nThis section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are\n\n\n\n\n\n\nInstantiate an application (DAG)\n\n\n\n\n\n\n(Optional) Set Attributes\n\n\n\n\nAssign application name\n\n\nSet any other attributes as per application requirements\n\n\n\n\n\n\n\n\nCreate/re-use and instantiate operators\n\n\n\n\nAssign operator name that is unique within the  application\n\n\nDeclare schema upfront for each operator (and thereby its  \nports\n)\n\n\n(Optional) Set \nproperties\n\u00a0\n and \nattributes\n\u00a0\n on the dag as per specification\n\n\nConnect ports of operators via streams\n\n\nEach stream connects one output port of an operator to one or  more input ports of other operators.\n\n\n(Optional) Set attributes on the streams\n\n\n\n\n\n\n\n\n\n\n\n\nTest the application.\n\n\n\n\n\n\nThere are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.\n\n\nJava API\n\n\nThe Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.\n\n\nThe developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.\n\n\nLet us revisit how the Yahoo! Finance test application constructs the DAG:\n\n\npublic class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n}\n\n\n\n\nProperty File API\n\n\nThe platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.\n\n\nCreate an application (DAG): myApplication.properties\n\n\n# input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort\n\n\n\n\nAbove snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.\n\n\nAttributes\n\n\nAttributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.\n\n\nOperators\n\n\nOperators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the  \nOperator Developer Guide\n. As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.\n\n\nAll operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.\n\n\nEach operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith Apex CLI\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.\n\n\nOperator Interface\n\n\nOperator interface in a DAG consists of \nports\n,\n\u00a0\nproperties\n,\n\u00a0and\n \nattributes\n\n\n.\n\u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.\n\n\nPorts\n\n\nPorts are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.\n\n\nHere are examples of an input and an output port from the operator\nSum.\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort\nV\n data = new DefaultInputPort\nV\n() {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort\nV\n sum = new DefaultOutputPort\nV\n(){ \u2026 };\n\n\n\n\nThe process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.\n\n\nThere is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.\n\n\nPort connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.\n\n\nAttributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe \nParallel\nPartitions\n\u00a0\nsection.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.\n\n\nProperties\n\n\nProperties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.\n\n\nAll non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.\n\n\nAttributes\n\n\nAttributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in  \nConfiguration Guide\n.\n\n\nOperator State\n\n\nThe state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe \ncheckpointed\n\u00a0\nevery\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.\n\n\nThe distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.\n\n\nStateless\n\n\nA Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.\n\n\nStateful\n\n\nA Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.\n\n\nOperator API\n\n\nThe Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.\n\n\nThe APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.\n\n\nIn the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.\n\n\nStreaming Window\n\n\nStreaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows\n\n\npublic void process(\ntuple_type\n tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown\n\n\n\n\nA tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.\n\n\nAggregate Application Window\n\n\nAn operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.\n\n\nSliding Application Window\n\n\nA sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.\n\n\nSingle vs Multi-Input Operator\n\n\nA single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.\n\n\nRecovery Mechanisms\n\n\nApplication developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely\n\n\n\n\nAt-least-once: All atomic batches are processed at least once.\n    No data loss occurs.\n\n\nAt-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.\n\n\nExactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.\n\n\n\n\nAt-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.\n\n\nRecovery mechanisms can be specified per Operator while writing\nthe application as shown below.\n\n\nOperator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);\n\n\n\n\nAlso note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.\n\n\nDetails are explained in the chapter on Fault Tolerance\nbelow\n.\n\n\nStreams\n\n\nA stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics\n\n\n\n\nTuples are always delivered in the same order in which they\n    were emitted.\n\n\nConsists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.\n\n\nA stream that connects two containers passes through a\n    buffer server.\n\n\nAll streams can be persisted (by default in HDFS).\n\n\nExactly one output port writes to the stream.\n\n\nCan be read by one or more input ports.\n\n\nConnects operators within an application, not outside\n    an application.\n\n\nHas an unique name within an application.\n\n\nHas attributes which act as hints to STRAM.\n\n\n\n\nStreams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:\n\n\n\n\nTHREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.\n\n\nCONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.\n\n\nNODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.\n\n\nRACK_LOCAL: On nodes in the same rack; also called\n    in-rack.\n\n\nunspecified: No guarantee. Could be anywhere within the\n    cluster\n\n\n\n\n\n\n\n\nAn example of a stream declaration is given below\n\n\nDAG dag = new DAG();\n \u2026\ndag.addStream(\nviews\n, viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality\n\n\n\n\nThe platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.\n\n\nIn a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of  \nstandard operator template\nlibrary\n \n\u00a0\nfollow\nthese principles.\n\n\nA logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.\n\n\nModes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.\n\n\nTHREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.\n\n\nValidating an Application\n\n\nThe platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely\n\n\n\n\nCompile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.\n\n\nInitialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.\n\n\nRun Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.\n\n\n\n\nCompile Time\n\n\nCompile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include\n\n\n\n\nSchema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.\n\n\nStream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream\n\n\nNaming: Compile time checks ensures that applications\n    components operators, streams are named\n\n\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.\n\n\nExamples include\n\n\n\n\n\n\nJavaBeans Validation\n:\n    Examples include\n\n\n\n\n@Max(): Value must be less than or equal to the number\n\n\n@Min(): Value must be greater than or equal to the\n    number\n\n\n@NotNull: The value of the field or property must not be\n    null\n\n\n@Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression\n\n\nInput port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)\n\n\nOutput Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)\n\n\n@Valid: For nested property validation \n a property should have this\n    annotation if its value  is itself an object whose properties\n    need to be validated.\n\n\n\n\n\n\n\n\nUnique names in application scope: Operators, streams, must have\n    unique names.\n\n\n\n\nCycles in the dag: DAG cannot have a cycle.\n\n\nUnique names in operator scope: Ports, properties, annotations\n    must have unique names.\n\n\nOne stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.\n\n\nApplication Window Period: Has to be an integral multiple the\n    streaming window period.\n\n\n\n\nRun Time\n\n\nRun time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to  \ndemos\n \nt\no\nillustrate these.\n\n\nError ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.\n\n\n\n\nMulti-Tenancy and Security\n\n\nHadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the\n\nOperation and Installation\nGuide\n\n.\n\n\nSecurity\n\n\nThe platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.\n\n\nResource Limits\n\n\nHadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.\n\n\n\n\nScalability and Partitioning\n\n\nScalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.\n\n\nDaily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.\n\n\nThe platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.\n\n\nPartitioning\n\n\nIf all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.\n\n\nTo address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition\n\n\n\n\nLoad balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.\n\n\nSticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.\n\n\n\n\nWe plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.\n\n\nSticky Partition vs Round Robin\n\n\nAs noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.\n\n\nStream Codec\n\n\nThe platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.\n\n\nStatic Partitioning\n\n\nDAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.\n\n\nDynamic Partitioning\n\n\nIn streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.\n\n\nSince partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.\n\n\nDefault Partitioning\n\n\nThe platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.\n\n\nTypically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.\n\n\nDefault Dynamic Partitioning\n\n\nTriggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.\n\n\nThe default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0 and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\na partition split\u00a0occurs, resulting in 00\nand 10 with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the 01 partition, leading to a split into 001 and 101\nwith mask 111, etc.\n\n\nShould load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.\n\n\nNxM Partitions\n\n\nWhen two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.\n\n\nFigure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.\n\n\n\n\nParallel\n\n\nIn cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.\n\n\nIn Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.\n\n\nSince operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.\n\n\n\n\nThe following code shows an example of creating a parallel partition.\n\n\ndag.addStream(\nDenormalizedUserId\n, idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);\n\n\n\n\nParallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.\n\n\nParallel Partitions with Streams Modes\n\n\nParallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1-\n2 and 2-\n3 significantly impacts the performance.\n\n\nCONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.\n\n\nA NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.\n\n\nA RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.\n\n\nParallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.\n\n\n\n\nParallel-Partition\n\n\nParallel-Partition with THREAD_LOCAL stream\n\n\nParallel-Partition with CONTAINER_LOCAL stream\n\n\nParallel-Partition with NODE_LOCAL stream\n\n\nParallel-Partition with RACK_LOCAL stream\n\n\n\n\nThese attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.\n\n\n\n\nSkew Balancing Partition\n\n\nSkew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.\n\n\nFigure 7 shows an example of skew balancing partition. An example\nof 3x1 partition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.\n\n\nLet's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.\n\n\n\n\nSkew Unifier Partition\n\n\nIn this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.\n\n\nTo trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.\n\n\nFigure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.\n\n\nIn the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.\n\n\n\n\nCascading Unifier\n\n\nLet's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.\n\n\nCascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.\n\n\n\n\nFigure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1\n\n F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk) \n F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following\n\n\n\n\nI/O limit on containers to allow proper behavior in an\n    multi-tenant environment\n\n\nLoad on oprD instance\n\n\nBuffer server limits on fan-in, fan-out\n\n\nSize of reservoir buffer for inbound fan-in\n\n\n\n\nA more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.\n\n\nSLA\n\n\nA Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.\n\n\n\n\nFault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.\n\n\nState of the Application\n\n\nThe state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).\n\n\nOperators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.\n\n\nRecovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.\n\n\nCheckpointing\n\n\nSTRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).\n\n\nThe only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.\n\n\nIn case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.\n\n\nIf an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.\n\n\nCheckpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.\n\n\nAn operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.\n\n\nThe serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.\n\n\nA complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.\n\n\nIn general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.\n\n\nAt Least Once\n\n\nAt least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.\n\n\nIn general for this recovery mode, the average time lag on a node\noutage is\n\n\n= (CP/2*SW)*T + HC\n\n\nwhere\n\n\n\n\nCP\n\u00a0\u00a0- Checkpointing period (default value is 30 seconds)\n\n\nSW\n\u00a0\u00a0- Streaming window period (default value is 0.5 seconds)\n\n\nT\n\u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory\n\n\nHC\n\u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones\n\n\n\n\nA lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.\n\n\nAt Most Once\n\n\nThis recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.\n\n\nFor multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.\n\n\nIn general, in this recovery mode, the average time lag on a node\noutage is\n\n\n= SW/2 + HC\n\n\nwhere\n\n\n\n\n\n\nSW\n\u00a0- Streaming window period (default value is 0.5\nseconds)\n\n\n\n\n\n\nHC\n\u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones\n\n\n\n\n\n\nExactly Once\n\n\nThis recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.\n\n\nSpeculative Execution\n\n\nIn future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.\n\n\n\n\n\n\nAt an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways\n\n\n\n\nStatically as dictated by STRAM\n\n\nDynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality\n\n\n\n\n\n\n\n\nAt a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner\n\n\n\n\nEntire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.\n\n\n\n\nIn all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.\n\n\n\n\nDynamic Application Modifications\n\n\nDynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.\n\n\nSome examples are\n\n\n\n\nDynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.\n\n\nModification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.\n\n\nModification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.\n\n\nModification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.\n\n\nQuery Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).\n\n\n\n\nDynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to  \nConfiguration Guide\n\n.\n\n\n\n\nUser Interface\n\n\nThe platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to  \nUI Console Guide\n.\n\n\nDemos\n\n\nIn this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source\n\nApache Apex-Malhar repository\n.\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.\n\n\n\n\nComputation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).\n\n\nYahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).\n\n\nEchoserver Reads messages from a\n    network connection and echoes them back out.\n\n\nTwitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes\n\n\nTwitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes\n\n\nTwitter top N frequent words:\n    Computes top N frequent words in a sliding window\n\n\nWord count: Computes word count for\n    all words within a large file\n\n\nMobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).\n\n\nFrauddetect: Analyzes a stream of\n    credit card merchant transactions.\n\n\nMroperator:Contains several\n    map-reduce applications.\n\n\nR: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).\n\n\nMachinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Applications"
        }, 
        {
            "location": "/application_development/#application-developer-guide", 
            "text": "Real-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.  The DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.  DataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.  In the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called  Operators  interconnected\nby the data-flow edges called   Streams .\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the  Operator Development Guide .", 
            "title": "Application Developer Guide"
        }, 
        {
            "location": "/application_development/#running-a-test-application", 
            "text": "This chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:   Open up platform files in your IDE (for example NetBeans, or Eclipse)  Open Demos project  Open Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package  See the results in your system console   Congratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.  // Generates random numbers\nRandomEventGenerator rand = dag.addOperator( rand , new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of  x  and  y \nRoundRobinHashMap String,Object  rrhm = dag.addOperator( rrhm , new RoundRobinHashMap String, Object ());\nrrhm.setKeys(new String[] {  x ,  y  });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator( picalc , new Script());\ncalc.setPassThru(false);\ncalc.put( i ,0);\ncalc.put( count ,0);\ncalc.addSetupScript( function pi() { if (x*x+y*y  =  +maxValue*maxValue+ ) { i++; } count++; return i / count * 4; } );\ncalc.setInvoke( pi );\ndag.addStream( rand_rrhm , rand.integer_data, rrhm.data);\ndag.addStream( rrhm_calc , rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator( console , new ConsoleOutputOperator());\ndag.addStream( rand_console ,calc.result, console.input);  You can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.", 
            "title": "Running A Test Application"
        }, 
        {
            "location": "/application_development/#test-application-yahoo-finance-quotes", 
            "text": "The PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from   Yahoo! Finance  \u00a0and computes the\nfollowing for four tickers, namely  IBM , GOOG ,  YHOO .   Quote: Consisting of last trade price, last trade time, and\n    total volume for the day  Per-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute  Simple Moving Average: trade price over 5 minutes   Total volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.   The operator StockTickerInput:\u00a0StockTickerInput \u00a0 is\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:  $ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1' IBM ,203.966,1513041, 1:43pm  GOOG ,762.68,1879741, 1:43pm  AAPL ,444.3385,11738366, 1:43pm  YHOO ,19.3681,14707163, 1:43pm   Among all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.  Here is the class implementation for StockTickInput:  package com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap String, Long  lastVolume = new HashMap String, Long ();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Double  price = new DefaultOutputPort KeyValPair String, Double ();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Long  volume = new DefaultOutputPort KeyValPair String, Long ();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, String  time = new DefaultOutputPort KeyValPair String, String ();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str =  http://download.finance.yahoo.com/d/quotes.csv?s= ;\n    for (int i = 0; i   symbols.length; i++) {\n      if (i != 0) {\n        str +=  , ;\n      }\n      str += symbols[i];\n    }\n    str +=  f=sl1vt1 e=.csv ;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter( http.protocol.cookie-policy , CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println( Method failed:   + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List String[]  myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList String  tuple = new ArrayList String (Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is  Symbol , Price , Volume , Time \n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol   0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair String, Double (symbol, currentPrice));\n            volume.emit(new KeyValPair String, Long (symbol, vol));\n            time.emit(new KeyValPair String, String (symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}  The operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.  Important: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.  The method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.  Method\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.  Note that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.  The operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal K,V \u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal String,Long , where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if cumulative was set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.  The operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal K \u00a0from the\nstream\u00a0package.  The operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal K,V \u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal String,Double .  The operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal String,Long , but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.  The operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.  The operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage String,Double , which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.  The operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.  Connecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.  package com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo.  p \n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal String, Long  getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal String, Long  getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal String, Double  getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal String, Double  oper = dag.addOperator(name, new RangeKeyVal String, Double ());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal String,Double,Long,String,?,?  getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,Double,Long,String,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,Double,Long,String,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal String,HighLow,Long,?,?,?  getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,HighLow,Long,Object,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage String, Double  getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage String, Double  oper = dag.addOperator(name, new SimpleMovingAverage String, Double ());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort Object  getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix +  : %s );\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n       DefaultPartitionCodec String, Double  codec = new DefaultPartitionCodec String, Double ();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n\n}  Note that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.  In the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the  Installation Guide .", 
            "title": "Test Application: Yahoo! Finance Quotes"
        }, 
        {
            "location": "/application_development/#running-a-test-application_1", 
            "text": "We will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).  The platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.  The instructions below assume that the platform was installed in a\ndirectory  INSTALL_DIR  and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in local mode \u00a0 (in IDE or from command line) or on a   Hadoop cluster   .  To start the Apex CLI run  INSTALL_DIR /bin/apex  The command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)  apex  launch -local  INSTALL_DIR /yahoo-finance-demo-3.4.0.apa  To terminate the application in local mode, enter Ctrl-C  Tu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)  apex  launch  INSTALL_DIR /yahoo-finance-demo-3.4.0.apa  To stop the application running in Hadoop, terminate it in the Apex CLI:  apex  kill-app  Executing the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.", 
            "title": "Running a Test Application"
        }, 
        {
            "location": "/application_development/#local-mode", 
            "text": "In local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/application_development/#hadoop-cluster", 
            "text": "In this section we discuss various Hadoop cluster setups.", 
            "title": "Hadoop Cluster"
        }, 
        {
            "location": "/application_development/#single-node-cluster", 
            "text": "In a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.  In this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.", 
            "title": "Single Node Cluster"
        }, 
        {
            "location": "/application_development/#multi-node-cluster", 
            "text": "In a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.  Before you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.", 
            "title": "Multi-Node Cluster"
        }, 
        {
            "location": "/application_development/#apache-apex-platform-overview", 
            "text": "", 
            "title": "Apache Apex Platform Overview"
        }, 
        {
            "location": "/application_development/#streaming-computational-model", 
            "text": "In this chapter, we describe the the basics of the real-time streaming platform and its computational model.  The platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .  Applications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.  The streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.  A fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0  This atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.  The platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.  Note that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.   Alongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave   properties  \u00a0 that can be set to specify the\ndesired computation. Those interested in details, should refer to Apex Malhar Operator Library \n.  The platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.  A streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.  An operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.", 
            "title": "Streaming Computational Model"
        }, 
        {
            "location": "/application_development/#streaming-application-manager-stram", 
            "text": "Streaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include    Running the Application   Read the\u00a0logical plan\u00a0of the application (DAG) submitted by the client  Validate the logical plan  Translate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.  Request resources (Hadoop containers) from Resource Manager,\n    per physical plan  Based on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.  Executes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.     Continually monitoring the application via heartbeats from each StreamingContainer   Collecting Application System Statistics and Logs  Logging all application-wide decisions taken  Providing system data on the state of the application via a  Web Service.   Supporting  Fault Tolerance  a.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper    Supporting  Dynamic\n    Partitioning : \u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).   Enabling  Security : \u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.  Enabling  Dynamic  modification \u00a0 of\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.   An example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.   An example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.", 
            "title": "Streaming Application Manager (STRAM)"
        }, 
        {
            "location": "/application_development/#hadoop-components", 
            "text": "In this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.  A streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future  All investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.", 
            "title": "Hadoop Components"
        }, 
        {
            "location": "/application_development/#yarn", 
            "text": "YARN  is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through YARN's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).", 
            "title": "YARN"
        }, 
        {
            "location": "/application_development/#resource-manager-rm", 
            "text": "ResourceManager (RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.", 
            "title": "Resource Manager (RM)"
        }, 
        {
            "location": "/application_development/#application-master-am", 
            "text": "The AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.", 
            "title": "Application Master (AM)"
        }, 
        {
            "location": "/application_development/#node-managers-nm", 
            "text": "There is one  NodeManager (NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.", 
            "title": "Node Managers (NM)"
        }, 
        {
            "location": "/application_development/#rpc-protocol", 
            "text": "Communication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.", 
            "title": "RPC Protocol"
        }, 
        {
            "location": "/application_development/#hdfs", 
            "text": "Hadoop includes a highly fault tolerant, high throughput\ndistributed file system ( HDFS ).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.", 
            "title": "HDFS"
        }, 
        {
            "location": "/application_development/#developing-an-application", 
            "text": "In this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.", 
            "title": "Developing An Application"
        }, 
        {
            "location": "/application_development/#development-process", 
            "text": "While the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.", 
            "title": "Development Process"
        }, 
        {
            "location": "/application_development/#design", 
            "text": "Identify common, reusable operators. Use a library\n    if possible.  Identify scalability and performance requirements before\n    designing the DAG.  Leverage attributes that the platform supports for scalability\n    and performance.  Use operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.  Use THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completely. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.  The overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.  Do not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.  Persist key information to HDFS if possible; it may be useful\n    for debugging later.  Decide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.", 
            "title": "Design"
        }, 
        {
            "location": "/application_development/#creating-new-project", 
            "text": "Please refer to the  Apex Application Packages \u00a0for\nthe basic steps for creating a new project.", 
            "title": "Creating New Project"
        }, 
        {
            "location": "/application_development/#writing-the-application-code", 
            "text": "Preferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.", 
            "title": "Writing the application code"
        }, 
        {
            "location": "/application_development/#testing", 
            "text": "Write test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.  Good test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)", 
            "title": "Testing"
        }, 
        {
            "location": "/application_development/#running-an-application", 
            "text": "The platform provides a command line tool called Apex CLI (apex)\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.  Apex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.  For more details on CLI please refer to the  Apex CLI Guide .", 
            "title": "Running an application"
        }, 
        {
            "location": "/application_development/#application-api", 
            "text": "This section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are    Instantiate an application (DAG)    (Optional) Set Attributes   Assign application name  Set any other attributes as per application requirements     Create/re-use and instantiate operators   Assign operator name that is unique within the  application  Declare schema upfront for each operator (and thereby its   ports )  (Optional) Set  properties \u00a0  and  attributes \u00a0  on the dag as per specification  Connect ports of operators via streams  Each stream connects one output port of an operator to one or  more input ports of other operators.  (Optional) Set attributes on the streams       Test the application.    There are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.", 
            "title": "Application API"
        }, 
        {
            "location": "/application_development/#java-api", 
            "text": "The Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.  The developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.  Let us revisit how the Yahoo! Finance test application constructs the DAG:  public class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n}", 
            "title": "Java API"
        }, 
        {
            "location": "/application_development/#property-file-api", 
            "text": "The platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.  Create an application (DAG): myApplication.properties  # input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort  Above snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.", 
            "title": "Property File API"
        }, 
        {
            "location": "/application_development/#attributes", 
            "text": "Attributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operators", 
            "text": "Operators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the   Operator Developer Guide . As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.  All operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.  Each operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith Apex CLI\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.", 
            "title": "Operators"
        }, 
        {
            "location": "/application_development/#operator-interface", 
            "text": "Operator interface in a DAG consists of  ports , \u00a0 properties , \u00a0and\n  attributes  . \u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.", 
            "title": "Operator Interface"
        }, 
        {
            "location": "/application_development/#ports", 
            "text": "Ports are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.  Here are examples of an input and an output port from the operator\nSum.  @InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort V  data = new DefaultInputPort V () {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort V  sum = new DefaultOutputPort V (){ \u2026 };  The process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.  There is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.  Port connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.  Attributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe  Parallel\nPartitions \u00a0 section.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.", 
            "title": "Ports"
        }, 
        {
            "location": "/application_development/#properties", 
            "text": "Properties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.  All non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.", 
            "title": "Properties"
        }, 
        {
            "location": "/application_development/#attributes_1", 
            "text": "Attributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in   Configuration Guide .", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operator-state", 
            "text": "The state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe  checkpointed \u00a0 every\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.  The distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.", 
            "title": "Operator State"
        }, 
        {
            "location": "/application_development/#stateless", 
            "text": "A Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.", 
            "title": "Stateless"
        }, 
        {
            "location": "/application_development/#stateful", 
            "text": "A Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.", 
            "title": "Stateful"
        }, 
        {
            "location": "/application_development/#operator-api", 
            "text": "The Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.  The APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.  In the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.", 
            "title": "Operator API"
        }, 
        {
            "location": "/application_development/#streaming-window", 
            "text": "Streaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows  public void process( tuple_type  tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown  A tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.", 
            "title": "Streaming Window"
        }, 
        {
            "location": "/application_development/#aggregate-application-window", 
            "text": "An operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.", 
            "title": "Aggregate Application Window"
        }, 
        {
            "location": "/application_development/#sliding-application-window", 
            "text": "A sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.", 
            "title": "Sliding Application Window"
        }, 
        {
            "location": "/application_development/#single-vs-multi-input-operator", 
            "text": "A single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.", 
            "title": "Single vs Multi-Input Operator"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms", 
            "text": "Application developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely   At-least-once: All atomic batches are processed at least once.\n    No data loss occurs.  At-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.  Exactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.   At-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.  Recovery mechanisms can be specified per Operator while writing\nthe application as shown below.  Operator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);  Also note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.  Details are explained in the chapter on Fault Tolerance\nbelow .", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#streams", 
            "text": "A stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics   Tuples are always delivered in the same order in which they\n    were emitted.  Consists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.  A stream that connects two containers passes through a\n    buffer server.  All streams can be persisted (by default in HDFS).  Exactly one output port writes to the stream.  Can be read by one or more input ports.  Connects operators within an application, not outside\n    an application.  Has an unique name within an application.  Has attributes which act as hints to STRAM.   Streams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:   THREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.  CONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.  NODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.  RACK_LOCAL: On nodes in the same rack; also called\n    in-rack.  unspecified: No guarantee. Could be anywhere within the\n    cluster     An example of a stream declaration is given below  DAG dag = new DAG();\n \u2026\ndag.addStream( views , viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality  The platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.  In a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of   standard operator template\nlibrary   \u00a0 follow\nthese principles.  A logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.  Modes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.  THREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.", 
            "title": "Streams"
        }, 
        {
            "location": "/application_development/#validating-an-application", 
            "text": "The platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely   Compile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.  Initialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.  Run Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.", 
            "title": "Validating an Application"
        }, 
        {
            "location": "/application_development/#compile-time", 
            "text": "Compile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include   Schema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.  Stream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream  Naming: Compile time checks ensures that applications\n    components operators, streams are named", 
            "title": "Compile Time"
        }, 
        {
            "location": "/application_development/#initializationinstantiation-time", 
            "text": "Initialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.  Examples include    JavaBeans Validation :\n    Examples include   @Max(): Value must be less than or equal to the number  @Min(): Value must be greater than or equal to the\n    number  @NotNull: The value of the field or property must not be\n    null  @Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression  Input port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)  Output Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)  @Valid: For nested property validation   a property should have this\n    annotation if its value  is itself an object whose properties\n    need to be validated.     Unique names in application scope: Operators, streams, must have\n    unique names.   Cycles in the dag: DAG cannot have a cycle.  Unique names in operator scope: Ports, properties, annotations\n    must have unique names.  One stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.  Application Window Period: Has to be an integral multiple the\n    streaming window period.", 
            "title": "Initialization/Instantiation Time"
        }, 
        {
            "location": "/application_development/#run-time", 
            "text": "Run time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to   demos   t o\nillustrate these.  Error ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.", 
            "title": "Run Time"
        }, 
        {
            "location": "/application_development/#multi-tenancy-and-security", 
            "text": "Hadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the Operation and Installation\nGuide \n.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/application_development/#security", 
            "text": "The platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.", 
            "title": "Security"
        }, 
        {
            "location": "/application_development/#resource-limits", 
            "text": "Hadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.", 
            "title": "Resource Limits"
        }, 
        {
            "location": "/application_development/#scalability-and-partitioning", 
            "text": "Scalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.  Daily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.  The platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.", 
            "title": "Scalability and Partitioning"
        }, 
        {
            "location": "/application_development/#partitioning", 
            "text": "If all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.  To address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition   Load balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.  Sticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.   We plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/application_development/#sticky-partition-vs-round-robin", 
            "text": "As noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.", 
            "title": "Sticky Partition vs Round Robin"
        }, 
        {
            "location": "/application_development/#stream-codec", 
            "text": "The platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.", 
            "title": "Stream Codec"
        }, 
        {
            "location": "/application_development/#static-partitioning", 
            "text": "DAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.", 
            "title": "Static Partitioning"
        }, 
        {
            "location": "/application_development/#dynamic-partitioning", 
            "text": "In streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.  Since partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.", 
            "title": "Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#default-partitioning", 
            "text": "The platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.  Typically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.", 
            "title": "Default Partitioning"
        }, 
        {
            "location": "/application_development/#default-dynamic-partitioning", 
            "text": "Triggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.  The default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0 and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\na partition split\u00a0occurs, resulting in 00\nand 10 with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the 01 partition, leading to a split into 001 and 101\nwith mask 111, etc.  Should load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.", 
            "title": "Default Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#nxm-partitions", 
            "text": "When two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.  Figure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.", 
            "title": "NxM Partitions"
        }, 
        {
            "location": "/application_development/#parallel", 
            "text": "In cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.  In Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.  Since operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.   The following code shows an example of creating a parallel partition.  dag.addStream( DenormalizedUserId , idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);  Parallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.", 
            "title": "Parallel"
        }, 
        {
            "location": "/application_development/#parallel-partitions-with-streams-modes", 
            "text": "Parallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1- 2 and 2- 3 significantly impacts the performance.  CONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.  A NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.  A RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.  Parallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.   Parallel-Partition  Parallel-Partition with THREAD_LOCAL stream  Parallel-Partition with CONTAINER_LOCAL stream  Parallel-Partition with NODE_LOCAL stream  Parallel-Partition with RACK_LOCAL stream   These attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.", 
            "title": "Parallel Partitions with Streams Modes"
        }, 
        {
            "location": "/application_development/#skew-balancing-partition", 
            "text": "Skew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.  Figure 7 shows an example of skew balancing partition. An example\nof 3x1 partition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.  Let's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.", 
            "title": "Skew Balancing Partition"
        }, 
        {
            "location": "/application_development/#skew-unifier-partition", 
            "text": "In this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.  To trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.  Figure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.  In the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.", 
            "title": "Skew Unifier Partition"
        }, 
        {
            "location": "/application_development/#cascading-unifier", 
            "text": "Let's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.  Cascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.   Figure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1  F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk)   F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following   I/O limit on containers to allow proper behavior in an\n    multi-tenant environment  Load on oprD instance  Buffer server limits on fan-in, fan-out  Size of reservoir buffer for inbound fan-in   A more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.", 
            "title": "Cascading Unifier"
        }, 
        {
            "location": "/application_development/#sla", 
            "text": "A Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.", 
            "title": "SLA"
        }, 
        {
            "location": "/application_development/#fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/application_development/#state-of-the-application", 
            "text": "The state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).  Operators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.  Recovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.", 
            "title": "State of the Application"
        }, 
        {
            "location": "/application_development/#checkpointing", 
            "text": "STRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).  The only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.  In case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.  If an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.  Checkpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.  An operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.  The serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.  A complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms_1", 
            "text": "Recovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.  In general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#at-least-once", 
            "text": "At least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.  In general for this recovery mode, the average time lag on a node\noutage is  = (CP/2*SW)*T + HC  where   CP \u00a0\u00a0- Checkpointing period (default value is 30 seconds)  SW \u00a0\u00a0- Streaming window period (default value is 0.5 seconds)  T \u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory  HC \u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones   A lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.", 
            "title": "At Least Once"
        }, 
        {
            "location": "/application_development/#at-most-once", 
            "text": "This recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.  For multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.  In general, in this recovery mode, the average time lag on a node\noutage is  = SW/2 + HC  where    SW \u00a0- Streaming window period (default value is 0.5\nseconds)    HC \u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones", 
            "title": "At Most Once"
        }, 
        {
            "location": "/application_development/#exactly-once", 
            "text": "This recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.", 
            "title": "Exactly Once"
        }, 
        {
            "location": "/application_development/#speculative-execution", 
            "text": "In future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.    At an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways   Statically as dictated by STRAM  Dynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality     At a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner   Entire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.   In all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.", 
            "title": "Speculative Execution"
        }, 
        {
            "location": "/application_development/#dynamic-application-modifications", 
            "text": "Dynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.  Some examples are   Dynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.  Modification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.  Modification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.  Modification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.  Query Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).   Dynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to   Configuration Guide \n.", 
            "title": "Dynamic Application Modifications"
        }, 
        {
            "location": "/application_development/#user-interface", 
            "text": "The platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to   UI Console Guide .", 
            "title": "User Interface"
        }, 
        {
            "location": "/application_development/#demos", 
            "text": "In this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source Apache Apex-Malhar repository .\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.   Computation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).  Yahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).  Echoserver Reads messages from a\n    network connection and echoes them back out.  Twitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes  Twitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes  Twitter top N frequent words:\n    Computes top N frequent words in a sliding window  Word count: Computes word count for\n    all words within a large file  Mobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).  Frauddetect: Analyzes a stream of\n    credit card merchant transactions.  Mroperator:Contains several\n    map-reduce applications.  R: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).  Machinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Demos"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Application Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nPreliminaries\n\n\nPlease read \nApache Apex Development Environment Setup\n\nfor detailed instructions on:\n\n\n\n\nPre-requisite tools such as JDK, Maven, Git, etc.\n\n\nSetting up the Datatorrent sandbox, if necessary.\n\n\nCreating a new Apache Apex project using either the command line or one of the\n  common IDEs.\n\n\nRunning the unit test from the newly created project.\n\n\n\n\nWriting Your Own App Package\n\n\nPlease refer to the \nBeginner's Guide\n on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to \nOperator Development Guide\n), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \ndt\n is a constant, \nattr\n is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\n\ncom.datatorrent.api.Context.DAGContext\n and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \ndt\n is a\nconstant, \noperator\n is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \nattr\n is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \ninput\n to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\n\ncom.datatorrent.api.Context.OperatorContext\n and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \nprop\n is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \nhost\n of the\nredis server for a \nredis\n output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \nset\n and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\n\nsetHost\n. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method \nsetHost\n\nwill be called on the \nredis\n operator with \n127.0.0.1\n as the argument.\n\n\nA property that is a simple collection like a list or a map can also be initialized\nin this way but it needs a couple of additional steps: (a) The property\nmust be initialized with an empty collection of the appropriate type.\n(b) An additional setter method for initializing items of the collection\nmust be present. For example, suppose we have properties named \nlist\n\nand \nmap\n in an operator named \nopA\n initialized with empty collections:\n\n\n  private List\nString\n list = new ArrayList\n();\n  private Map\nString, String\n map = new HashMap\n();\n\n  public List\nString\n getList() { return list; }\n  public void setList(List\nString\n v) { list = v; }\n\n  public Map\nString, String\n getMap() { return map; }\n  public void setMap(Map\nString, String\n v) { map = v; }\n\n\n\n\nYou can add items to those empty collections by using fragments like this in\nyour properties file provided you also add the setters shown below:\n\n\n  \nproperty\n\n    \nname\ndt.operator.opA.mapItem(abc)\n/name\n\n    \nvalue\n123\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.mapItem(pqr)\n/name\n\n    \nvalue\n567\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[0]\n/name\n\n    \nvalue\n1000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[1]\n/name\n\n    \nvalue\n2000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[3]\n/name\n\n    \nvalue\n3000\n/value\n\n  \n/property\n\n\n\n\n\nThe required additional setter methods in operator \nopA\n:\n\n\n  public void setListItem(int index, String value) {\n    final int need = index - list.size() + 1;\n    for (int i = 0; i \n need; i++) list.add(null);\n    list.set(index, value);\n  }\n\n  public void setMapItem(String key, String value) {\n    map.put(key, value);\n  }\n\n\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \ninputport\n is used to denote an input port\nand \noutputport\n to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \ninput\n of an operator named \nrange\n to\nbe 4000.\n\n\nproperty\n\n  \nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cinputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nOperation and Installation Guide\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.version\n3.2.0-incubating\n/apex.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\nManaging Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages\n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages\n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}\n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}\n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/applications/MyFirstApplication/launch\n\n\n\n\nPlease refer to the \nGateway API reference\n for the complete specification of the REST API.\n\n\nExamining and Launching Application Packages Through Apex CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (apex).  Please refer to the \nGateway API\n\nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n apex\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n apex\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\napex\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\n apex\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Apache Apex Application Packages"
        }, 
        {
            "location": "/application_packages/#preliminaries", 
            "text": "Please read  Apache Apex Development Environment Setup \nfor detailed instructions on:   Pre-requisite tools such as JDK, Maven, Git, etc.  Setting up the Datatorrent sandbox, if necessary.  Creating a new Apache Apex project using either the command line or one of the\n  common IDEs.  Running the unit test from the newly created project.", 
            "title": "Preliminaries"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the  Beginner's Guide  on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to  Operator Development Guide ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix  dt  is a constant,  attr  is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in com.datatorrent.api.Context.DAGContext  and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix  dt  is a\nconstant,  operator  is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator,  attr  is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named  input  to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in com.datatorrent.api.Context.OperatorContext  and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword  prop  is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property  host  of the\nredis server for a  redis  output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword  set  and the property name with the first character of the name\ncapitalized. In the above example the setter method would become setHost . The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method  setHost \nwill be called on the  redis  operator with  127.0.0.1  as the argument.  A property that is a simple collection like a list or a map can also be initialized\nin this way but it needs a couple of additional steps: (a) The property\nmust be initialized with an empty collection of the appropriate type.\n(b) An additional setter method for initializing items of the collection\nmust be present. For example, suppose we have properties named  list \nand  map  in an operator named  opA  initialized with empty collections:    private List String  list = new ArrayList ();\n  private Map String, String  map = new HashMap ();\n\n  public List String  getList() { return list; }\n  public void setList(List String  v) { list = v; }\n\n  public Map String, String  getMap() { return map; }\n  public void setMap(Map String, String  v) { map = v; }  You can add items to those empty collections by using fragments like this in\nyour properties file provided you also add the setters shown below:     property \n     name dt.operator.opA.mapItem(abc) /name \n     value 123 /value \n   /property \n   property \n     name dt.operator.opA.mapItem(pqr) /name \n     value 567 /value \n   /property \n   property \n     name dt.operator.opA.listItem[0] /name \n     value 1000 /value \n   /property \n   property \n     name dt.operator.opA.listItem[1] /name \n     value 2000 /value \n   /property \n   property \n     name dt.operator.opA.listItem[3] /name \n     value 3000 /value \n   /property   The required additional setter methods in operator  opA :    public void setListItem(int index, String value) {\n    final int need = index - list.size() + 1;\n    for (int i = 0; i   need; i++) list.add(null);\n    list.set(index, value);\n  }\n\n  public void setMapItem(String key, String value) {\n    map.put(key, value);\n  }", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword  inputport  is used to denote an input port\nand  outputport  to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named  input  of an operator named  range  to\nbe 4000.  property \n   name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cinputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Operation and Installation Guide .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.version 3.2.0-incubating /apex.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/application_packages/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages  The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/application_packages/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}  The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}  The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/applications/MyFirstApplication/launch  Please refer to the  Gateway API reference  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-apex-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (apex).  Please refer to the  Gateway API \nto see samples for these commands.", 
            "title": "Examining and Launching Application Packages Through Apex CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   apex  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   apex  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  apex  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.   apex  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/configuration_packages/", 
            "text": "Apache Apex Configuration Packages\n\n\nAn Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nApex CLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.4.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.4.0 \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File-\nNew Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.4.0 (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of \n\n\n\n\ndatatorrent.apppackage.name\n\n\ndatatorrent.apppackage.minversion\n\n\ndatatorrent.apppackage.maxversion\n\n\n\n\nto reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\napex\n launch mydtapp-1.0.0.apa -conf mydtconfig.apc\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\n\n\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n\n\n$ curl -XPOST -T {name}.apc http://{yourhost:port}/ws/v2/configPackages\n\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName. \n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n\n\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \n xyz.apc\n$ unzip -t xyz.apc\n\n\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration package stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#apache-apex-configuration-packages", 
            "text": "An Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nApex CLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Apache Apex Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.4.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuration_packages/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/configuration_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.4.0 \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/configuration_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File- New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.4.0 (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/configuration_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/configuration_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of    datatorrent.apppackage.name  datatorrent.apppackage.minversion  datatorrent.apppackage.maxversion   to reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/configuration_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/configuration_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/configuration_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  apex  launch mydtapp-1.0.0.apa -conf mydtconfig.apc  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/configuration_packages/#related-rest-api", 
            "text": "", 
            "title": "Related REST API"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2configpackages", 
            "text": "Payload: Raw content of configuration package zip  Function: Creates or replace a configuration package zip file in HDFS  Curl example:  $ curl -XPOST -T {name}.apc http://{yourhost:port}/ws/v2/configPackages", 
            "title": "POST /ws/v2/configPackages"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesuserapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusername", 
            "text": "Function: Returns the information of the specified configuration package", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusernamedownload", 
            "text": "Function: Returns the raw config package file  Curl example:  $ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download   xyz.apc\n$ unzip -t xyz.apc", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;/download"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2apppackagesuserapp-pkg-nameapp-pkg-versionapplicationsapp-namelaunchconfigpackageuserconfpkgname", 
            "text": "Function: Launches the app package with the specified configuration package stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "POST /ws/v2/appPackages/&lt;user&gt;/&lt;app-pkg-name&gt;/&lt;app-pkg-version&gt;/applications/{app-name}/launch?configPackage=&lt;user&gt;/&lt;confpkgname&gt;"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nin parallel. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nin parallel.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nTherea are 3 type of operators based on function: \n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nOperators Position in a DAG\n\n\nWe may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote that there are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nOperators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nHow Operator Works\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nDeveloping Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found \nhere\n.\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n\n\nsetup(OperatorContext context)\n\n\nbeginWindow(long windowId)\n\n\nendWindow()\n\n\ntearDown()\n\n\n\n\nIn order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nApex Operators\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Configures the output frequency from the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.\n\n\nThe following tasks are executed as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nAs part of testing our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Operators"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nin parallel. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nin parallel.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nTherea are 3 type of operators based on function:    Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#operators-position-in-a-dag", 
            "text": "We may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note that there are no cycles formed in the application\u00a0DAG.", 
            "title": "Operators Position in a DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "Operators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#how-operator-works", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "How Operator Works"
        }, 
        {
            "location": "/operator_development/#developing-custom-operators", 
            "text": "", 
            "title": "Developing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#functionality", 
            "text": "We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.", 
            "title": "Functionality"
        }, 
        {
            "location": "/operator_development/#inputs", 
            "text": "As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.", 
            "title": "Inputs"
        }, 
        {
            "location": "/operator_development/#outputs", 
            "text": "We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Outputs"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found  here .", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#the-operator-class", 
            "text": "The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:   setup(OperatorContext context)  beginWindow(long windowId)  endWindow()  tearDown()   In order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Apex Operators \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}", 
            "title": "The Operator Class"
        }, 
        {
            "location": "/operator_development/#class-operator-properties", 
            "text": "We define the following class variables:   sendPerTuple \u00a0- Configures the output frequency from the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();", 
            "title": "Class (Operator) properties"
        }, 
        {
            "location": "/operator_development/#the-constructor", 
            "text": "The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.  globalCounts = Maps.newHashMap();", 
            "title": "The Constructor"
        }, 
        {
            "location": "/operator_development/#setup-call", 
            "text": "The setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.  The following tasks are executed as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.", 
            "title": "Setup call"
        }, 
        {
            "location": "/operator_development/#begin-window-call", 
            "text": "The begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.", 
            "title": "Begin Window call"
        }, 
        {
            "location": "/operator_development/#process-tuple-call", 
            "text": "The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.", 
            "title": "Process Tuple call"
        }, 
        {
            "location": "/operator_development/#end-window-call", 
            "text": "This call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.", 
            "title": "End Window call"
        }, 
        {
            "location": "/operator_development/#teardown-call", 
            "text": "This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Teardown call"
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "As part of testing our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/library_operators/", 
            "text": "Operator Library\n\n\nThe following operators, classified into three groups, are available:\n\n\n\n\n\n\nInput\n\n\n\n\nKafka Input \nGuide\n\n  and \nJava Doc\n\n\nHDFS Input \nGuide\n\n  and \nJava Doc\n\n\nHDFS Input for large files \nGuide\n\n  and \nJava Doc\n\n\nJDBC Input \nGuide\n\n   and \nJava Doc\n\n\nJMS Input \nGuide\n\n  and \nJava Doc\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nBlock Reader \nGuide\n\n  and \nJava Doc\n\n\nCSV Parser \nGuide\n\n  and \nJava Doc\n\n\nJSON Parser \nGuide\n\n  and \nJava Doc\n\n\nJSON Formatter  \nGuide\n\n  and \nJava Doc\n\n\nDeduper  \nGuide\n\n  and \nJava Doc\n\n\nEnrich \nGuide\n\n  and \nJava Doc\n\n\nFilter  \nGuide\n\n  and \nJava Doc\n\n\nTransform  \nGuide\n\n  and \nJava Doc\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\nHDFS Output \nGuide\n\n  and \nJava Doc\n\n\nJMS Output \nGuide\n\n  and \nJava Doc", 
            "title": "List of Operators"
        }, 
        {
            "location": "/library_operators/#operator-library", 
            "text": "The following operators, classified into three groups, are available:    Input   Kafka Input  Guide \n  and  Java Doc  HDFS Input  Guide \n  and  Java Doc  HDFS Input for large files  Guide \n  and  Java Doc  JDBC Input  Guide \n   and  Java Doc  JMS Input  Guide \n  and  Java Doc     Process   Block Reader  Guide \n  and  Java Doc  CSV Parser  Guide \n  and  Java Doc  JSON Parser  Guide \n  and  Java Doc  JSON Formatter   Guide \n  and  Java Doc  Deduper   Guide \n  and  Java Doc  Enrich  Guide \n  and  Java Doc  Filter   Guide \n  and  Java Doc  Transform   Guide \n  and  Java Doc     Output   HDFS Output  Guide \n  and  Java Doc  JMS Output  Guide \n  and  Java Doc", 
            "title": "Operator Library"
        }, 
        {
            "location": "/operators/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/block_reader/#ports", 
            "text": "blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/block_reader/#readercontext", 
            "text": "This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.", 
            "title": "readerContext"
        }, 
        {
            "location": "/operators/block_reader/#abstract-methods", 
            "text": "STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/block_reader/#auto-scalability", 
            "text": "Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .", 
            "title": "Auto-scalability"
        }, 
        {
            "location": "/operators/block_reader/#configuration", 
            "text": "maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#example-application", 
            "text": "This simple dag demonstrates how any concrete implementation of  AbstractFSBlockReader  can be plugged into an application.    In the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.  public class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( File-splitter , new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator( Block-reader , new LineReader());\n    Filter filter = dag.addOperator( Filter , new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator( Record-writer , new RecordOutputOperator());\n\n    dag.addStream( file-block metadata , input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream( records , blockReader.messages, filter.input);\n    dag.addStream( filtered-records , filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader String \n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort AbstractBlockReader.ReaderRecord String  output = new DefaultOutputPort ();\n    public final transient DefaultInputPort AbstractBlockReader.ReaderRecord String  input = new DefaultInputPort AbstractBlockReader.ReaderRecord String ()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord String  stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(),  . )) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator AbstractBlockReader.ReaderRecord String \n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}  Configuration to parallel partition block reader with its downstream operators.     property \n     name dt.operator.Filter.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property \n   property \n     name dt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property", 
            "title": "Example Application"
        }, 
        {
            "location": "/operators/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/block_reader/#readaheadlinereadercontext", 
            "text": "In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "ReadAheadLineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/block_reader/#linereadercontext", 
            "text": "This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "LineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/block_reader/#fixedbytesreadercontext", 
            "text": "This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.", 
            "title": "FixedBytesReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#configuration_1", 
            "text": "readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/operators/block_reader/#processstats", 
            "text": "The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.", 
            "title": "processStats "
        }, 
        {
            "location": "/operators/block_reader/#definepartitions", 
            "text": "Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "definePartitions"
        }, 
        {
            "location": "/operators/deduper/", 
            "text": "Dedup Operator\n\n\nThis document is intended as a guide for understanding and using the\nDedup operator/module.\n\n\nDedup - \u201cWhat\u201d in a Nutshell\n\n\nDedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.\n\n\n\n\nDedup - \u201cHow\u201d in a Nutshell\n\n\nIn order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.\n\n\n\n\nFollowing are the different components of the Deduper Operator\n\n\n\n\nDedup Operator\n - This is responsible for the overall functionality\n    of the operator. This in turn makes use of other components to\n    establish the end goal of deciding whether a tuple is a duplicate of\n    some earlier tuple, or is a unique tuple.\n\n\nBucket Store\n - This is responsible for storing the unique tuples as\n    supplied by the Deduper and storing them into Buckets in HDFS.\n\n\nBucket Manager\n - Since, all of the data cannot be stored in memory,\n    this component is responsible for loading and unloading of the\n    buckets to and from the memory as requested by the Deduper.\n\n\n\n\nThis was a very small introduction to the functioning of the Deduper.\nFollowing sections will go into more detail on each of the components.\n\n\nUse case - Basic Dedup\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.\n\n\nConsider an example schema and two sample tuples\n\n\n{Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nLet us assume that the Dedup Key is\n\n\n{Name, Phone}\n\n\n\n\nIn this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as\n\n\n{Phone, Email}\n\n\n\n\nthen in this case, the two are unique tuples as the keys of both tuples\ndo not match.\n\n\nUse case Details\n\n\nConsider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.\n\n\n{Name, Phone, Email, Date, City, Zip, Country}\n\n\n\n\nAlso consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.\n\n\nAs part of configuring the operator for this use case, we need to set\nthe following parameters:\n\n\nDedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to\n\n\n{Name,Email}\n\n\n\n\nThe above configuration is sufficient to resolve the use case.\n\n\n\n\nUse case - Dedup with Expiry\n\n\nMotivation\n\n\nThe Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.\n\n\nExpiry Key\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.\n\n\nUse case Details\n\n\nConsider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.\n\n\nThe expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.\n\n\nConfiguring the below parameters will solve the problem for this use\ncase:\n\n\n\n\nDedup Key\n - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.\n\n\nExpiry Key\n - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.\n\n\nExpiry Period\n - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.\n\n\n\n\nConfiguration of these parameters would resolve this use case.\n\n\nUse cases - Summary\n\n\n\n\nBasic Dedup\n - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.\n\n\nDedup with Expiry\n - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:\n\n\nTime based\n - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time\n\n\nWith respect to system time\n - Time progresses with system time. Any expiry criterions are executed with this notion of system time.\n\n\nWith respect to tuple time\n - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.\n\n\nAny Ordered Key\n - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.\n\n\nCategorical Key\n - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.\n\n\n\n\n\n\nTechnical Architecture\n\n\nBlock Diagram\n\n\n\n\nThe deduper has a single input port and multiple output ports.\n\n\n\n\ninput\n - This is the input port through which the tuples arrive at\n    the Deduper.\n\n\nunique\n\u00a0- This is the output port on which unique tuples are sent out\n    by the Deduper.\n\n\nduplicate\n\u00a0- This is the output port on which duplicate tuples are\n    sent out by the Deduper.\n\n\nexpired\n\u00a0- This is the output port on which expired tuples are sent\n    out by the Deduper.\n\n\nerror\n\u00a0- This is the output port on which the error tuples are sent\n    out by the Deduper.\n\n\n\n\n\n\nConcepts\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.\n\n\nExpiry Key\n\n\nA tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.\n\n\nAt the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.\n\n\nIn such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.\n\n\nIn order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.\n\n\n\n\nThe domain of the key must be ordered. Example - Timestamp field\n\n\nThe domain of the key must be categorical and sorted. Example - City\n    names grouped together\n\n\n\n\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.\n\n\nTime Points\n\n\nFor every dataset that the deduper processes, it maintains a set of time\npoints:\n\n\n\n\nLatest Point\n\u00a0- This is the maximum time point observed in all the\n    processed tuples.\n\n\nExpiry Point\n\u00a0- This is given by: \nExpiry Point = Latest Point -\n    Expiry Period\n\n\n\n\nThese points help the deduper to make decisions related to expiry of a\ntuple.\n\n\nExample - Expiry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTuple Id\n\n\nExpiry Key\n\n\n(Expiry Period = 10)\n\n\nLatest Point\n\n\nExpiry Point\n\n\nDecision for Tuple\n\n\n\n\n\n\n1\n\n\n10\n\n\n10\n\n\n1\n\n\nNot Expired\n\n\n\n\n\n\n2\n\n\n20\n\n\n20\n\n\n11\n\n\nNot Expired\n\n\n\n\n\n\n3\n\n\n25\n\n\n25\n\n\n16\n\n\nNot Expired\n\n\n\n\n\n\n4\n\n\n40\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n5\n\n\n21\n\n\n40\n\n\n31\n\n\nExpired\n\n\n\n\n\n\n6\n\n\n35\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n7\n\n\n45\n\n\n45\n\n\n36\n\n\nNot Expired\n\n\n\n\n\n\n8\n\n\n57\n\n\n57\n\n\n48\n\n\nNot Expired\n\n\n\n\n\n\n\n\n\nBuckets\n\n\nOne of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.\n\n\nBucket Span\n\n\nBucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.\n\n\nBucket Key\n\n\nBucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.\n\n\nWe define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.\n\n\nIn case of Basic Dedup:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with Expiry:\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nNumber of Buckets\n\n\nThe number of buckets can be given by\n\n\nNum Buckets = Expiry Period / Bucket Span.\n\n\n\n\nThis is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.\n\n\nBucket Index\n\n\nA Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).\n\n\nExample - Buckets\n\n\n\n\nAssumptions\n\n\nAssumption 1\n\n\nThis assumption is only applicable in case of Dedup with Expiry.\n\n\nFor any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2\n\n\n\n\nIn other words, there may never be\u00a0two tuples t1 and t2 such that\n\n\nTuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2\n\n\n\n\nThis assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.\n\n\nThe reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.\n\n\nFlow of a Tuple through Dedup Operator\n\n\nTuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.\n\n\nDeduper View\n\n\nA tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.\n\n\nIdentify Bucket Key\n\n\nIdentify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with expiry, we calculate the Bucket key as\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nCheck if tuple is Expired\n\n\nThis is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.\n\n\nif ( Latest Point - Expiry Key \n Expiry Point ) then Expired\n\n\n\n\nIf the tuple is expired, then put it out to the expired port.\n\n\nCheck if tuple is a Duplicate or Unique\n\n\nOnce a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed \nhere\n.\n\n\nDuplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the \nfirst\nstep\n. The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.\n\n\nCase I - Bucket available in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.\n\n\nCase II - Bucket not in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed\n\nhere\n. Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.\n\n\nHandling tuples after Buckets are loaded\n\n\nThe Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:\n\n\n\n\nThe bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.\n\n\nThe bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.\n\n\n\n\nAfter processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section \nCase II\n- Bucket not in memory\n. For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for \nBuckets available in memory\n.\n\n\nBucket Manager\n\n\nBucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.\n\n\nIdentify Bucket Index\n\n\nBucket index is discussed \nhere\n. Bucket index can be\ncalculated as follows:\n\n\nBucket Index = Requested Bucket Key % Number of Buckets,\n\n\n\n\nwhere Number of Buckets is as defined \nhere\n.\n\n\nRequest Bucket Load from Store\n\n\nOnce we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned \nhere\n. We detail the process of fetching the\nbucket data from the store in \nthis\n\u00a0section.\n\n\nBucket Eviction\n\n\nIt may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.\n\n\nBucket Store\n\n\nThe Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.\n\n\nData Format\n\n\nBucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.\n\n\n\n\nAll the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the \nData Structures\n\u00a0section.\n\n\nData Structures\n\n\nHDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.\n\n\n\n\nBucket Positions\n - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.\n\n\nWindow To Buckets\n - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.\n\n\nWindow to Timestamp\n - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.\n\n\n\n\nBucket Fetch\n\n\nFor fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Deduper"
        }, 
        {
            "location": "/operators/deduper/#dedup-operator", 
            "text": "This document is intended as a guide for understanding and using the\nDedup operator/module.", 
            "title": "Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#dedup-what-in-a-nutshell", 
            "text": "Dedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.", 
            "title": "Dedup - \u201cWhat\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#dedup-how-in-a-nutshell", 
            "text": "In order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.", 
            "title": "Dedup - \u201cHow\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#use-case-basic-dedup", 
            "text": "", 
            "title": "Use case - Basic Dedup"
        }, 
        {
            "location": "/operators/deduper/#dedup-key", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.  Consider an example schema and two sample tuples  {Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}  Let us assume that the Dedup Key is  {Name, Phone}  In this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as  {Phone, Email}  then in this case, the two are unique tuples as the keys of both tuples\ndo not match.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#use-case-details", 
            "text": "Consider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.  {Name, Phone, Email, Date, City, Zip, Country}  Also consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.  As part of configuring the operator for this use case, we need to set\nthe following parameters:  Dedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to  {Name,Email}  The above configuration is sufficient to resolve the use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-case-dedup-with-expiry", 
            "text": "", 
            "title": "Use case - Dedup with Expiry"
        }, 
        {
            "location": "/operators/deduper/#motivation", 
            "text": "The Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.", 
            "title": "Motivation"
        }, 
        {
            "location": "/operators/deduper/#expiry-key", 
            "text": "The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period", 
            "text": "The expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#use-case-details_1", 
            "text": "Consider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.  The expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.  Configuring the below parameters will solve the problem for this use\ncase:   Dedup Key  - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.  Expiry Key  - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.  Expiry Period  - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.   Configuration of these parameters would resolve this use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-cases-summary", 
            "text": "Basic Dedup  - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.  Dedup with Expiry  - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:  Time based  - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time  With respect to system time  - Time progresses with system time. Any expiry criterions are executed with this notion of system time.  With respect to tuple time  - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.  Any Ordered Key  - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.  Categorical Key  - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.", 
            "title": "Use cases - Summary"
        }, 
        {
            "location": "/operators/deduper/#technical-architecture", 
            "text": "", 
            "title": "Technical Architecture"
        }, 
        {
            "location": "/operators/deduper/#block-diagram", 
            "text": "", 
            "title": "Block Diagram"
        }, 
        {
            "location": "/operators/deduper/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/deduper/#dedup-key_1", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-key_1", 
            "text": "A tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.  At the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.  In such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.  In order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.  The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.   The domain of the key must be ordered. Example - Timestamp field  The domain of the key must be categorical and sorted. Example - City\n    names grouped together   This expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period_1", 
            "text": "The Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#time-points", 
            "text": "For every dataset that the deduper processes, it maintains a set of time\npoints:   Latest Point \u00a0- This is the maximum time point observed in all the\n    processed tuples.  Expiry Point \u00a0- This is given by:  Expiry Point = Latest Point -\n    Expiry Period   These points help the deduper to make decisions related to expiry of a\ntuple.", 
            "title": "Time Points"
        }, 
        {
            "location": "/operators/deduper/#example-expiry", 
            "text": "Tuple Id  Expiry Key  (Expiry Period = 10)  Latest Point  Expiry Point  Decision for Tuple    1  10  10  1  Not Expired    2  20  20  11  Not Expired    3  25  25  16  Not Expired    4  40  40  31  Not Expired    5  21  40  31  Expired    6  35  40  31  Not Expired    7  45  45  36  Not Expired    8  57  57  48  Not Expired", 
            "title": "Example - Expiry"
        }, 
        {
            "location": "/operators/deduper/#buckets", 
            "text": "One of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.", 
            "title": "Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-span", 
            "text": "Bucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.", 
            "title": "Bucket Span"
        }, 
        {
            "location": "/operators/deduper/#bucket-key", 
            "text": "Bucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.  We define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.  In case of Basic Dedup:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with Expiry:  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#number-of-buckets", 
            "text": "The number of buckets can be given by  Num Buckets = Expiry Period / Bucket Span.  This is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.", 
            "title": "Number of Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-index", 
            "text": "A Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).", 
            "title": "Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#example-buckets", 
            "text": "", 
            "title": "Example - Buckets"
        }, 
        {
            "location": "/operators/deduper/#assumptions", 
            "text": "", 
            "title": "Assumptions"
        }, 
        {
            "location": "/operators/deduper/#assumption-1", 
            "text": "This assumption is only applicable in case of Dedup with Expiry.  For any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2  In other words, there may never be\u00a0two tuples t1 and t2 such that  Tuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2  This assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.  The reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.", 
            "title": "Assumption 1"
        }, 
        {
            "location": "/operators/deduper/#flow-of-a-tuple-through-dedup-operator", 
            "text": "Tuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.", 
            "title": "Flow of a Tuple through Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#deduper-view", 
            "text": "A tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.", 
            "title": "Deduper View"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-key", 
            "text": "Identify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with expiry, we calculate the Bucket key as  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Identify Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-expired", 
            "text": "This is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.  if ( Latest Point - Expiry Key   Expiry Point ) then Expired  If the tuple is expired, then put it out to the expired port.", 
            "title": "Check if tuple is Expired"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-a-duplicate-or-unique", 
            "text": "Once a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed  here .  Duplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the  first\nstep . The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.", 
            "title": "Check if tuple is a Duplicate or Unique"
        }, 
        {
            "location": "/operators/deduper/#case-i-bucket-available-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.", 
            "title": "Case I - Bucket available in memory"
        }, 
        {
            "location": "/operators/deduper/#case-ii-bucket-not-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed here . Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.", 
            "title": "Case II - Bucket not in memory"
        }, 
        {
            "location": "/operators/deduper/#handling-tuples-after-buckets-are-loaded", 
            "text": "The Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:   The bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.  The bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.   After processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section  Case II\n- Bucket not in memory . For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for  Buckets available in memory .", 
            "title": "Handling tuples after Buckets are loaded"
        }, 
        {
            "location": "/operators/deduper/#bucket-manager", 
            "text": "Bucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.", 
            "title": "Bucket Manager"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-index", 
            "text": "Bucket index is discussed  here . Bucket index can be\ncalculated as follows:  Bucket Index = Requested Bucket Key % Number of Buckets,  where Number of Buckets is as defined  here .", 
            "title": "Identify Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#request-bucket-load-from-store", 
            "text": "Once we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned  here . We detail the process of fetching the\nbucket data from the store in  this \u00a0section.", 
            "title": "Request Bucket Load from Store"
        }, 
        {
            "location": "/operators/deduper/#bucket-eviction", 
            "text": "It may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.", 
            "title": "Bucket Eviction"
        }, 
        {
            "location": "/operators/deduper/#bucket-store", 
            "text": "The Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.", 
            "title": "Bucket Store"
        }, 
        {
            "location": "/operators/deduper/#data-format", 
            "text": "Bucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.   All the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the  Data Structures \u00a0section.", 
            "title": "Data Format"
        }, 
        {
            "location": "/operators/deduper/#data-structures", 
            "text": "HDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.   Bucket Positions  - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.  Window To Buckets  - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.  Window to Timestamp  - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.", 
            "title": "Data Structures"
        }, 
        {
            "location": "/operators/deduper/#bucket-fetch", 
            "text": "For fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Bucket Fetch"
        }, 
        {
            "location": "/operators/dimensions_computation/", 
            "text": "Dimensions Computation\n\n\nBig data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends.\n\nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as \nDimensions Computation\n.\n\n\nThis document provides an overview of \nDimensions Computation\n, as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.\n\n\nOverview\n\n\nWhat is Dimensions Computation?\n\n\nDimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.\n\n\nDimensions Computation\n provides a way for businesses to perform aggregations on configured numeric data. The \nDimensionsComputation\n operator works along with the \nDimensionStore\n operator, which provides the capability for applications to display historical data and trends.\n\n\nKey Concepts\n\n\nKey set\n\n\nA key set is a set of fields in the incoming tuple that is used to combine data for aggregation.\n\n\nValue set\n\n\nA value set is the set of fields in the incoming tuple on which \nAggregator\n(s)  are applied.\n\n\nAggregator\n\n\nAn aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.\n\n\nAggregates\n\n\nAggregates are objects containing the aggregated values for a configured value set and key combination.\n\n\nTime buckets\n\n\nTime buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12:\n01:34\n PM * and * 12:\n01:59\n PM * will be * 12:\n01:00\n PM\n. Similarly, for an hourly time bucket, floor time-value for * \n15\n:02:34 PM\n and * \n15\n:34:00 PM\n will be * \n15:00:00\n PM\n.\n\n\nAfter calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.\n\n\nCombinations\n\n\nCombinations indicate a group of the keys that are used for aggregate computations.\n\n\nIncremental \nAggregators\n\n\nIncremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,\n\n\nSUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value} \n {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}\n\n\n\n\nOn-the-fly \nAggregators\n (OTF Aggregators)\n\n\nOn-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,\n\n\nAVERAGE = {Current_SUM} / {Current_COUNT}\n\n\n\n\nDimensionsComputation operator\n\n\nThe DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.\n\n\nDimensionsStore operator\n\n\nThe DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.\n\n\nDimensions Computation use cases\n\n\nConsider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.\n\n\nArchitecture\n\n\nDimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given \nDAG\n is for a complete Dimensions Computation application.\n\n\nThe operators within Dimensions Computation are described in detail.\n\n\nDimensionsComputation\n\n\nThe DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the \nconfiguration\n, the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in \nDAG\n) for calculating cumulative aggregates.\n\n\nDimensionsStore\n\n\nThe DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:\n\n\nSUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2\n\n\n\n\nThe DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.\n\n\nQuery\n\n\nThe Query operator interfaces with the \npubsub server\n of \nDataTorrent Gateway\n. The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.\n\n\nQueryResult\n\n\nThe QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the \npubsub server\n for publishing to UI widgets.\n\n\nDAG \n\n\n\n\nDimensions Computation Configuration \n\n\nConfiguration Definitions\n\n\nThe configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:\n\n\nDimensions Computation Schema Configuration\n\n\nThe Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:\n\n\n{\nkeys\n:[{\nname\n:\npublisher\n,\ntype\n:\nstring\n,\nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n]},\n         {\nname\n:\nadvertiser\n,\ntype\n:\nstring\n,\nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n]},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n,\nenumValues\n:[\nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n]}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\npublisher\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nThe schema configuration is a JSON string that contains the following information:\n\n\n\n\n\n\nkeys:\n - This contains the set of keys derived from an input tuple. The \nname\n field stands for the name of the field from input tuple. The \ntype\n can be defined for individual keys. The probable values for individual keys can be provided using \nenumValues\n.\n\n\n\n\n\n\nvalues:\n - This contains the set of fields from an input tuple on which aggregates are calculated. The \nname\n field stands for the name of the field from an input tuple. The \ntype\n can be defined for individual keys. The \naggregators\n can be defined separately for individual values. Only configured aggregator functions are executed on values.\n\n\n\n\n\n\ntimeBuckets:\n - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are \n\"1m\", \"1h\", \"1d\"\n\n\n\n\n\n\ndimensions:\n - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in \ncombination\n with the JSON key. \nadditionalValues\n can be used for mentioning additional aggregators for any \nvalue\n. For example, \nimpressions:MIN\n indicates that for a given combination, calculate \"\nMIN\n\" for value \"\nimpression\n\" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.\n\n\n\n\n\n\nOperator Configurations\n\n\nOperator configurations is another set of configuration that can be used to configure individual operators.\n\n\nProperties\n\n\n\n\ndt.operator.QueryResult.topic:\n - This is the name of the topic on which UI widgets listen for results.\n\n\ndt.operator.Query.topic:\n - his is the name of the topic on which Query operator listen for queries.\n\n\ndt.operator.QueryResult.numRetries\n - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.\n\n\n\n\nAttributes\n\n\n\n\ndt.operator.DimensionsComputation.attr.PARTITIONER:\n - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the \nPartitioning\n section for details on partitioning.\n\n\ndt.operator.DimensionsComputation.attr.MEMORY_MB:\n - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.\n\n\ndt.operator.Store.attr.MEMORY_MB:\n - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.\n\n\ndt.port.*.attr.QUEUE_CAPACITY\n - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.\n\n\n\n\nVisualizing Dimensions Computation\n\n\nWhen Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.\n\n\nAnd example of a Dashboard UI Widget is as follows:\n\n\n\n\nCreating Dimensions Computation Application\n\n\nConsider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.\n\n\nSample publisher event\n\n\nAn event might look like this:\n\n\npublic class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}\n\n\n\n\nCreating an Application using out-of-the-box operators\n\n\nDimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:\n\n\n@ApplicationAnnotation(name=\nAdEventDemo\n)\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA = \nadsGenericEventSchema.json\n;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString(\neventSchema.json\n);\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver object.\n    AdEventReceiver receiver = dag.addOperator(\nEvent Receiver\n, AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map\nString, String\n keyToExpression = Maps.newHashMap();\n    keyToExpression.put(\nadvertiser\n, \ngetAdvertiser()\n);\n    keyToExpression.put(\nlocation\n, \ngetLocation()\n);\n    keyToExpression.put(\ntime\n, \ngetTime()\n);\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map\nString, String\n valueToExpression = Maps.newHashMap();\n    valueToExpression.put(\ncost\n, \ngetCost()\n);\n    valueToExpression.put(\nrevenue\n, \ngetRevenue()\n);\n    valueToExpression.put(\nimpressions\n, \ngetImpressions()\n);\n    valueToExpression.put(\nclicks\n, \ngetClicks()\n);\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\nStore\n, AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\ndataStorePath\n);\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator(\nQuery\n, PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\nQueryResult\n, PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream(\nQuery\n, wsIn.outputPort, store.query);\n    dag.addStream(\nQueryResult\n, store.queryResult, wsOut.input);\n    dag.addStream(\nInputStream\n, receiver.output, dimensions.input);\n    dag.addStream(\nDimensionalData\n, dimensions.output, store.input);\n  }\n}\n\n\n\n\nConfiguration for Sample Predefined Use Cases\n\n\nThe following configuration can be used for the predefined use case of the advertiser-publisher.\n\n\nDimensions Schema Configuration\n\n\n{\nkeys\n:[{\nname\n:\nadvertiser\n,\ntype\n:\nstring\n},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nOperator Configuration\n\n\nNote:\n This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.PARTITIONER\n/name\n\n    \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.MEMORY_MB\n/name\n\n    \nvalue\n16384\n/value\n\n  \n/property\n\n  \nproperty\n\n     \nname\ndt.port.*.attr.QUEUE_CAPACITY\n/name\n\n     \nvalue\n32000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Query.topic\n/name\n\n    \nvalue\nAdsEventQuery\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.QueryResult.topic\n/name\n\n    \nvalue\nAdsEventQueryResult\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.\n\n\nAdvanced Concepts\n\n\nPartitioning \n\n\nThe Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the \nproperties.xml\n file, or in the \ndt-site.xml\n file.\n\n\nproperty\n\n  \nname\ndt.operator.DimensionsComputations.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n\n/property\n\n\n\n\n\nThis adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of \n8\n is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.\n\n\nAlong with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:\n\n\nDimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n\n\n\nHere the unifier used is \nDimensionsComputationUnifierImpl\n which is an out-of-the-box operator present in the DataTorrent distribution.\n\n\nConclusion\n\n\nAggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Dimension Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation", 
            "text": "Big data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends. \nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as  Dimensions Computation .  This document provides an overview of  Dimensions Computation , as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.", 
            "title": "Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/operators/dimensions_computation/#what-is-dimensions-computation", 
            "text": "Dimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.  Dimensions Computation  provides a way for businesses to perform aggregations on configured numeric data. The  DimensionsComputation  operator works along with the  DimensionStore  operator, which provides the capability for applications to display historical data and trends.", 
            "title": "What is Dimensions Computation?"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-concepts", 
            "text": "", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-set", 
            "text": "A key set is a set of fields in the incoming tuple that is used to combine data for aggregation.", 
            "title": "Key set"
        }, 
        {
            "location": "/operators/dimensions_computation/#value-set", 
            "text": "A value set is the set of fields in the incoming tuple on which  Aggregator (s)  are applied.", 
            "title": "Value set"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregator", 
            "text": "An aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.", 
            "title": "Aggregator"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregates", 
            "text": "Aggregates are objects containing the aggregated values for a configured value set and key combination.", 
            "title": "Aggregates"
        }, 
        {
            "location": "/operators/dimensions_computation/#time-buckets", 
            "text": "Time buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12: 01:34  PM * and * 12: 01:59  PM * will be * 12: 01:00  PM . Similarly, for an hourly time bucket, floor time-value for *  15 :02:34 PM  and *  15 :34:00 PM  will be *  15:00:00  PM .  After calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.", 
            "title": "Time buckets"
        }, 
        {
            "location": "/operators/dimensions_computation/#combinations", 
            "text": "Combinations indicate a group of the keys that are used for aggregate computations.", 
            "title": "Combinations"
        }, 
        {
            "location": "/operators/dimensions_computation/#incremental-aggregators", 
            "text": "Incremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,  SUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value}   {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}", 
            "title": "Incremental Aggregators"
        }, 
        {
            "location": "/operators/dimensions_computation/#on-the-fly-aggregators-otf-aggregators", 
            "text": "On-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,  AVERAGE = {Current_SUM} / {Current_COUNT}", 
            "title": "On-the-fly Aggregators (OTF Aggregators)"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation-operator", 
            "text": "The DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.", 
            "title": "DimensionsComputation operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore-operator", 
            "text": "The DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.", 
            "title": "DimensionsStore operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-use-cases", 
            "text": "Consider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.", 
            "title": "Dimensions Computation use cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#architecture", 
            "text": "Dimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given  DAG  is for a complete Dimensions Computation application.  The operators within Dimensions Computation are described in detail.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation", 
            "text": "The DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the  configuration , the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in  DAG ) for calculating cumulative aggregates.", 
            "title": "DimensionsComputation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore", 
            "text": "The DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:  SUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2  The DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.", 
            "title": "DimensionsStore"
        }, 
        {
            "location": "/operators/dimensions_computation/#query", 
            "text": "The Query operator interfaces with the  pubsub server  of  DataTorrent Gateway . The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.", 
            "title": "Query"
        }, 
        {
            "location": "/operators/dimensions_computation/#queryresult", 
            "text": "The QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the  pubsub server  for publishing to UI widgets.", 
            "title": "QueryResult"
        }, 
        {
            "location": "/operators/dimensions_computation/#dag", 
            "text": "", 
            "title": "DAG "
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-configuration", 
            "text": "", 
            "title": "Dimensions Computation Configuration "
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-definitions", 
            "text": "The configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:", 
            "title": "Configuration Definitions"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-schema-configuration", 
            "text": "The Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:  { keys :[{ name : publisher , type : string , enumValues :[ twitter , facebook , yahoo ]},\n         { name : advertiser , type : string , enumValues :[ starbucks , safeway , mcdonalds ]},\n         { name : location , type : string , enumValues :[ N , LREC , SKY , AL , AK ]}],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : clicks , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : cost , type : double , aggregators :[ SUM , COUNT , AVG ]},\n   { name : revenue , type : double , aggregators :[ SUM , COUNT , AVG ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ publisher ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ advertiser , location ]},\n   { combination :[ publisher , location ]},\n   { combination :[ publisher , advertiser ]},\n   { combination :[ publisher , advertiser , location ]}]\n}  The schema configuration is a JSON string that contains the following information:    keys:  - This contains the set of keys derived from an input tuple. The  name  field stands for the name of the field from input tuple. The  type  can be defined for individual keys. The probable values for individual keys can be provided using  enumValues .    values:  - This contains the set of fields from an input tuple on which aggregates are calculated. The  name  field stands for the name of the field from an input tuple. The  type  can be defined for individual keys. The  aggregators  can be defined separately for individual values. Only configured aggregator functions are executed on values.    timeBuckets:  - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are  \"1m\", \"1h\", \"1d\"    dimensions:  - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in  combination  with the JSON key.  additionalValues  can be used for mentioning additional aggregators for any  value . For example,  impressions:MIN  indicates that for a given combination, calculate \" MIN \" for value \" impression \" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.", 
            "title": "Dimensions Computation Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configurations", 
            "text": "Operator configurations is another set of configuration that can be used to configure individual operators.", 
            "title": "Operator Configurations"
        }, 
        {
            "location": "/operators/dimensions_computation/#properties", 
            "text": "dt.operator.QueryResult.topic:  - This is the name of the topic on which UI widgets listen for results.  dt.operator.Query.topic:  - his is the name of the topic on which Query operator listen for queries.  dt.operator.QueryResult.numRetries  - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.", 
            "title": "Properties"
        }, 
        {
            "location": "/operators/dimensions_computation/#attributes", 
            "text": "dt.operator.DimensionsComputation.attr.PARTITIONER:  - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the  Partitioning  section for details on partitioning.  dt.operator.DimensionsComputation.attr.MEMORY_MB:  - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.  dt.operator.Store.attr.MEMORY_MB:  - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.  dt.port.*.attr.QUEUE_CAPACITY  - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.", 
            "title": "Attributes"
        }, 
        {
            "location": "/operators/dimensions_computation/#visualizing-dimensions-computation", 
            "text": "When Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.  And example of a Dashboard UI Widget is as follows:", 
            "title": "Visualizing Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-dimensions-computation-application", 
            "text": "Consider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.", 
            "title": "Creating Dimensions Computation Application"
        }, 
        {
            "location": "/operators/dimensions_computation/#sample-publisher-event", 
            "text": "An event might look like this:  public class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}", 
            "title": "Sample publisher event"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-an-application-using-out-of-the-box-operators", 
            "text": "Dimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:  @ApplicationAnnotation(name= AdEventDemo )\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA =  adsGenericEventSchema.json ;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString( eventSchema.json );\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver object.\n    AdEventReceiver receiver = dag.addOperator( Event Receiver , AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map String, String  keyToExpression = Maps.newHashMap();\n    keyToExpression.put( advertiser ,  getAdvertiser() );\n    keyToExpression.put( location ,  getLocation() );\n    keyToExpression.put( time ,  getTime() );\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map String, String  valueToExpression = Maps.newHashMap();\n    valueToExpression.put( cost ,  getCost() );\n    valueToExpression.put( revenue ,  getRevenue() );\n    valueToExpression.put( impressions ,  getImpressions() );\n    valueToExpression.put( clicks ,  getClicks() );\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator( Store , AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( dataStorePath );\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator( Query , PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator( QueryResult , PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream( Query , wsIn.outputPort, store.query);\n    dag.addStream( QueryResult , store.queryResult, wsOut.input);\n    dag.addStream( InputStream , receiver.output, dimensions.input);\n    dag.addStream( DimensionalData , dimensions.output, store.input);\n  }\n}", 
            "title": "Creating an Application using out-of-the-box operators"
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-for-sample-predefined-use-cases", 
            "text": "The following configuration can be used for the predefined use case of the advertiser-publisher.", 
            "title": "Configuration for Sample Predefined Use Cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-schema-configuration", 
            "text": "{ keys :[{ name : advertiser , type : string },\n         { name : location , type : string }],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : clicks , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : cost , type : double , aggregators :[ SUM , MAX , MIN ]},\n   { name : revenue , type : double , aggregators :[ SUM , MAX , MIN ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ]},\n   { combination :[ advertiser , location ]}]\n}", 
            "title": "Dimensions Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configuration", 
            "text": "Note:  This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.  ?xml version= 1.0  encoding= UTF-8  standalone= no ?  configuration \n   property \n     name dt.operator.DimensionsComputation.attr.PARTITIONER /name \n     value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value \n   /property \n   property \n     name dt.operator.DimensionsComputation.attr.MEMORY_MB /name \n     value 16384 /value \n   /property \n   property \n      name dt.port.*.attr.QUEUE_CAPACITY /name \n      value 32000 /value \n   /property \n   property \n     name dt.operator.Query.topic /name \n     value AdsEventQuery /value \n   /property \n   property \n     name dt.operator.QueryResult.topic /name \n     value AdsEventQueryResult /value \n   /property  /configuration   The above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.", 
            "title": "Operator Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#advanced-concepts", 
            "text": "", 
            "title": "Advanced Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#partitioning", 
            "text": "The Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the  properties.xml  file, or in the  dt-site.xml  file.  property \n   name dt.operator.DimensionsComputations.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value  /property   This adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of  8  is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.  Along with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:  DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());  Here the unifier used is  DimensionsComputationUnifierImpl  which is an out-of-the-box operator present in the DataTorrent distribution.", 
            "title": "Partitioning "
        }, 
        {
            "location": "/operators/dimensions_computation/#conclusion", 
            "text": "Aggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/operators/file_output/", 
            "text": "AbstractFileOutputOperator\n\n\nThe abstract file output operator in Apache Apex Malhar library \n \nAbstractFileOutputOperator\n writes streaming data to files. The main features of this operator are:\n\n\n\n\nPersisting data to files.\n\n\nAutomatic rotation of files based on:\n\n  a. maximum length of a file.\n\n  b. time-based rotation where time is specified using a count of application windows.\n\n\nFault-tolerance.\n\n\nCompression and encryption of data before it is persisted.\n\n\n\n\nIn this tutorial we will cover the details of the basic structure and implementation of all the above features in \nAbstractFileOutputOperator\n. Configuration items related to each feature are discussed as they are introduced in the section of that feature.\n\n\nPersisting data to files\n\n\nThe principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:\n\n\nfilePath\n: path specifying the directory where files are written.\n\n\nDifferent types of file system that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The file system instance which is used for creating streams is constructed from the \nfilePath\n URI.\n\n\nFileSystem.newInstance(new Path(filePath).toUri(), new Configuration())\n\n\n\n\nTuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.\n\n\nPorts\n\n\n\n\ninput\n: the input port on which tuples to be persisted are received.\n\n\n\n\nstreamsCache\n\n\nThis transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.\n\n\nstreamsCache\n is of type \ncom.google.common.cache.LoadingCache\n. A \nLoadingCache\n has an attached \nCacheLoader\n which is responsible to load value of a key when the key is not present in the cache. Details are explained here- \nCachesExplained\n.\n\n\nThe operator constructs this cache in \nsetup(...)\n. It is built with the following configuration items:\n\n\n\n\nmaxOpenFiles\n: maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit. \nDefault\n: 100\n\n\nexpireStreamAfterAcessMillis\n: expires streams after the specified duration has passed since the stream was last accessed. \nDefault\n: value of attribute- \nOperatorContext.SPIN_MILLIS\n.\n\n\n\n\nAn important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.\n\n\nCacheLoader\n\n\nstreamsCache\n is created with a \nCacheLoader\n that opens an \nFSDataOutputStream\n for a file which is not in the cache. The output stream is opened in either \nappend\n or \ncreate\n mode and the basic logic to determine this is explained by the simple diagram below.\n\n\n\n\nThis process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.\n\n\nFollowing are a few configuration items used for opening the streams:\n\n\n\n\nreplication\n: specifies the replication factor of the output files. \nDefault\n: \nfs.getDefaultReplication(new Path(filePath))\n\n\nfilePermission\n: specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command. \nDefault\n: 0777\n\n\n\n\nRemovalListener\n\n\nA \nGuava\n cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since \nstreamsCache\n is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to \nstreamsCache\n which closes the stream when it is evicted.\n\n\nsetup(OperatorContext context)\n\n\nDuring setup the following main tasks are performed:\n\n\n\n\nFileSystem instance is created.\n\n\nThe cache of streams is created.\n\n\nFiles are recovered (see Fault-tolerance section).\n\n\nStray part files are cleaned (see Automatic rotation section).\n\n\n\n\nprocessTuple(INPUT tuple)\n\n\nThe code snippet below highlights the basic steps of processing a tuple.\n\n\nprotected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}\n\n\n\n\nendWindow()\n\n\nIt should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's \nendWindow()\n.\n\n\nMap\nString, FSFilterStreamContext\n openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}\n\n\n\n\nFSFilterStreamContext\n will be explained with compression and encryption.\n\n\nteardown()\n\n\nWhen any operator in a DAG fails then the application master invokes \nteardown()\n for that operator and its downstream operators. In \nAbstractFileOutputOperator\n we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in \nteardown()\n.\n\n\nAutomatic rotation\n\n\nIn a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.\n\n\nTo help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.\n\n\nPart filename\n\n\nThe filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,\n\n\norigfile.partnum\n\n\nThis naming scheme can be changed by the user. It can be done so by overriding the following method\n\n\nprotected String getPartFileName(String fileName, int part)\n\n\n\n\nThis method is passed the original filename and part number as arguments and should return the part filename.\n\n\nMechanisms\n\n\nThe user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.\n\n\nSize Based\n\n\nWith size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property\n\n\nmaxLength\n\n\nLike any other property this can be set in Java application code or in the property file.\n\n\nTime Based\n\n\nIn time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property\n\n\nrotationWindows\n\n\nsetup(OperatorContext context)\n\n\nWhen an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process\n\n\n\n\nFault-tolerance\n\n\nThere are two issues that should be addressed in order to make the operator fault-tolerant:\n\n\n\n\n\n\nThe operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.\n\n\n\n\n\n\nWhile writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:  \n\n\n\n\nalwaysWriteToTmp\n: enables/disables writing to a temporary file. \nDefault\n: true.\n\n\n\n\nMost of the complexity in the code comes from making this operator fault-tolerant.\n\n\nCheckpointed states needed for fault-tolerance\n\n\n\n\n\n\nendOffsets\n: contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator \nsetup(...)\n and is also used while loading a stream to find out if the operator has seen a file before.\n\n\n\n\n\n\nfileNameToTmpName\n: contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.\n\n\n\n\n\n\nfinalizedFiles\n: contains set of files which were requested to be finalized per window id.\n\n\n\n\n\n\nfinalizedPart\n: contains the latest \npart\n of each file which was requested to be finalized.\n\n\n\n\n\n\nThe use of \nfinalizedFiles\n and \nfinalizedPart\n are explained in detail under \nrequestFinalize(...)\n method.\n\n\nRecovering files\n\n\nWhen the operator is re-deployed, it checks in its \nsetup(...)\n method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the \nendOffsets\n. When it doesn't the operator truncates the file.\n\n\nFor example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.\n\n\nrequestFinalize(String fileName)\n\n\nWhen the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is referred to as \nfinalization\n of files and the method allows the user code to specify when a file is ready for finalization.\n\n\nIn this method, the requested file (or in the case of rotation \n all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to \nfinalizedFiles\n state and updating \nfinalizedPart\n.\n\n\nThe process of \nfinalization\n of all the files which were requested till the window \nw\n is deferred till window \nw\n is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.\n\n\nWhen rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "File Output"
        }, 
        {
            "location": "/operators/file_output/#abstractfileoutputoperator", 
            "text": "The abstract file output operator in Apache Apex Malhar library    AbstractFileOutputOperator  writes streaming data to files. The main features of this operator are:   Persisting data to files.  Automatic rotation of files based on: \n  a. maximum length of a file. \n  b. time-based rotation where time is specified using a count of application windows.  Fault-tolerance.  Compression and encryption of data before it is persisted.   In this tutorial we will cover the details of the basic structure and implementation of all the above features in  AbstractFileOutputOperator . Configuration items related to each feature are discussed as they are introduced in the section of that feature.", 
            "title": "AbstractFileOutputOperator"
        }, 
        {
            "location": "/operators/file_output/#persisting-data-to-files", 
            "text": "The principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:  filePath : path specifying the directory where files are written.  Different types of file system that are implementations of  org.apache.hadoop.fs.FileSystem  are supported. The file system instance which is used for creating streams is constructed from the  filePath  URI.  FileSystem.newInstance(new Path(filePath).toUri(), new Configuration())  Tuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.", 
            "title": "Persisting data to files"
        }, 
        {
            "location": "/operators/file_output/#ports", 
            "text": "input : the input port on which tuples to be persisted are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_output/#streamscache", 
            "text": "This transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.  streamsCache  is of type  com.google.common.cache.LoadingCache . A  LoadingCache  has an attached  CacheLoader  which is responsible to load value of a key when the key is not present in the cache. Details are explained here-  CachesExplained .  The operator constructs this cache in  setup(...) . It is built with the following configuration items:   maxOpenFiles : maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit.  Default : 100  expireStreamAfterAcessMillis : expires streams after the specified duration has passed since the stream was last accessed.  Default : value of attribute-  OperatorContext.SPIN_MILLIS .   An important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.", 
            "title": "streamsCache"
        }, 
        {
            "location": "/operators/file_output/#cacheloader", 
            "text": "streamsCache  is created with a  CacheLoader  that opens an  FSDataOutputStream  for a file which is not in the cache. The output stream is opened in either  append  or  create  mode and the basic logic to determine this is explained by the simple diagram below.   This process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.  Following are a few configuration items used for opening the streams:   replication : specifies the replication factor of the output files.  Default :  fs.getDefaultReplication(new Path(filePath))  filePermission : specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command.  Default : 0777", 
            "title": "CacheLoader"
        }, 
        {
            "location": "/operators/file_output/#removallistener", 
            "text": "A  Guava  cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since  streamsCache  is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to  streamsCache  which closes the stream when it is evicted.", 
            "title": "RemovalListener"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context", 
            "text": "During setup the following main tasks are performed:   FileSystem instance is created.  The cache of streams is created.  Files are recovered (see Fault-tolerance section).  Stray part files are cleaned (see Automatic rotation section).", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#automatic-rotation", 
            "text": "In a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.  To help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.", 
            "title": "Automatic rotation"
        }, 
        {
            "location": "/operators/file_output/#part-filename", 
            "text": "The filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,  origfile.partnum  This naming scheme can be changed by the user. It can be done so by overriding the following method  protected String getPartFileName(String fileName, int part)  This method is passed the original filename and part number as arguments and should return the part filename.", 
            "title": "Part filename"
        }, 
        {
            "location": "/operators/file_output/#mechanisms", 
            "text": "The user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.", 
            "title": "Mechanisms"
        }, 
        {
            "location": "/operators/file_output/#size-based", 
            "text": "With size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property  maxLength  Like any other property this can be set in Java application code or in the property file.", 
            "title": "Size Based"
        }, 
        {
            "location": "/operators/file_output/#time-based", 
            "text": "In time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property  rotationWindows", 
            "title": "Time Based"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context_1", 
            "text": "When an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#fault-tolerance", 
            "text": "There are two issues that should be addressed in order to make the operator fault-tolerant:    The operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.    While writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:     alwaysWriteToTmp : enables/disables writing to a temporary file.  Default : true.   Most of the complexity in the code comes from making this operator fault-tolerant.", 
            "title": "Fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#checkpointed-states-needed-for-fault-tolerance", 
            "text": "endOffsets : contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator  setup(...)  and is also used while loading a stream to find out if the operator has seen a file before.    fileNameToTmpName : contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.    finalizedFiles : contains set of files which were requested to be finalized per window id.    finalizedPart : contains the latest  part  of each file which was requested to be finalized.    The use of  finalizedFiles  and  finalizedPart  are explained in detail under  requestFinalize(...)  method.", 
            "title": "Checkpointed states needed for fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#recovering-files", 
            "text": "When the operator is re-deployed, it checks in its  setup(...)  method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the  endOffsets . When it doesn't the operator truncates the file.  For example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.", 
            "title": "Recovering files"
        }, 
        {
            "location": "/operators/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/file_splitter/#ports", 
            "text": "Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#abstract-methods", 
            "text": "FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/file_splitter/#configuration", 
            "text": "blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/file_splitter/#example-application", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_1", 
            "text": "Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_1", 
            "text": "file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/file_splitter/#example-application_1", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_2", 
            "text": "Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_2", 
            "text": "scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/hdht/", 
            "text": "HDHT\n\n\nSome applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.\n\n\nThe programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are\n\n\n\n\nEmbedded, Hadoop native solution. Does not require install/manage of other services.\n\n\nIntegrates with Apex check-pointing mechanism to provide exactly once guarantee.\n\n\n\n\nThis document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.\n\n\nConcepts\n\n\nWrite Ahead Log\n\n\nEach tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.\n\n\nUncommitted Cache\n\n\nUncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.\n\n\nData Files\n\n\nHDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n- \nTFile\n: Files are written in hadoop \nTfile\n format\n- \nDTFile\n: Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n- \nHFile\n : Files are written in HBase format.\n\n\nMetadata\n\n\nMetadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.\n\n\nPartition (HDHT Bucket)\n\n\nBy default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.\n\n\nInterface\n\n\nHDHT supports two basic operations \nget\n and \nput\n, they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.\n\n\nOperations supported by HDHT are.\n\n\nbyte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);\n\n\n\n\nAll methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.\n\n\n\n\nput\n store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.\n\n\ngetUncommitted\n does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.\n\n\nget\n does a lookup in persisted storage file and return the data. \nNote\n \nget\n does not\n return data from uncommitted cache.\n\n\n\n\nArchitecture\n\n\nPlease refer to \nHDHT Blog\n\nfor the architecture of HDHT.\n\n\nCodec\n\n\nHDHT provides an abstract implementation \nAbstractSinglePortHDHTWriter\n, which uses a user defined codec for serialization and de-serialization.\n\n\npublic interface HDHTCodec\nEVENT\n extends StreamCodec\nEVENT\n\n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}\n\n\n\n\nIt has following methods\n- \ngetKeyBytes\n Return key as a byte array from the event.\n- \ngetValueBytes\n Return value as a byte array from event.\n- \nfromKeyValue\n HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n- \ngetPartition\n This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.\n\n\nConfiguration\n\n\nfileStore\n\n\nThis setting determines the format in which files are written. Default is DTFileImpl.\n\n\nbasePath\n\n\nLocation in HDFS where data files are stored. This is required configuration parameter.\n\n\nProperty File Syntax\n\n\n   \nproperty\n\n     \nname\ndt.operator.{name}.store.basePath\n/name\n\n     \nvalue\n/home/hdhtdatadir\n/value\n\n  \n/property\n\n\n\n\n\nJava API.\n\n\n/* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath(\nHDHTdata\n);\nstore.setFileStore(hdsFile);\n\n\n\n\nmaxFileSize\n\n\nSize of each file. Default value is 134217728134217728 (i.e 128MB).\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.maxFileSize\n/name\n\n   \nvalue\n{value in bytes}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxFileSize(64 * 1024 * 1024);\n\n\n\n\nflushSize\n\n\nHDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.flushSize\n/name\n\n  \nvalue\n{number}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushSize(1000000);\n\n\n\n\nflushIntervalCount\n\n\nThis setting will force data flush even if number of tuples are below \nflushSize\n. Default value is 120 windows.\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.flushIntervalCount\n/name\n\n   \nvalue\n{number of windows}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushIntervalCount(120);\n\n\n\n\nmaxWalFileSize\n\n\nWrite Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.maxWalFileSize\n/name\n\n  \nvalue\n{value in bytes}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxWalFileSize(128 * 1024 * 1024);\n\n\n\n\nExample\n\n\nThis is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.\n\n\nStore Operator\n\n\nHDHT provides following abstract implementations\n\n \nHDHTReader\n - This class implements functionality required for \nget\n, It access HDHT\nin read-only mode.\n\n \nHDHTWriter\n - This class extends functionality of \nHDHTReader\n by adding support for \nput\n,\n  this class also maintains \nuncommitted cache\n, which can be accessed through \ngetUncommitted\n\n  method.\n* \nAbstractSinglePortHDHTWriter\n - This class extends from \nHDHTWriter\n and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.\n\n\nFor this example we will use \nAbstractSinglePortHDHTWriter\n for the store, we need to\nimplement codec which is used by \nAbstractSinglePortHDHTWriter\n for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.\n\n\nImplement a Codec\n\n\n  public static class StringCodec extends KryoSerializableStreamCodec\nString\n implements HDHTCodec\nString\n {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }\n\n\n\n\nThe store operator is implemented as shown below, we will need to provide an implementation of\n\ngetCodec\n, and override \nprocessEvent\n to change default behavior of storing data in HDHT\ndirectly.\n\n\npublic class HDHTWordCounter extends AbstractSinglePortHDHTWriter\nString\n\n{\n  public transient DefaultOutputPort\nPair\nString, Long\n out = new DefaultOutputPort\n();\n  private transient HashMap\nString, AtomicLong\n cache;\n\n  @Override\n  protected HDHTCodec\nString\n getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry\nString, AtomicLong\n entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair\n(word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap\n();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException(\nUnable to flush to HDHT\n);\n    }\n  }\n}\n\n\n\n\nSample Application.\n\n\n@ApplicationAnnotation(name=\nHDHTWordCount\n)\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator(\nReader\n, new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory(\n/home/data\n);\n\n    WordSplitter splitter = dag.addOperator(\nSplitter\n, new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator(\nStore\n, new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator(\nConsole\n, new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\nWALBenchMarkDir\n);\n    store.setFileStore(hdsFile);\n\n    dag.addStream(\nlines\n, gen.output, splitter.input);\n    dag.addStream(\ns1\n, splitter.output, store.input);\n    dag.addStream(\ns2\n, store.out, console.input);\n  }\n}\n\n\n\n\nPerformance tuning\n\n\nEffect of frequent WAL flushes.\n\n\nHDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.\n\n\nApplication Level Cache\n\n\nMaintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.\n\n\nKey design\n\n\nHDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.\n\n\nFor keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is \nS\n bytes and flush is\ntriggered every \nT\n seconds, and HDFS write bandwidth per container is \nB\n bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most \n(T / (S/B))\n files. \n\n\nIf S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.\n\n\nFile Backend\n\n\nPrefer \nDTFile\n backend implementation over \nTFile\n backend implementation if you are going to issue frequent \nget\n operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.\n\n\nLimitations\n\n\n\n\nDynamic Partitioning is not supported yet.\n\n\nWrite to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#hdht", 
            "text": "Some applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.  The programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are   Embedded, Hadoop native solution. Does not require install/manage of other services.  Integrates with Apex check-pointing mechanism to provide exactly once guarantee.   This document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/hdht/#write-ahead-log", 
            "text": "Each tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.", 
            "title": "Write Ahead Log"
        }, 
        {
            "location": "/operators/hdht/#uncommitted-cache", 
            "text": "Uncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.", 
            "title": "Uncommitted Cache"
        }, 
        {
            "location": "/operators/hdht/#data-files", 
            "text": "HDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n-  TFile : Files are written in hadoop  Tfile  format\n-  DTFile : Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n-  HFile  : Files are written in HBase format.", 
            "title": "Data Files"
        }, 
        {
            "location": "/operators/hdht/#metadata", 
            "text": "Metadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.", 
            "title": "Metadata"
        }, 
        {
            "location": "/operators/hdht/#partition-hdht-bucket", 
            "text": "By default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.", 
            "title": "Partition (HDHT Bucket)"
        }, 
        {
            "location": "/operators/hdht/#interface", 
            "text": "HDHT supports two basic operations  get  and  put , they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.  Operations supported by HDHT are.  byte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);  All methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.   put  store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.  getUncommitted  does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.  get  does a lookup in persisted storage file and return the data.  Note   get  does not\n return data from uncommitted cache.", 
            "title": "Interface"
        }, 
        {
            "location": "/operators/hdht/#architecture", 
            "text": "Please refer to  HDHT Blog \nfor the architecture of HDHT.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/hdht/#codec", 
            "text": "HDHT provides an abstract implementation  AbstractSinglePortHDHTWriter , which uses a user defined codec for serialization and de-serialization.  public interface HDHTCodec EVENT  extends StreamCodec EVENT \n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}  It has following methods\n-  getKeyBytes  Return key as a byte array from the event.\n-  getValueBytes  Return value as a byte array from event.\n-  fromKeyValue  HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n-  getPartition  This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.", 
            "title": "Codec"
        }, 
        {
            "location": "/operators/hdht/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/hdht/#filestore", 
            "text": "This setting determines the format in which files are written. Default is DTFileImpl.", 
            "title": "fileStore"
        }, 
        {
            "location": "/operators/hdht/#basepath", 
            "text": "Location in HDFS where data files are stored. This is required configuration parameter.  Property File Syntax      property \n      name dt.operator.{name}.store.basePath /name \n      value /home/hdhtdatadir /value \n   /property   Java API.  /* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath( HDHTdata );\nstore.setFileStore(hdsFile);", 
            "title": "basePath"
        }, 
        {
            "location": "/operators/hdht/#maxfilesize", 
            "text": "Size of each file. Default value is 134217728134217728 (i.e 128MB).  Property File Syntax    property \n    name dt.operator.{name}.maxFileSize /name \n    value {value in bytes} /value \n  /property   Java API.  store.setMaxFileSize(64 * 1024 * 1024);", 
            "title": "maxFileSize"
        }, 
        {
            "location": "/operators/hdht/#flushsize", 
            "text": "HDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.  Property File Syntax  property \n   name dt.operator.{name}.flushSize /name \n   value {number} /value  /property   Java API.  store.setFlushSize(1000000);", 
            "title": "flushSize"
        }, 
        {
            "location": "/operators/hdht/#flushintervalcount", 
            "text": "This setting will force data flush even if number of tuples are below  flushSize . Default value is 120 windows.  Property File Syntax    property \n    name dt.operator.{name}.flushIntervalCount /name \n    value {number of windows} /value \n  /property   Java API.  store.setFlushIntervalCount(120);", 
            "title": "flushIntervalCount"
        }, 
        {
            "location": "/operators/hdht/#maxwalfilesize", 
            "text": "Write Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)  Property File Syntax  property \n   name dt.operator.{name}.maxWalFileSize /name \n   value {value in bytes} /value  /property   Java API.  store.setMaxWalFileSize(128 * 1024 * 1024);", 
            "title": "maxWalFileSize"
        }, 
        {
            "location": "/operators/hdht/#example", 
            "text": "This is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.", 
            "title": "Example"
        }, 
        {
            "location": "/operators/hdht/#store-operator", 
            "text": "HDHT provides following abstract implementations   HDHTReader  - This class implements functionality required for  get , It access HDHT\nin read-only mode.   HDHTWriter  - This class extends functionality of  HDHTReader  by adding support for  put ,\n  this class also maintains  uncommitted cache , which can be accessed through  getUncommitted \n  method.\n*  AbstractSinglePortHDHTWriter  - This class extends from  HDHTWriter  and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.  For this example we will use  AbstractSinglePortHDHTWriter  for the store, we need to\nimplement codec which is used by  AbstractSinglePortHDHTWriter  for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.", 
            "title": "Store Operator"
        }, 
        {
            "location": "/operators/hdht/#implement-a-codec", 
            "text": "public static class StringCodec extends KryoSerializableStreamCodec String  implements HDHTCodec String  {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }  The store operator is implemented as shown below, we will need to provide an implementation of getCodec , and override  processEvent  to change default behavior of storing data in HDHT\ndirectly.  public class HDHTWordCounter extends AbstractSinglePortHDHTWriter String \n{\n  public transient DefaultOutputPort Pair String, Long  out = new DefaultOutputPort ();\n  private transient HashMap String, AtomicLong  cache;\n\n  @Override\n  protected HDHTCodec String  getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry String, AtomicLong  entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair (word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap ();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException( Unable to flush to HDHT );\n    }\n  }\n}  Sample Application.  @ApplicationAnnotation(name= HDHTWordCount )\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator( Reader , new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory( /home/data );\n\n    WordSplitter splitter = dag.addOperator( Splitter , new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator( Store , new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator( Console , new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( WALBenchMarkDir );\n    store.setFileStore(hdsFile);\n\n    dag.addStream( lines , gen.output, splitter.input);\n    dag.addStream( s1 , splitter.output, store.input);\n    dag.addStream( s2 , store.out, console.input);\n  }\n}", 
            "title": "Implement a Codec"
        }, 
        {
            "location": "/operators/hdht/#performance-tuning", 
            "text": "", 
            "title": "Performance tuning"
        }, 
        {
            "location": "/operators/hdht/#effect-of-frequent-wal-flushes", 
            "text": "HDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.", 
            "title": "Effect of frequent WAL flushes."
        }, 
        {
            "location": "/operators/hdht/#application-level-cache", 
            "text": "Maintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.", 
            "title": "Application Level Cache"
        }, 
        {
            "location": "/operators/hdht/#key-design", 
            "text": "HDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.  For keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is  S  bytes and flush is\ntriggered every  T  seconds, and HDFS write bandwidth per container is  B  bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most  (T / (S/B))  files.   If S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.", 
            "title": "Key design"
        }, 
        {
            "location": "/operators/hdht/#file-backend", 
            "text": "Prefer  DTFile  backend implementation over  TFile  backend implementation if you are going to issue frequent  get  operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.", 
            "title": "File Backend"
        }, 
        {
            "location": "/operators/hdht/#limitations", 
            "text": "Dynamic Partitioning is not supported yet.  Write to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "Limitations"
        }, 
        {
            "location": "/operators/kafkaInputOperator/", 
            "text": "Kafka Input Operator\n\n\nIntroduction\n\n\nThis is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is a fault-tolerant and scalable Malhar Operator.\n\n\nWhy is it needed ?\n\n\nKafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.\n\n\nAbstractKafkaInputOperator\n\n\nThis is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.\n\n\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\nmaxTuplesPerWindow\n\n\nControls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE \n\n\n\n\n\n\nidempotentStorageManager\n\n\nThis is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager.\nNoopIdempotentStorageManager\n\n\n\n\n\n\nstrategy\n\n\nOperator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.\n\n\nONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.\n\n\nONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE\n\n\n\n\n\n\nmsgRateUpperBound\n\n\nMaximum messages upper bound. Operator repartitions when the \nmsgProcessedPS\n exceeds this bound. \nmsgProcessedPS\n is the average number of messages processed per second by this operator.\n\n\n\n\n\n\nbyteRateUpperBound\n\n\nMaximum bytes upper bound. Operator repartitions when the \nbytesPS\n exceeds this bound. \nbytesPS\n is the average number of bytes processed per second by this operator.\n\n\n\n\n\n\n\n\noffsetManager\n\n\nThis is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)\n\n\n\n\n\n\nrepartitionInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds\n\n\n\n\n\n\nrepartitionCheckInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds\n\n\n\n\n\n\ninitialPartitionCount\n\n\nWhen the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1\n\n\n\n\n\n\nconsumer\n\n\nThis is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\nvoid emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.\n\n\nKafkaConsumer\n\n\nThis is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.\n\n\nPre-requisites\n\n\nThis operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nzookeeper\n\n\nString\n\n\n\n\nSpecifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere\n\n\nc1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster\n\n\n\n\n\n\ncacheSize\n\n\nint\n\n\n1024\n\n\nMaximum of buffered messages hold in memory.\n\n\n\n\n\n\ntopic\n\n\nString\n\n\ndefault_topic\n\n\nIndicates the name of the topic.\n\n\n\n\n\n\ninitialOffset\n\n\nString\n\n\nlatest\n\n\nIndicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.\n\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\n\n\nvoid commitOffset(): Commit the offsets at checkpoint.\n\n\nMap \nKafkaPartition, Long\n getCurrentOffsets(): Return the current\n    offset status.\n\n\nresetPartitionsAndOffset(Set \nKafkaPartition\n partitionIds,\n    Map \nKafkaPartition, Long\n startOffset): Reset the partitions with\n    partitionIds and offsets with startOffset.\n\n\n\n\nConfiguration Parameters\u00a0for SimpleKafkaConsumer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nbufferSize\n\n\nint\n\n\n1 MB\n\n\nSpecifies the maximum total size of messages for each fetch request.\n\n\n\n\n\n\nmetadataRefreshInterval\n\n\nint\n\n\n30 Seconds\n\n\nInterval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.\n\n\n\n\n\n\nmetadataRefreshRetryLimit\n\n\nint\n\n\n-1\n\n\nSpecifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.\n\n\n\n\n\n\n\n\n\nOffsetManager\n\n\nThis is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\npublic interface OffsetManager\n{\n  public Map\nKafkaPartition, Long\n loadInitialOffsets();\n  public void updateOffsets(Map\nKafkaPartition, Long\n offsetsOfPartitions);\n}\n\n\n\n\nAbstract Methods\n\n\nMap \nKafkaPartition, Long\n loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.\n\n\nupdateOffsets(Map \nKafkaPartition, Long\n offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.\n\n\nPartitioning\n\n\nThe logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.\n\n\nResponse processStats(BatchedOperatorStats stats)\n\n\nThe application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.\n\n\nDefinePartitions\n\n\nBased on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.\n\n\nAbstractSinglePortKafkaInputOperator\n\n\nThis class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.\n\n\nPorts\n\n\noutputPort \nT\n: Tuples extracted from Kafka messages are emitted through\nthis port.\n\n\nAbstract Methods\n\n\nT getTuple(Message msg) : Converts the Kafka message to tuple.\n\n\nConcrete Classes\n\n\n\n\n\n\nKafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.\n\n\n\n\n\n\nKafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.\n\n\n\n\n\n\nApplication Example\n\n\nThis section builds an Apex application using Kafka input operator.\nBelow is the code snippet:\n\n\n@ApplicationAnnotation(name = \nKafkaApp\n)\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator(\nMessageReader\n, new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator(\nOutput\n, new ConsoleOutputOperator());\n\n  dag.addStream(\nMessageData\n, input.outputPort, output.input);\n}\n}\n\n\n\n\nBelow is the configuration for using the earliest offset, \u201ctest\u201d as the topic name and\n\u201clocalhost:2181\u201d as the zookeeper forum:\n\n\nproperty\n\n  \nname\ndt.operator.MessageReader.prop.initialOffset\n/name\n\n  \nvalue\nearliest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.topic\n/name\n\n\nvalue\ntest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.zookeeper\n/nam\n\n\nvalue\nlocalhost:2181\n/value\n\n\n/property\n\n\n\n\n\nPlease note that \nMessageReader\n is the string passed as the first argument to the\n\naddOperator()\n call. The above stanza sets these parameters for this operator\nregardless of the application it resides in; if you want to set them on a\nper-application basis, you can use this instead (where \nKafkaApp\n is the name of\nthe application):\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.initialOffset\n/name\n\n  \nvalue\nearliest\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.topic\n/name\n\n  \nvalue\ntest-topic\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.zookeeper\n/name\n\n  \nvalue\nnode21:2181\n/value\n\n\n/property", 
            "title": "Kafka Input"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafka-input-operator", 
            "text": "", 
            "title": "Kafka Input Operator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#introduction", 
            "text": "This is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is a fault-tolerant and scalable Malhar Operator.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#why-is-it-needed", 
            "text": "Kafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.", 
            "title": "Why is it needed ?"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractkafkainputoperator", 
            "text": "This is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.", 
            "title": "AbstractKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters", 
            "text": "Parameter  Description    maxTuplesPerWindow  Controls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE     idempotentStorageManager  This is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager. NoopIdempotentStorageManager    strategy  Operator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.  ONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.  ONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE    msgRateUpperBound  Maximum messages upper bound. Operator repartitions when the  msgProcessedPS  exceeds this bound.  msgProcessedPS  is the average number of messages processed per second by this operator.    byteRateUpperBound  Maximum bytes upper bound. Operator repartitions when the  bytesPS  exceeds this bound.  bytesPS  is the average number of bytes processed per second by this operator.     offsetManager  This is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)    repartitionInterval  Interval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds    repartitionCheckInterval  Interval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds    initialPartitionCount  When the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1    consumer  This is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods", 
            "text": "void emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafkaconsumer", 
            "text": "This is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.", 
            "title": "KafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#pre-requisites", 
            "text": "This operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters_1", 
            "text": "Parameter  Type  Default  Description    zookeeper  String   Specifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where  c1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster    cacheSize  int  1024  Maximum of buffered messages hold in memory.    topic  String  default_topic  Indicates the name of the topic.    initialOffset  String  latest  Indicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_1", 
            "text": "void commitOffset(): Commit the offsets at checkpoint.  Map  KafkaPartition, Long  getCurrentOffsets(): Return the current\n    offset status.  resetPartitionsAndOffset(Set  KafkaPartition  partitionIds,\n    Map  KafkaPartition, Long  startOffset): Reset the partitions with\n    partitionIds and offsets with startOffset.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters-for-simplekafkaconsumer", 
            "text": "Parameter  Type  Default  Description    bufferSize  int  1 MB  Specifies the maximum total size of messages for each fetch request.    metadataRefreshInterval  int  30 Seconds  Interval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.    metadataRefreshRetryLimit  int  -1  Specifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.", 
            "title": "Configuration Parameters\u00a0for SimpleKafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#offsetmanager", 
            "text": "This is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  public interface OffsetManager\n{\n  public Map KafkaPartition, Long  loadInitialOffsets();\n  public void updateOffsets(Map KafkaPartition, Long  offsetsOfPartitions);\n}", 
            "title": "OffsetManager"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_2", 
            "text": "Map  KafkaPartition, Long  loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.  updateOffsets(Map  KafkaPartition, Long  offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#partitioning", 
            "text": "The logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#response-processstatsbatchedoperatorstats-stats", 
            "text": "The application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.", 
            "title": "Response processStats(BatchedOperatorStats stats)"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#definepartitions", 
            "text": "Based on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.", 
            "title": "DefinePartitions"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractsingleportkafkainputoperator", 
            "text": "This class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.", 
            "title": "AbstractSinglePortKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#ports", 
            "text": "outputPort  T : Tuples extracted from Kafka messages are emitted through\nthis port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_3", 
            "text": "T getTuple(Message msg) : Converts the Kafka message to tuple.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#concrete-classes", 
            "text": "KafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.    KafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.", 
            "title": "Concrete Classes"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#application-example", 
            "text": "This section builds an Apex application using Kafka input operator.\nBelow is the code snippet:  @ApplicationAnnotation(name =  KafkaApp )\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator( MessageReader , new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator( Output , new ConsoleOutputOperator());\n\n  dag.addStream( MessageData , input.outputPort, output.input);\n}\n}  Below is the configuration for using the earliest offset, \u201ctest\u201d as the topic name and\n\u201clocalhost:2181\u201d as the zookeeper forum:  property \n   name dt.operator.MessageReader.prop.initialOffset /name \n   value earliest /value  /property  property  name dt.operator.MessageReader.prop.topic /name  value test /value  /property  property  name dt.operator.MessageReader.prop.zookeeper /nam  value localhost:2181 /value  /property   Please note that  MessageReader  is the string passed as the first argument to the addOperator()  call. The above stanza sets these parameters for this operator\nregardless of the application it resides in; if you want to set them on a\nper-application basis, you can use this instead (where  KafkaApp  is the name of\nthe application):  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.initialOffset /name \n   value earliest /value  /property  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.topic /name \n   value test-topic /value  /property  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.zookeeper /name \n   value node21:2181 /value  /property", 
            "title": "Application Example"
        }, 
        {
            "location": "/operators/snapshot_server/", 
            "text": "Snapshot Server Operators\n\n\nThis document is intended as a guide for understanding and using the Snapshot Server operators.\n\n\nIntroduction\n\n\nThe Snapshot Server is the operator which houses the most recent\nsnapshot of tabular data. The Snapshot Server receives queries from an\nembedded query operator and sends query results to a result\noperator. The most recent tabular data can be sent in two forms: a\nlist of maps and a list of POJOs. We will now describe how to\nvisualize both forms of tabular data.\n\n\nAppDataSnapshotServerMap\n\n\nThe AppDataSnapshotServerMap operator accepts a list of maps. Each map\nrepresents a row of a table. Each key in the map represents a column\nof the table and each value represents the value of a column for a\nparticular row. The name, and type of each column is specified through\nthe snapshotSchemaJSON property. The following is an example of a\nsnapshotSchema:\n\n\n{\n  \nvalues\n: [\n    {\nname\n: \nword\n, \ntype\n: \nstring\n}, \n    {\nname\n: \ncount\n, \ntype\n: \ninteger\n}\n  ]\n}\n\n\n\n\nHere you can see that the table has two columns word and count of\ntypes string and integer respectively. According to this schema it is\nexpected that the Maps in the provided list of Maps will have values\nfor the keys \u201cword\u201d and \u201ccount\u201d.\n\n\nNow that the Snapshot server is configured we need to connect the\nembedded query operator and the result operator. The code snippet\nbelow shows how to do this:\n\n\nString gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\nURI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n\nAppDataSnapshotServerMap snapshotServer = dag.addOperator(\nSnapshotServer\n, new AppDataSnapshotServerMap());\nsnapshotServer.setSnapshotSchemaJSON(snapshotServerJSON);\n\nPubSubWebSocketAppDataQuery wsQuery = new PubSubWebSocketAppDataQuery();\nwsQuery.setUri(uri);\nwsQuery.setTopic(\u201cquery\u201d);\nsnapshotServer.setEmbeddableQueryInfoProvider(wsQuery);\n\nPubSubWebSocketAppDataResult wsResult = dag.addOperator(\nQueryResult\n, new PubSubWebSocketAppDataResult());\nwsResult.setUri(uri);\nwsResult.setTopic(\u201cresult\u201d);\n\ndag.addStream(\nResult\n, snapshotServer.queryResult, queryResultPort);\n\n\n\n\nNote that the \ndt.attr.GATEWAY_CONNECT_ADDRESS\n property needs to be\nspecified in the dt-site.xml and must contain the gateway\u2019s ip\naddress. This value will be automatically set if the application is\nlaunched through dtGateway.\n\n\nAppDataSnapshotServerPOJO\n\n\nThe AppDataSnapshotServerPOJO operator accepts a list of POJOs. Each\nPOJO represents a row of a table. Each field of the POJO represents\nrepresents the value for a particular column. The\nAppDataSnapshotServerPOJO is configured in the same way as the\nAppDataSnapshotServerMap except that there is an addition property\nthat must be set. This additional property is fieldToGetter\nproperty. The fieldToGetter property is a Map from String to\nString. The key is the name of a column as defined in the SchemaJSON\nstring. The value is the java getter method that is required to\nextract the value of that column. Let\u2019s walk through an example of a\nvalid setting for fieldToGetter. If this schema is configured:\n\n\n{\n  \nvalues\n: [\n    {\nname\n: \nword\n, \ntype\n: \nstring\n}, \n    {\nname\n: \ncount\n, \ntype\n: \ninteger\n}\n  ]\n}\n\n\n\n\nA valid setting for fieldToGetter would be this:\n\n\nfieldToGetter.put(\u201cword\u201d, \u201cgetWord()\u201d);\nfieldToGetter.put(\u201ccount\u201d, \u201cgetCount()\u201d);\n\n\n\n\nExamples\n\n\nPlease look at the following java code for example usages of the\noperators mentioned above:\n\n\n\n\nApplicationAppData.java\n\n\nTwitterTrendingHashtagsApplication.java", 
            "title": "Snapshot Server"
        }, 
        {
            "location": "/operators/snapshot_server/#snapshot-server-operators", 
            "text": "This document is intended as a guide for understanding and using the Snapshot Server operators.", 
            "title": "Snapshot Server Operators"
        }, 
        {
            "location": "/operators/snapshot_server/#introduction", 
            "text": "The Snapshot Server is the operator which houses the most recent\nsnapshot of tabular data. The Snapshot Server receives queries from an\nembedded query operator and sends query results to a result\noperator. The most recent tabular data can be sent in two forms: a\nlist of maps and a list of POJOs. We will now describe how to\nvisualize both forms of tabular data.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operators/snapshot_server/#appdatasnapshotservermap", 
            "text": "The AppDataSnapshotServerMap operator accepts a list of maps. Each map\nrepresents a row of a table. Each key in the map represents a column\nof the table and each value represents the value of a column for a\nparticular row. The name, and type of each column is specified through\nthe snapshotSchemaJSON property. The following is an example of a\nsnapshotSchema:  {\n   values : [\n    { name :  word ,  type :  string }, \n    { name :  count ,  type :  integer }\n  ]\n}  Here you can see that the table has two columns word and count of\ntypes string and integer respectively. According to this schema it is\nexpected that the Maps in the provided list of Maps will have values\nfor the keys \u201cword\u201d and \u201ccount\u201d.  Now that the Snapshot server is configured we need to connect the\nembedded query operator and the result operator. The code snippet\nbelow shows how to do this:  String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\nURI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n\nAppDataSnapshotServerMap snapshotServer = dag.addOperator( SnapshotServer , new AppDataSnapshotServerMap());\nsnapshotServer.setSnapshotSchemaJSON(snapshotServerJSON);\n\nPubSubWebSocketAppDataQuery wsQuery = new PubSubWebSocketAppDataQuery();\nwsQuery.setUri(uri);\nwsQuery.setTopic(\u201cquery\u201d);\nsnapshotServer.setEmbeddableQueryInfoProvider(wsQuery);\n\nPubSubWebSocketAppDataResult wsResult = dag.addOperator( QueryResult , new PubSubWebSocketAppDataResult());\nwsResult.setUri(uri);\nwsResult.setTopic(\u201cresult\u201d);\n\ndag.addStream( Result , snapshotServer.queryResult, queryResultPort);  Note that the  dt.attr.GATEWAY_CONNECT_ADDRESS  property needs to be\nspecified in the dt-site.xml and must contain the gateway\u2019s ip\naddress. This value will be automatically set if the application is\nlaunched through dtGateway.", 
            "title": "AppDataSnapshotServerMap"
        }, 
        {
            "location": "/operators/snapshot_server/#appdatasnapshotserverpojo", 
            "text": "The AppDataSnapshotServerPOJO operator accepts a list of POJOs. Each\nPOJO represents a row of a table. Each field of the POJO represents\nrepresents the value for a particular column. The\nAppDataSnapshotServerPOJO is configured in the same way as the\nAppDataSnapshotServerMap except that there is an addition property\nthat must be set. This additional property is fieldToGetter\nproperty. The fieldToGetter property is a Map from String to\nString. The key is the name of a column as defined in the SchemaJSON\nstring. The value is the java getter method that is required to\nextract the value of that column. Let\u2019s walk through an example of a\nvalid setting for fieldToGetter. If this schema is configured:  {\n   values : [\n    { name :  word ,  type :  string }, \n    { name :  count ,  type :  integer }\n  ]\n}  A valid setting for fieldToGetter would be this:  fieldToGetter.put(\u201cword\u201d, \u201cgetWord()\u201d);\nfieldToGetter.put(\u201ccount\u201d, \u201cgetCount()\u201d);", 
            "title": "AppDataSnapshotServerPOJO"
        }, 
        {
            "location": "/operators/snapshot_server/#examples", 
            "text": "Please look at the following java code for example usages of the\noperators mentioned above:   ApplicationAppData.java  TwitterTrendingHashtagsApplication.java", 
            "title": "Examples"
        }, 
        {
            "location": "/app_data_framework/", 
            "text": "App Data Framework User Guide\n\n\nIntroduction\n\n\nThe App Data Framework provides a way for data in Apex applications to be queried and delivered, so that the end user can easily access the application data and visualize it. \n\n\nIn this document, we will first look at a simple example of how an application developer can quickly add this capability to an application. We will then explore the basic building blocks of the App Data Framework, and the data schemas that Apache Apex and DataTorrent RTS support.\n\n\nThis document assumes that the reader has the basic knowledge of Apache Apex.\n\n\nExamples\n\n\nA Simple Example\n\n\nIn Apache Apex Malhar, the Twitter example demonstrates the usage of the App Data Framework. The application calculates top 10 hashtags mentioned in tweets in the last 5 minutes across a 1% random tweet sampling on a rolling window basis.\n\n\nIn DataTorrent RTS, you can create a dashboard for this Twitter application that has a bar chart widget with the current top hashtags in Twitter constantly updated:\n\n\n\n\nThe topology of the Twitter example looks like this:\n\n\n\n\nThe \nSnapshotServer\n operator (along with a Query Operator embedded in it) and the \nQueryResult\n operator enable the latest Twitter top hashtags to be visualizable in the widget. \n\n\nWe will explain them in the following sections.\n\n\nThe code of this Twitter example with App Data support is available \nhere\n\n\nA More Complicated Example\n\n\nIn DataTorrent RTS, the Sales example demonstrates a more complicated usage of the App Data Framework. The application generates random sales events, which has dimension keys of sales channels, region and products. Then it aggregates tax, sales amount and discount across all the key combinations.\n\n\n\n\nThe topology of the Sales example looks like this:\n\n\n\n\nSimilar to the Twitter example, the \nStore\n operator along with the Query Operator embedded in it, and the \nQueryResult\n operator enable the data to be queried and be delivered to the widget for visualization.\n\n\nThe code of this Sales example is available \nhere\n\n\nArchitecture Overview\n\n\nAt a very high level, this is the architecture diagram for the App Data Framework. We will explain each of the components.\n\n\n\n\nApp Data Source\n\n\nEach App Data Source is a queryable unit, and it is represented by three operators in your application. It is possible to have multiple such App Data Sources in one application.\n\n\n\n\nData Source\n\n\nThe Data Source Operator that runs in an Apex application that processes the queries from the Query Operator and gives back the results. In the Twitter example, it is the \nSnapshot Server\n operator. The Java class of this operator is \ncom.datatorrent.lib.appdata.snapshot.AppDataSnapshotServerMap\n in Apex Malhar. In the Sales example, it is the \nStore\n operator. The Java class of this operator is \ncom.datatorrent.contrib.dimensions.AppDataSingleSchemaDimensionStoreHDHT\n, which is provided by DataTorrent RTS.\n\n\nQuery Operator\n\n\nThe Query Operator runs embedded in the Data Source Operator, listens for incoming queries from the Message Bus and hands it over to the Data Source Operator. Note that since the Query Operator is embedded within the Data Source Operator, it will not be visible in the DAG view in \ndtManage\n. In both the Twitter and the Sales examples, the Java class that implements the Query Operator is \ncom.datatorrent.lib.io.PubSubWebSocketAppDataQuery\n.\n\n\nResult Operator\n\n\nThe Result Operator runs in an Apex application that publishes the results from the Data Source Operator to the Message Bus. In the Twitter example, it is the \nQueryResult\n operator. The Java class of this operator in both the Twitter and Sales examples is \ncom.datatorrent.lib.io.PubSubWebSocketAppDataResult\n.\n\n\nMessage Bus\n\n\nBecause the operators can run in any node in the cluster, a Message Bus with a pub-sub mechanism is used for delivery for both the queries and the results. This mechanism requires that the caller sends the query message to a topic that the Query Operator listens to, and the Result Operator sends the query result message passed from the Data Source Operator to a different topic that the caller listens to. Typically, the caller is a web browser.\n\n\nIn DataTorrent RTS, this mechanism is provided by \ndtGateway\n. It uses WebSocket to achieve this so that the queries can be made and the results can be processed directly by a web browser. \n\n\nReferring back to the architecture diagram, the web browser, the Query Operator and the Result Operator all connect to dtGateway via WebSocket, with a pub-sub protocol on top of it.\n\n\nSchemas\n\n\nEach App Data Source must provide a schema that describes what is a valid data query and what is a valid data response, so that the caller knows how to query and how to interpret the response from the App Data Sources. We will describe two schemas that underlie the Twitter example and the Sales example.\n\n\nUI Widgets\n\n\nIn order to allow the end users to visualize the result data in a web browser, we need web-based UI widgets that take user input and send the queries and display the results according to the App Data Sources\u2019 schemas. In DataTorrent RTS, we have a number of UI widgets built in. We will also describe how a user can write their own custom widgets.\n\n\nTechnical Details\n\n\nApp Data Source\n\n\nAs described in the overview, an App Data Source consists of three operators: Data Source, Query and Result.\n\n\nThe Result Operator must implement this interface in \ncom.datatorrent.common.experimental.AppData\n:\n\n\ninterface ConnectionInfoProvider \n{\n  String getAppDataUrl();\n  String getTopic();\n}\n\n\n\n\nThe Query Operator, which is embedded in the Data Store Operator, must implement this interface in \ncom.datatorrent.common.experimental.AppData\n:\n\n\ninterface EmbeddableQueryInfoProvider\nQUERY_TYPE\n extends Operator, ConnectionInfoProvider, Operator.ActivationListener\nOperatorContext\n\n{\n  DefaultOutputPort\nQUERY_TYPE\n getOutputPort();\n\n  void enableEmbeddedMode();\n}\n\n\n\n\nThe port that is returned by the embedded Query Operator\u2019s \ngetOutputPort()\n method will be connected to the input port of the Data Source Operator that is annotated with the marker annotation \ncom.datatorrent.common.experimental.AppData.QueryPort\n.\n\n\nOn the DAG level, the Result Operator needs to connect to an output port of the Data Source Operator that is annotated with the marker annotation \ncom.datatorrent.common.experimental.AppData.ResultPort\n. \n\n\nWith this setup, the Data Source, along with the query topic and the result topic, will be discoverable by STRAM. STRAM will return the information to the caller via its REST API.\n\n\nBelow is the App Data Source discovery information returned by the \n/ws/v2/applications/{appid}\n REST call from dtGateway for the Twitter example:\n\n\n{\n  ...\n  \nappDataSources\n: [\n    {\n      \nname\n: \nSnapshotServer.queryResult\n,\n      \noperatorName\n: \nSnapshotServer\n,\n      \nportName\n: \nqueryResult\n,\n      \nquery\n: {\n        \noperatorName\n: \nSnapshotServer.query\n,\n        \ntopic\n: \nTwitterHashtagQueryDemo\n,\n        \nurl\n: \npubsub\n\n      },\n      \nresult\n: {\n        \nappendQIDToTopic\n: true,\n        \noperatorName\n: \nQueryResult\n,\n        \u201ctopic\n: \nTwitterHashtagQueryResultDemo\n,\n        \nurl\n: \npubsub\n\n      }\n    }, ... ]\n  ...\n}\n\n\n\n\nThe \nname\n field contains the name of the Data Source, and it has the following form: \ndataSourceOperatorName\n.\nresultPortName\n.\n\n\nThe \noperatorName\n field is the Data Source operator name.\n\n\nThe \nportName\n field is the result port name of the Data Source operator.\n\n\nThe \nquery\n field describes the query mechanism, including the topic. \n\n\nThe \nresult\n field describes the result mechanism, including the topic and whether the query ID will be appended to the topic. Setting \nappendQIDToTopic\n to true increases the granularity of topics so that callers can receive results only for their queries. If the cost of creating such a large number of topics is high, it can be set to false; callers then will need to filter the results suitably.\n\n\nMessage Bus\n\n\ndtGateway has a WebSocket service that provides the pubsub mechanism required by the Message Bus. The URL is \nws://{dtGatewayAddress}/pubsub\n. You can publish data with a topic by sending a WebSocket message like this:\n\n\n{\n  \u201ctype\u201d: \u201cpublish\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}\n\n\n\n\nThe \ntopic\n field is the topic name, and the \ndata\n field is the payload and can be any JSON object.\n\n\nAll client that have subscribed to the topic will receive the data. The format is:\n\n\n{\n  \u201ctype\u201d: \u201cdata\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}\n\n\n\n\nAnd to subscribe to the topic, the client must send this message to dtGateway using WebSocket:\n\n\n{\n  \u201ctype\u201d: \u201csubscribe\u201d, \u201ctopic\u201d: \u201c{topic}\u201d\n}\n\n\n\n\nYou can try it with dtGateway using a web browser with the help of a WebSocket client (e.g. \u201cSimple WebSocket Client\u201d extension in Google Chrome).\n\n\nNote that dtGateway does not keep the history of the messages. \n\n\nInteraction between Data Source and Web Browser\n\n\nThe web browser, the Query Operator and the Result Operator all connect to dtGateway via the Message Bus. The following describes the step-by-step interaction among them on a high level:\n\n\n\n\nThe UI widget in the browser subscribes to the result topic\n\n\nThe UI widget sends a schema query to the query topic.\n\n\nThe Data Source Operator gets the schema query and sends the schema result to the result topic, which the UI widget receives.\n\n\nThe UI widget in the browser subscribes to the result topic\n\n\nThe UI widget sends a data query\n\n\nThe Data Source Operator gets the data query and processes it and sends back the data result to the result topic.\n\n\nThe UI widget receives the data result and renders it.\n\n\n\n\nLet\u2019s look at the actual messages being exchanged for the Twitter example.\n\n\nSnapshot Schema\n\n\nThe Snapshot Schema serves a simple snapshot of tabular data, which is typically constantly being updated and is used in the Twitter example. We will describe the Snapshot schema as we look at the messages.\n\n\nStep 1: Browser sends Schema Response Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.20716154835833223\n\n}\n\n\n\n\nIn preparation for a Schema Query, the browser first subscribes to the result topic (with the query ID appended) to get the Schema Result back. In this case, the browser generates a random string, \u201c0.20716154835833223\u201d for the query ID, and appends the query ID to the result topic that it is subscribing to. \n\n\nStep 2: Browser sends Schema Query\n\n\n{\n  \ntype\n: \npublish\n,\n  \ntopic\n: \nTwitterHashtagQueryDemo\n,\n  \ndata\n: {\n    \nid\n: 0.20716154835833223,\n    \ntype\n: \nschemaQuery\n\n  }\n}\n\n\n\n\nThe browser sends a message with type, \nschemaQuery\n. This is the query that asks for the schema type and schema data. \n\n\nStep 3: Data Source sends Schema Response\n\n\n{\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.20716154835833223\n,\n  \ndata\n: {\n    \nid\n: \n0.20716154835833223\n,\n    \ntype\n: \nschemaResult\n,\n    \ndata\n: [\n      {\n        \nvalues\n: [\n          {\n            \nname\n: \nhashtag\n,\n            \ntype\n: \nstring\n\n          },\n          {\n            \nname\n: \ncount\n,\n            \ntype\n: \ninteger\n\n          }\n        ],\n        \nschemaType\n: \nsnapshot\n,\n        \nschemaVersion\n: \n1.0\n\n      }\n    ]\n  },\n  \ntype\n: \ndata\n\n}\n\n\n\n\nThe Data Source sends back the \nschemaResult\n, which contains the response to \nschemaQuery\n, which contains schema type and schema data. In this case, schemaType is snapshot and schemaVersion is 1.0, and the available fields in the data are \u201chashtag\u201d, which is a string, and \u201ccount\u201d, which is an integer.\n\n\nStep 4: Browser sends Data Result Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.6760250790172551\n\n}\n\n\n\n\nSimilar to the previous subscribe message, the browser subscribes to the result topic to get the response for the \ndataQuery\n that the browser is going to issue.\n\n\nStep 5: Browser sends Data Query Request\n\n\n{\n  \ntype\n: \npublish\n,\n  \ntopic\n: \nTwitterHashtagQueryDemo\n,\n  \ndata\n: {\n    \nid\n: 0.6760250790172551,\n    \ntype\n: \ndataQuery\n,\n    \ndata\n: {\n      \nfields\n: [\n        \nhashtag\n,\n        \ncount\n\n      ]\n    },\n    \ncountdown\n: 30,\n    \nincompleteResultOK\n: true\n  }\n}\n\n\n\n\nThe browser sends the \ndataQuery\n message that asks for actual data.\n\n\nThe \ncountdown\n field expects an optional integer value. It tells the Data Source that the Data Source should return results once for subsequent {countdown} application windows. In this example, the value is 30. That means the Data Source should execute this query 30 times, once for each application window for the next 30 application windows.\n\n\nThe \nincompleteResultOK\n field is an optional boolean value default to be false. If the value is true, the Data Source should return results as soon as they are available even if they are partial results. This is useful when {countdown} is greater than 1 and the Data Source could take a long time to return the complete result set and if it is desirable for the caller to receive the results as soon as possible. If this value is false, Data Source should return the complete result set to the caller\n\n\nWithin the \ndata\n field, the \nfields\n field tells the Data Source what fields to return in the result.\n\n\nStep 6: Data Source sends Data Response\n\n\n{\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.6760250790172551\n,\n  \ndata\n: {\n    \nid\n: \n0.6760250790172551\n,\n    \ntype\n: \ndataResult\n,\n    \ndata\n: [\n      {\n        \ncount\n: \n1398\n,\n        \nhashtag\n: \n\uc0ac\uc124\ud1a0\ud1a0\ucd94\ucc9c\uc0ac\uc774\ud2b8\n\n      },\n      {\n        \ncount\n: \n1415\n,\n        \nhashtag\n: \nTopDance\n\n      },\n      {\n        \ncount\n: \n1498\n,\n        \nhashtag\n: \nisola\n\n      },\n      {\n        \ncount\n: \n1521\n,\n        \nhashtag\n: \nRT\u3057\u305f\u4eba\u5168\u54e1\u30d5\u30a9\u30ed\u30fc\u3059\u308b\n\n      },\n      {\n        \ncount\n: \n1728\n,\n        \nhashtag\n: \n4\u670819\u65e5\u306f\u897f\u6728\u91ce\u771f\u59eb\u306e\u8a95\u751f\u65e5\n\n      },\n      {\n        \ncount\n: \n1787\n,\n        \nhashtag\n: \n\ub124\uc784\ub4dc\uc0ac\ub2e4\ub9ac\n\n      },\n      {\n        \ncount\n: \n2079\n,\n        \nhashtag\n: \nALDUBActing101\n\n      },\n      {\n        \ncount\n: \n2280\n,\n        \nhashtag\n: \n\u5730\u9707\n\n      },\n      {\n        \ncount\n: \n2712\n,\n        \nhashtag\n: \n\u897f\u6728\u91ce\u771f\u59eb\u751f\u8a95\u796d2016\n\n      },\n      {\n        \ncount\n: \n2714\n,\n        \nhashtag\n: \n\u0634\u0639\u064a\u0628_\u064a\u0633\u064a\u0621_\u0644\u0644\u0633\u0639\u0648\u062f\u064a\u0647\n\n      }\n    ],\n    \ncountdown\n: \n29\n\n  },\n  \ntype\n: \ndata\n\n}\n\n\n\n\nThe Data Source sends the \ndataResult\n message in response to the \ndataQuery\n. In this case, it contains a list of (hashtag, count) sets in the \ndata\n field.\n\n\nIf the \ncountdown\n field of the \ndataQuery\n is non-zero, there will be multiple such messages that correspond to one \ndataQuery\n message.  The \ncountdown\n field in the \ndataResult\n message decrements every time it sends a result.  When it counts down to zero, the query has expired and the client must send another \ndataQuery\n message to receive further updates.\n\n\nIn this case, because the countdown value is 30 in the \ndataQuery\n, the Data Source will keep sending the latest data set for the next 29 application windows following this initial response.\n\n\nFor more information about the operators behind this Snapshot Schema, please refer to \nthis document\n.\n\n\nDimensions Schema\n\n\nThe Dimensions Schema is an extension of the Snapshot Schema with the notion of time and key dimensions. It is supported in DataTorrent RTS. \n\n\nWe will look at the message exchange in the Sales example.\n\n\nStep 1: Browser sends Schema Response Subscribe\n\n\n{\ntype\n:\nsubscribe\n,\ntopic\n:\nSalesQueryResultDemo.0.676153457723558\n}\n\n\n\n\nThis is similar to the Snapshot example we saw earlier.\n\n\nStep 2: Browser sends Schema Query\n\n\n{\n   \ntype\n:\npublish\n,\n   \ntopic\n:\nSalesQueryDemo\n,\n   \ndata\n:{\n      \nid\n: 0.22710832906886935,\n      \ntype\n:\nschemaQuery\n\n   }\n}\n\n\n\n\nAgain, this is similar to the Snapshot example we saw earlier.\n\n\nStep 3: Data Source sends Schema Response.\n\n\n{\n   \ntopic\n:\nSalesQueryResultDemo.0.22710832906886935\n,\n   \ndata\n:{\n      \nid\n:\n0.22710832906886935\n,\n      \ntype\n:\nschemaResult\n,\n      \ndata\n:[\n         {\n            \nschemaType\n:\ndimensions\n,\n            \nschemaVersion\n:\n1.0\n,\n            \ntime\n:{\n               \nbuckets\n:[\n1m\n,\n1h\n,\n1d\n,\n5m\n],\n               \nslidingAggregateSupported\n:true\n            },\n            \nkeys\n:[\n               {\n                  \nname\n:\nchannel\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nMobile\n, \nOnline\n, \nStore\n]\n               },\n               {\n                  \nname\n:\nregion\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nAtlanta\n, \nBoston\n,\nChicago\n,\nCleveland\n,\nDallas\n,\n                     \nMinneapolis\n,\nNew York\n,\nPhiladelphia\n,\nSan Francisco\n,\nSt. Louis\n]\n               },\n               {\n                  \nname\n:\nproduct\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nLaptops\n,\nPrinters\n,\nRouters\n,\nSmart Phones\n,\nTablets\n]\n               }\n            ],\n            \nvalues\n:[\n               {\n                  \nname\n:\ntax:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nsales:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ndiscount:SUM\n,\n                  \ntype\n:\ndouble\n\n               }\n            ],\n            \ndimensions\n:[\n               {\n                  \ncombination\n:[\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nregion\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nregion\n,\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nregion\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nregion\n,\n                     \nchannel\n\n                  ]\n               }\n            ]\n         }\n      ]\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nCompared to the Snapshot Schema, this is considerably more complex. The response to the query describes the keys and aggregates that are provided by the data source. The basic components of the schema are the following:\n\n\ntimebuckets\n: These are the time buckets over which aggregations are computed. For example, 1d (one day), 1h (one hour), 1m (one minute), 1s (one second).\n\n\nkeys\n: These are the search parameters you need to provide when you do a query. The keys have an enumValues. This is an optional section which contains all possible values of a key. This section may be updated by the back end if new key values are encountered.\n\n\nvalues\n: These are the results which are returned by a query. Notice that the values are of the form (name):(aggregation). The first portion of the value name describes what is being aggregated. The second portion of the value name describes the aggregation being used.\n\n\ncombinations\n: This describes the valid combinations of keys that can be specified when doing a query.\n\n\nStep 4: Browser sends Data Result Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nSalesQueryResultDemo.0.3180227109696716\n\n}\n\n\n\n\nStep 5: Browser sends Data Query Request\n\n\nNow that the valid combinations, keys, values, and timebuckets are known, we can issue a query using them. \n\n\n{\n   \ntype\n:\npublish\n,\n   \ntopic\n:\nSalesQueryDemo\n,\n   \ndata\n:{\n      \nid\n: \n0.3180227109696716\n,\n      \ntype\n:\ndataQuery\n,\n      \ndata\n:{\n         \ntime\n:{\n            \nlatestNumBuckets\n:10,\n            \nbucket\n:\n1m\n\n         },\n         \nincompleteResultOK\n:true,\n         \nkeys\n:{\n\n         },\n         \nfields\n:[\n            \ntime\n,\n            \nchannel\n,\n            \nregion\n,\n            \nproduct\n,\n            \ntax:SUM\n,\n            \nsales:SUM\n,\n            \ndiscount:SUM\n\n         ]\n      },\n      \ncountdown\n:29,\n      \nincompleteResultOK\n:true\n   }\n}\n\n\n\n\ntime\n: This specifies which time buckets to query. In this case the most recent 10 one minute time buckets are queried. The example above illustrates how to query the last N timebuckets, but it is also possible to do history queries by specifying a time section like the following:\n\n\n        \ntime\n:{\n         \nbucket\n:\n1m\n,\n         \nfrom\n:1460765563547,\n         \nto\n:1460767363547\n       }\n\n\n\n\nThis time section specifies a historical query starting at the \u201cfrom\u201d unix timestamp (inclusive) up until the \u201cto\u201d timestamp (inclusive).\n\n\nkeys\n: These are the search parameters. This section can be empty to query global aggregations. If there are specific key combinations you want to search for you can do this:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: \u201cAtlanta\u201d\n}\n\n\n\n\nIt is possible to specify a key with a list like the following:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: [ \u201cAtlanta\u201d, \u201cDallas\u201d ]\n}\n\n\n\n\nThis will query all the data with a \nchannel\n value of \u201cMobile\u201d and a \nregion\n value of \u201cAtlanta\u201d OR \u201cDallas\u201d. It is also possible to specify a key with an empty array like the following:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: []\n}\n\n\n\n\nThis will query all the data with a \nchannel\n value of \u201cMobile\u201d and ANY value of \nregion\n.\nNote that this is different from not specifying the \nregion\n value at all, in which case, the aggregated value of all regions will be returned. This is similar to the difference between the SQL queries of \nSELECT SUM(sales) WHERE channel=\u2019Mobile\u2019\n (for the case of not specifying \nregion\n) and \nSELECT SUM(sales), region WHERE channel=\u2019Mobile\u2019 GROUP BY region\n (for the case of \nregion\n being an empty array).\n\n\nfields\n: This specifies what data to include in the query result. If the field \ntime\n is included, then a timestamp is included for each data point. Similarly the keys can also be specified as fields with the results. Lastly aggregated values can also be specified as fields to be returned in results.\n\n\nStep 6: Data Source sends Data Response.\n\n\nAfter a query is issued, results are periodically returned:\n\n\n{\n   \ntopic\n:\nSalesQueryResultDemo.0.3180227109696716\n,\n   \ndata\n: {\n      \nid\n:\n0.3180227109696716\n,\n      \ntype\n:\ndataResult\n,\n      \ndata\n:[\n         {\n            \ntime\n:\n1460771580000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n78363.47\n,\n            \nsales:SUM\n:\n922988.9199999997\n,\n            \ndiscount:SUM\n:\n69135.71999999999\n\n         },\n         {\n            \ntime\n:\n1460771640000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n142011.85\n,\n            \nsales:SUM\n:\n1672677.0099999998\n,\n            \ndiscount:SUM\n:\n125287.46999999999\n\n         },\n         {\n            \ntime\n:\n1460771700000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n139069.53999999998\n,\n            \nsales:SUM\n:\n1638011.5399999998\n,\n            \ndiscount:SUM\n:\n122692.48999999999\n\n         },\n         {\n            \ntime\n:\n1460771760000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n80699.90000000001\n,\n            \nsales:SUM\n:\n950502.3500000003\n,\n            \ndiscount:SUM\n:\n71196.95000000001\n\n         },\n         {\n            \ntime\n:\n1460771820000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n35914.69\n,\n            \nsales:SUM\n:\n423017.9000000001\n,\n            \ndiscount:SUM\n:\n69339.05000000002\n\n         },\n         {\n            \ntime\n:\n1460771880000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n106977.84999999998\n,\n            \nsales:SUM\n:\n1260017.5600000003\n,\n            \ndiscount:SUM\n:\n629939.14\n\n         },\n         {\n            \ntime\n:\n1460771940000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n253243.50000000003\n,\n            \nsales:SUM\n:\n2982796.9300000006\n,\n            \ndiscount:SUM\n:\n1491234.3599999999\n\n         },\n         {\n            \ntime\n:\n1460772000000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n161384.77999999997\n,\n            \nsales:SUM\n:\n1900857.2300000002\n,\n            \ndiscount:SUM\n:\n950325.01\n\n         },\n         {\n            \ntime\n:\n1460772060000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n125283.99000000002\n,\n            \nsales:SUM\n:\n1475625.3600000003\n,\n            \ndiscount:SUM\n:\n737730.77\n\n         },\n         {\n            \ntime\n:\n1460772120000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n58684.59\n,\n            \nsales:SUM\n:\n691200.9299999999\n,\n            \ndiscount:SUM\n:\n345562.2700000001\n\n         }\n      ],\n      \ncountdown\n:\n297\n\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nAs you can see each result represents a time bucket with the corresponding values for the fields. If the ANY value (with an empty list) query is specified for a key you will see multiple results for the same time bucket, but each result will have different values for the keys.\n\n\nDetailed documentation of the dimensions operators can be found here: \nhttp://docs.datatorrent.com/operators/dimensions_computation/\n\n\nAdvanced Topics\n\n\nSchema Keys\n\n\nAlthough in most cases, a Data Source has one fixed schema, it is sometimes necessary for a Data Source to support multiple schemas. This is useful if the schemas are dynamically created depending on the incoming data to the Data Source Operator.\n\n\nSchema Key is the feature we added to the App Data Framework for this purpose. If the Data Source has multiple schemas, the \nschemaResult\n will contain a list of schemas with the corresponding schema keys in the \ncontext\n field:\n\n\n{\n  \u201cid\u201d: \u201c{client_generated_id}\u201d,\n  \u201ctype\u201d: \u201cschemaResult\u201d,\n  \u201cdata\u201d: [ \n    {\n      \u201ccontext\u201d: {\n        \u201cschemaKeys\u201d: {\n          \u201c{key1}\u201d: \u201c{value1}\u201d,\n          \u201c{key2}\u201d: \u201c{value2}\u201d, ...\n         }\n      },\n      {rest of the schema}\n    }, ...\n  ]\n}\n\n\n\n\nAnd if the \nschemaResult\n returns multiple schemas, all \ndataQuery\n messages must include the schema keys:\n\n\n{\n   \u201cid\u201d: \u201c{client_generated_id}\u201d,\n   \u201ctype\u201d: \u201cdataQuery\u201d, \n   \u201cdata\u201d: {\n     \u201ccontext\u201d: {\n       \u201cschemaKeys\u201d { //optional\n    \u201c{keyName1}\u201d: \u201c{keyValue1}\u201d,\n    \u201c{keyName2}\u201d: \u201c{keyValue2}\u201d \n       }\n     }\n     {rest of the query}\n   }\n   \u201ccountdown\u201d: \u201c{countdown}\u201d,\n   \u201cincompleteResultOK\u201d: true/false\n}\n\n\n\n\nThis feature is supported by the DimensionsStore operator and is being used by the App Data Tracker.\n\n\nDimensions Schema: Additional Values for Combinations\n\n\nFor the Dimensions Schema supported by the DimensionsStore operator, there can be more complex schema specifications which have results for values only for certain combinations. This is the schema for yet another example about aggregation of advertisement data in the DataTorrent RTS example repository.\n\n\n{\n   \ntopic\n:\nAdsQueryGenericResultDemo.0.08170037856325507\n,\n   \ndata\n:{\n      \nid\n:\n0.08170037856325507\n,\n      \ntype\n:\nschemaResult\n,\n      \ndata\n:[\n         {\n            \nschemaType\n:\ndimensions\n,\n            \nschemaVersion\n:\n1.0\n,\n            \ntime\n:{\n               \nbuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n               \nslidingAggregateSupported\n:true,\n               \nfrom\n:\n1459987200000\n,\n               \nto\n:\n1460768100000\n\n            },\n            \nkeys\n:[\n               {\n                  \nname\n:\npublisher\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n,\ngoogle\n,\nbing\n,\namazon\n\n                  ]\n               },\n               {\n                  \nname\n:\nadvertiser\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n,\nmacys\n,\ntaco bell\n,\n                     \nwalmart\n,\nkohls\n,\nsan diego zoo\n,\npandas\n,\njack in the box\n,\n                     \ntomatina\n,\nron swanson\n\n                  ]\n               },\n               {\n                  \nname\n:\nlocation\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\n                     \nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n,\nAZ\n,\nAR\n,\nCA\n,\nCO\n,\nCT\n,\nDE\n,\nFL\n,\n                     \nGA\n,\nHI\n,\nID\n\n                  ]\n               }\n            ],\n            \nvalues\n:[\n               {\n                  \nname\n:\nimpressions:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nimpressions:SUM\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nimpressions:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nclicks:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nclicks:SUM\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nclicks:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ncost:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\ncost:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ncost:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nrevenue:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nrevenue:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nrevenue:AVG\n,\n                  \ntype\n:\ndouble\n\n               }\n            ],\n            \ndimensions\n:[\n               {\n                  \ncombination\n:[\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nadvertiser\n\n                  ],\n                  \nadditionalValues\n:[\n                     {\n                        \nname\n:\nimpressions:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nimpressions:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\ncost:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\ncost:MIN\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MIN\n,\n                        \ntype\n:\ndouble\n\n                     }\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \npublisher\n\n                  ],\n                  \nadditionalValues\n:[\n                     {\n                        \nname\n:\nimpressions:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nimpressions:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\ncost:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\ncost:MIN\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MIN\n,\n                        \ntype\n:\ndouble\n\n                     }\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \nadvertiser\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \npublisher\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nadvertiser\n,\n                     \npublisher\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \nadvertiser\n,\n                     \npublisher\n\n                  ]\n               }\n            ]\n         }\n      ]\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nHere you can see that a combination can also have \nadditionalValues\n specified. Additional values are values which are only available for a specific combination of keys.\n\n\nThe code for the ads example is available \nhere\n.\n\n\nUI Widgets\n\n\nComing soon\n\n\nApp Data Tracker\n\n\nApp Data Tracker is discussed \nhere\n.", 
            "title": "App Data Framework"
        }, 
        {
            "location": "/app_data_framework/#app-data-framework-user-guide", 
            "text": "", 
            "title": "App Data Framework User Guide"
        }, 
        {
            "location": "/app_data_framework/#introduction", 
            "text": "The App Data Framework provides a way for data in Apex applications to be queried and delivered, so that the end user can easily access the application data and visualize it.   In this document, we will first look at a simple example of how an application developer can quickly add this capability to an application. We will then explore the basic building blocks of the App Data Framework, and the data schemas that Apache Apex and DataTorrent RTS support.  This document assumes that the reader has the basic knowledge of Apache Apex.", 
            "title": "Introduction"
        }, 
        {
            "location": "/app_data_framework/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/app_data_framework/#a-simple-example", 
            "text": "In Apache Apex Malhar, the Twitter example demonstrates the usage of the App Data Framework. The application calculates top 10 hashtags mentioned in tweets in the last 5 minutes across a 1% random tweet sampling on a rolling window basis.  In DataTorrent RTS, you can create a dashboard for this Twitter application that has a bar chart widget with the current top hashtags in Twitter constantly updated:   The topology of the Twitter example looks like this:   The  SnapshotServer  operator (along with a Query Operator embedded in it) and the  QueryResult  operator enable the latest Twitter top hashtags to be visualizable in the widget.   We will explain them in the following sections.  The code of this Twitter example with App Data support is available  here", 
            "title": "A Simple Example"
        }, 
        {
            "location": "/app_data_framework/#a-more-complicated-example", 
            "text": "In DataTorrent RTS, the Sales example demonstrates a more complicated usage of the App Data Framework. The application generates random sales events, which has dimension keys of sales channels, region and products. Then it aggregates tax, sales amount and discount across all the key combinations.   The topology of the Sales example looks like this:   Similar to the Twitter example, the  Store  operator along with the Query Operator embedded in it, and the  QueryResult  operator enable the data to be queried and be delivered to the widget for visualization.  The code of this Sales example is available  here", 
            "title": "A More Complicated Example"
        }, 
        {
            "location": "/app_data_framework/#architecture-overview", 
            "text": "At a very high level, this is the architecture diagram for the App Data Framework. We will explain each of the components.", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/app_data_framework/#app-data-source", 
            "text": "Each App Data Source is a queryable unit, and it is represented by three operators in your application. It is possible to have multiple such App Data Sources in one application.", 
            "title": "App Data Source"
        }, 
        {
            "location": "/app_data_framework/#data-source", 
            "text": "The Data Source Operator that runs in an Apex application that processes the queries from the Query Operator and gives back the results. In the Twitter example, it is the  Snapshot Server  operator. The Java class of this operator is  com.datatorrent.lib.appdata.snapshot.AppDataSnapshotServerMap  in Apex Malhar. In the Sales example, it is the  Store  operator. The Java class of this operator is  com.datatorrent.contrib.dimensions.AppDataSingleSchemaDimensionStoreHDHT , which is provided by DataTorrent RTS.", 
            "title": "Data Source"
        }, 
        {
            "location": "/app_data_framework/#query-operator", 
            "text": "The Query Operator runs embedded in the Data Source Operator, listens for incoming queries from the Message Bus and hands it over to the Data Source Operator. Note that since the Query Operator is embedded within the Data Source Operator, it will not be visible in the DAG view in  dtManage . In both the Twitter and the Sales examples, the Java class that implements the Query Operator is  com.datatorrent.lib.io.PubSubWebSocketAppDataQuery .", 
            "title": "Query Operator"
        }, 
        {
            "location": "/app_data_framework/#result-operator", 
            "text": "The Result Operator runs in an Apex application that publishes the results from the Data Source Operator to the Message Bus. In the Twitter example, it is the  QueryResult  operator. The Java class of this operator in both the Twitter and Sales examples is  com.datatorrent.lib.io.PubSubWebSocketAppDataResult .", 
            "title": "Result Operator"
        }, 
        {
            "location": "/app_data_framework/#message-bus", 
            "text": "Because the operators can run in any node in the cluster, a Message Bus with a pub-sub mechanism is used for delivery for both the queries and the results. This mechanism requires that the caller sends the query message to a topic that the Query Operator listens to, and the Result Operator sends the query result message passed from the Data Source Operator to a different topic that the caller listens to. Typically, the caller is a web browser.  In DataTorrent RTS, this mechanism is provided by  dtGateway . It uses WebSocket to achieve this so that the queries can be made and the results can be processed directly by a web browser.   Referring back to the architecture diagram, the web browser, the Query Operator and the Result Operator all connect to dtGateway via WebSocket, with a pub-sub protocol on top of it.", 
            "title": "Message Bus"
        }, 
        {
            "location": "/app_data_framework/#schemas", 
            "text": "Each App Data Source must provide a schema that describes what is a valid data query and what is a valid data response, so that the caller knows how to query and how to interpret the response from the App Data Sources. We will describe two schemas that underlie the Twitter example and the Sales example.", 
            "title": "Schemas"
        }, 
        {
            "location": "/app_data_framework/#ui-widgets", 
            "text": "In order to allow the end users to visualize the result data in a web browser, we need web-based UI widgets that take user input and send the queries and display the results according to the App Data Sources\u2019 schemas. In DataTorrent RTS, we have a number of UI widgets built in. We will also describe how a user can write their own custom widgets.", 
            "title": "UI Widgets"
        }, 
        {
            "location": "/app_data_framework/#technical-details", 
            "text": "", 
            "title": "Technical Details"
        }, 
        {
            "location": "/app_data_framework/#app-data-source_1", 
            "text": "As described in the overview, an App Data Source consists of three operators: Data Source, Query and Result.  The Result Operator must implement this interface in  com.datatorrent.common.experimental.AppData :  interface ConnectionInfoProvider \n{\n  String getAppDataUrl();\n  String getTopic();\n}  The Query Operator, which is embedded in the Data Store Operator, must implement this interface in  com.datatorrent.common.experimental.AppData :  interface EmbeddableQueryInfoProvider QUERY_TYPE  extends Operator, ConnectionInfoProvider, Operator.ActivationListener OperatorContext \n{\n  DefaultOutputPort QUERY_TYPE  getOutputPort();\n\n  void enableEmbeddedMode();\n}  The port that is returned by the embedded Query Operator\u2019s  getOutputPort()  method will be connected to the input port of the Data Source Operator that is annotated with the marker annotation  com.datatorrent.common.experimental.AppData.QueryPort .  On the DAG level, the Result Operator needs to connect to an output port of the Data Source Operator that is annotated with the marker annotation  com.datatorrent.common.experimental.AppData.ResultPort .   With this setup, the Data Source, along with the query topic and the result topic, will be discoverable by STRAM. STRAM will return the information to the caller via its REST API.  Below is the App Data Source discovery information returned by the  /ws/v2/applications/{appid}  REST call from dtGateway for the Twitter example:  {\n  ...\n   appDataSources : [\n    {\n       name :  SnapshotServer.queryResult ,\n       operatorName :  SnapshotServer ,\n       portName :  queryResult ,\n       query : {\n         operatorName :  SnapshotServer.query ,\n         topic :  TwitterHashtagQueryDemo ,\n         url :  pubsub \n      },\n       result : {\n         appendQIDToTopic : true,\n         operatorName :  QueryResult ,\n        \u201ctopic :  TwitterHashtagQueryResultDemo ,\n         url :  pubsub \n      }\n    }, ... ]\n  ...\n}  The  name  field contains the name of the Data Source, and it has the following form:  dataSourceOperatorName . resultPortName .  The  operatorName  field is the Data Source operator name.  The  portName  field is the result port name of the Data Source operator.  The  query  field describes the query mechanism, including the topic.   The  result  field describes the result mechanism, including the topic and whether the query ID will be appended to the topic. Setting  appendQIDToTopic  to true increases the granularity of topics so that callers can receive results only for their queries. If the cost of creating such a large number of topics is high, it can be set to false; callers then will need to filter the results suitably.", 
            "title": "App Data Source"
        }, 
        {
            "location": "/app_data_framework/#message-bus_1", 
            "text": "dtGateway has a WebSocket service that provides the pubsub mechanism required by the Message Bus. The URL is  ws://{dtGatewayAddress}/pubsub . You can publish data with a topic by sending a WebSocket message like this:  {\n  \u201ctype\u201d: \u201cpublish\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}  The  topic  field is the topic name, and the  data  field is the payload and can be any JSON object.  All client that have subscribed to the topic will receive the data. The format is:  {\n  \u201ctype\u201d: \u201cdata\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}  And to subscribe to the topic, the client must send this message to dtGateway using WebSocket:  {\n  \u201ctype\u201d: \u201csubscribe\u201d, \u201ctopic\u201d: \u201c{topic}\u201d\n}  You can try it with dtGateway using a web browser with the help of a WebSocket client (e.g. \u201cSimple WebSocket Client\u201d extension in Google Chrome).  Note that dtGateway does not keep the history of the messages.", 
            "title": "Message Bus"
        }, 
        {
            "location": "/app_data_framework/#interaction-between-data-source-and-web-browser", 
            "text": "The web browser, the Query Operator and the Result Operator all connect to dtGateway via the Message Bus. The following describes the step-by-step interaction among them on a high level:   The UI widget in the browser subscribes to the result topic  The UI widget sends a schema query to the query topic.  The Data Source Operator gets the schema query and sends the schema result to the result topic, which the UI widget receives.  The UI widget in the browser subscribes to the result topic  The UI widget sends a data query  The Data Source Operator gets the data query and processes it and sends back the data result to the result topic.  The UI widget receives the data result and renders it.   Let\u2019s look at the actual messages being exchanged for the Twitter example.", 
            "title": "Interaction between Data Source and Web Browser"
        }, 
        {
            "location": "/app_data_framework/#snapshot-schema", 
            "text": "The Snapshot Schema serves a simple snapshot of tabular data, which is typically constantly being updated and is used in the Twitter example. We will describe the Snapshot schema as we look at the messages.  Step 1: Browser sends Schema Response Subscribe  {\n   type :  subscribe ,\n   topic :  TwitterHashtagQueryResultDemo.0.20716154835833223 \n}  In preparation for a Schema Query, the browser first subscribes to the result topic (with the query ID appended) to get the Schema Result back. In this case, the browser generates a random string, \u201c0.20716154835833223\u201d for the query ID, and appends the query ID to the result topic that it is subscribing to.   Step 2: Browser sends Schema Query  {\n   type :  publish ,\n   topic :  TwitterHashtagQueryDemo ,\n   data : {\n     id : 0.20716154835833223,\n     type :  schemaQuery \n  }\n}  The browser sends a message with type,  schemaQuery . This is the query that asks for the schema type and schema data.   Step 3: Data Source sends Schema Response  {\n   topic :  TwitterHashtagQueryResultDemo.0.20716154835833223 ,\n   data : {\n     id :  0.20716154835833223 ,\n     type :  schemaResult ,\n     data : [\n      {\n         values : [\n          {\n             name :  hashtag ,\n             type :  string \n          },\n          {\n             name :  count ,\n             type :  integer \n          }\n        ],\n         schemaType :  snapshot ,\n         schemaVersion :  1.0 \n      }\n    ]\n  },\n   type :  data \n}  The Data Source sends back the  schemaResult , which contains the response to  schemaQuery , which contains schema type and schema data. In this case, schemaType is snapshot and schemaVersion is 1.0, and the available fields in the data are \u201chashtag\u201d, which is a string, and \u201ccount\u201d, which is an integer.  Step 4: Browser sends Data Result Subscribe  {\n   type :  subscribe ,\n   topic :  TwitterHashtagQueryResultDemo.0.6760250790172551 \n}  Similar to the previous subscribe message, the browser subscribes to the result topic to get the response for the  dataQuery  that the browser is going to issue.  Step 5: Browser sends Data Query Request  {\n   type :  publish ,\n   topic :  TwitterHashtagQueryDemo ,\n   data : {\n     id : 0.6760250790172551,\n     type :  dataQuery ,\n     data : {\n       fields : [\n         hashtag ,\n         count \n      ]\n    },\n     countdown : 30,\n     incompleteResultOK : true\n  }\n}  The browser sends the  dataQuery  message that asks for actual data.  The  countdown  field expects an optional integer value. It tells the Data Source that the Data Source should return results once for subsequent {countdown} application windows. In this example, the value is 30. That means the Data Source should execute this query 30 times, once for each application window for the next 30 application windows.  The  incompleteResultOK  field is an optional boolean value default to be false. If the value is true, the Data Source should return results as soon as they are available even if they are partial results. This is useful when {countdown} is greater than 1 and the Data Source could take a long time to return the complete result set and if it is desirable for the caller to receive the results as soon as possible. If this value is false, Data Source should return the complete result set to the caller  Within the  data  field, the  fields  field tells the Data Source what fields to return in the result.  Step 6: Data Source sends Data Response  {\n   topic :  TwitterHashtagQueryResultDemo.0.6760250790172551 ,\n   data : {\n     id :  0.6760250790172551 ,\n     type :  dataResult ,\n     data : [\n      {\n         count :  1398 ,\n         hashtag :  \uc0ac\uc124\ud1a0\ud1a0\ucd94\ucc9c\uc0ac\uc774\ud2b8 \n      },\n      {\n         count :  1415 ,\n         hashtag :  TopDance \n      },\n      {\n         count :  1498 ,\n         hashtag :  isola \n      },\n      {\n         count :  1521 ,\n         hashtag :  RT\u3057\u305f\u4eba\u5168\u54e1\u30d5\u30a9\u30ed\u30fc\u3059\u308b \n      },\n      {\n         count :  1728 ,\n         hashtag :  4\u670819\u65e5\u306f\u897f\u6728\u91ce\u771f\u59eb\u306e\u8a95\u751f\u65e5 \n      },\n      {\n         count :  1787 ,\n         hashtag :  \ub124\uc784\ub4dc\uc0ac\ub2e4\ub9ac \n      },\n      {\n         count :  2079 ,\n         hashtag :  ALDUBActing101 \n      },\n      {\n         count :  2280 ,\n         hashtag :  \u5730\u9707 \n      },\n      {\n         count :  2712 ,\n         hashtag :  \u897f\u6728\u91ce\u771f\u59eb\u751f\u8a95\u796d2016 \n      },\n      {\n         count :  2714 ,\n         hashtag :  \u0634\u0639\u064a\u0628_\u064a\u0633\u064a\u0621_\u0644\u0644\u0633\u0639\u0648\u062f\u064a\u0647 \n      }\n    ],\n     countdown :  29 \n  },\n   type :  data \n}  The Data Source sends the  dataResult  message in response to the  dataQuery . In this case, it contains a list of (hashtag, count) sets in the  data  field.  If the  countdown  field of the  dataQuery  is non-zero, there will be multiple such messages that correspond to one  dataQuery  message.  The  countdown  field in the  dataResult  message decrements every time it sends a result.  When it counts down to zero, the query has expired and the client must send another  dataQuery  message to receive further updates.  In this case, because the countdown value is 30 in the  dataQuery , the Data Source will keep sending the latest data set for the next 29 application windows following this initial response.  For more information about the operators behind this Snapshot Schema, please refer to  this document .", 
            "title": "Snapshot Schema"
        }, 
        {
            "location": "/app_data_framework/#dimensions-schema", 
            "text": "The Dimensions Schema is an extension of the Snapshot Schema with the notion of time and key dimensions. It is supported in DataTorrent RTS.   We will look at the message exchange in the Sales example.  Step 1: Browser sends Schema Response Subscribe  { type : subscribe , topic : SalesQueryResultDemo.0.676153457723558 }  This is similar to the Snapshot example we saw earlier.  Step 2: Browser sends Schema Query  {\n    type : publish ,\n    topic : SalesQueryDemo ,\n    data :{\n       id : 0.22710832906886935,\n       type : schemaQuery \n   }\n}  Again, this is similar to the Snapshot example we saw earlier.  Step 3: Data Source sends Schema Response.  {\n    topic : SalesQueryResultDemo.0.22710832906886935 ,\n    data :{\n       id : 0.22710832906886935 ,\n       type : schemaResult ,\n       data :[\n         {\n             schemaType : dimensions ,\n             schemaVersion : 1.0 ,\n             time :{\n                buckets :[ 1m , 1h , 1d , 5m ],\n                slidingAggregateSupported :true\n            },\n             keys :[\n               {\n                   name : channel ,\n                   type : string ,\n                   enumValues :[ Mobile ,  Online ,  Store ]\n               },\n               {\n                   name : region ,\n                   type : string ,\n                   enumValues :[ Atlanta ,  Boston , Chicago , Cleveland , Dallas ,\n                      Minneapolis , New York , Philadelphia , San Francisco , St. Louis ]\n               },\n               {\n                   name : product ,\n                   type : string ,\n                   enumValues :[ Laptops , Printers , Routers , Smart Phones , Tablets ]\n               }\n            ],\n             values :[\n               {\n                   name : tax:SUM ,\n                   type : double \n               },\n               {\n                   name : sales:SUM ,\n                   type : double \n               },\n               {\n                   name : discount:SUM ,\n                   type : double \n               }\n            ],\n             dimensions :[\n               {\n                   combination :[\n\n                  ]\n               },\n               {\n                   combination :[\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      region \n                  ]\n               },\n               {\n                   combination :[\n                      product \n                  ]\n               },\n               {\n                   combination :[\n                      region ,\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      region \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      region ,\n                      channel \n                  ]\n               }\n            ]\n         }\n      ]\n   },\n    type : data \n}  Compared to the Snapshot Schema, this is considerably more complex. The response to the query describes the keys and aggregates that are provided by the data source. The basic components of the schema are the following:  timebuckets : These are the time buckets over which aggregations are computed. For example, 1d (one day), 1h (one hour), 1m (one minute), 1s (one second).  keys : These are the search parameters you need to provide when you do a query. The keys have an enumValues. This is an optional section which contains all possible values of a key. This section may be updated by the back end if new key values are encountered.  values : These are the results which are returned by a query. Notice that the values are of the form (name):(aggregation). The first portion of the value name describes what is being aggregated. The second portion of the value name describes the aggregation being used.  combinations : This describes the valid combinations of keys that can be specified when doing a query.  Step 4: Browser sends Data Result Subscribe  {\n   type :  subscribe ,\n   topic :  SalesQueryResultDemo.0.3180227109696716 \n}  Step 5: Browser sends Data Query Request  Now that the valid combinations, keys, values, and timebuckets are known, we can issue a query using them.   {\n    type : publish ,\n    topic : SalesQueryDemo ,\n    data :{\n       id :  0.3180227109696716 ,\n       type : dataQuery ,\n       data :{\n          time :{\n             latestNumBuckets :10,\n             bucket : 1m \n         },\n          incompleteResultOK :true,\n          keys :{\n\n         },\n          fields :[\n             time ,\n             channel ,\n             region ,\n             product ,\n             tax:SUM ,\n             sales:SUM ,\n             discount:SUM \n         ]\n      },\n       countdown :29,\n       incompleteResultOK :true\n   }\n}  time : This specifies which time buckets to query. In this case the most recent 10 one minute time buckets are queried. The example above illustrates how to query the last N timebuckets, but it is also possible to do history queries by specifying a time section like the following:           time :{\n          bucket : 1m ,\n          from :1460765563547,\n          to :1460767363547\n       }  This time section specifies a historical query starting at the \u201cfrom\u201d unix timestamp (inclusive) up until the \u201cto\u201d timestamp (inclusive).  keys : These are the search parameters. This section can be empty to query global aggregations. If there are specific key combinations you want to search for you can do this:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: \u201cAtlanta\u201d\n}  It is possible to specify a key with a list like the following:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: [ \u201cAtlanta\u201d, \u201cDallas\u201d ]\n}  This will query all the data with a  channel  value of \u201cMobile\u201d and a  region  value of \u201cAtlanta\u201d OR \u201cDallas\u201d. It is also possible to specify a key with an empty array like the following:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: []\n}  This will query all the data with a  channel  value of \u201cMobile\u201d and ANY value of  region .\nNote that this is different from not specifying the  region  value at all, in which case, the aggregated value of all regions will be returned. This is similar to the difference between the SQL queries of  SELECT SUM(sales) WHERE channel=\u2019Mobile\u2019  (for the case of not specifying  region ) and  SELECT SUM(sales), region WHERE channel=\u2019Mobile\u2019 GROUP BY region  (for the case of  region  being an empty array).  fields : This specifies what data to include in the query result. If the field  time  is included, then a timestamp is included for each data point. Similarly the keys can also be specified as fields with the results. Lastly aggregated values can also be specified as fields to be returned in results.  Step 6: Data Source sends Data Response.  After a query is issued, results are periodically returned:  {\n    topic : SalesQueryResultDemo.0.3180227109696716 ,\n    data : {\n       id : 0.3180227109696716 ,\n       type : dataResult ,\n       data :[\n         {\n             time : 1460771580000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 78363.47 ,\n             sales:SUM : 922988.9199999997 ,\n             discount:SUM : 69135.71999999999 \n         },\n         {\n             time : 1460771640000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 142011.85 ,\n             sales:SUM : 1672677.0099999998 ,\n             discount:SUM : 125287.46999999999 \n         },\n         {\n             time : 1460771700000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 139069.53999999998 ,\n             sales:SUM : 1638011.5399999998 ,\n             discount:SUM : 122692.48999999999 \n         },\n         {\n             time : 1460771760000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 80699.90000000001 ,\n             sales:SUM : 950502.3500000003 ,\n             discount:SUM : 71196.95000000001 \n         },\n         {\n             time : 1460771820000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 35914.69 ,\n             sales:SUM : 423017.9000000001 ,\n             discount:SUM : 69339.05000000002 \n         },\n         {\n             time : 1460771880000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 106977.84999999998 ,\n             sales:SUM : 1260017.5600000003 ,\n             discount:SUM : 629939.14 \n         },\n         {\n             time : 1460771940000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 253243.50000000003 ,\n             sales:SUM : 2982796.9300000006 ,\n             discount:SUM : 1491234.3599999999 \n         },\n         {\n             time : 1460772000000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 161384.77999999997 ,\n             sales:SUM : 1900857.2300000002 ,\n             discount:SUM : 950325.01 \n         },\n         {\n             time : 1460772060000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 125283.99000000002 ,\n             sales:SUM : 1475625.3600000003 ,\n             discount:SUM : 737730.77 \n         },\n         {\n             time : 1460772120000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 58684.59 ,\n             sales:SUM : 691200.9299999999 ,\n             discount:SUM : 345562.2700000001 \n         }\n      ],\n       countdown : 297 \n   },\n    type : data \n}  As you can see each result represents a time bucket with the corresponding values for the fields. If the ANY value (with an empty list) query is specified for a key you will see multiple results for the same time bucket, but each result will have different values for the keys.  Detailed documentation of the dimensions operators can be found here:  http://docs.datatorrent.com/operators/dimensions_computation/", 
            "title": "Dimensions Schema"
        }, 
        {
            "location": "/app_data_framework/#advanced-topics", 
            "text": "", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/app_data_framework/#schema-keys", 
            "text": "Although in most cases, a Data Source has one fixed schema, it is sometimes necessary for a Data Source to support multiple schemas. This is useful if the schemas are dynamically created depending on the incoming data to the Data Source Operator.  Schema Key is the feature we added to the App Data Framework for this purpose. If the Data Source has multiple schemas, the  schemaResult  will contain a list of schemas with the corresponding schema keys in the  context  field:  {\n  \u201cid\u201d: \u201c{client_generated_id}\u201d,\n  \u201ctype\u201d: \u201cschemaResult\u201d,\n  \u201cdata\u201d: [ \n    {\n      \u201ccontext\u201d: {\n        \u201cschemaKeys\u201d: {\n          \u201c{key1}\u201d: \u201c{value1}\u201d,\n          \u201c{key2}\u201d: \u201c{value2}\u201d, ...\n         }\n      },\n      {rest of the schema}\n    }, ...\n  ]\n}  And if the  schemaResult  returns multiple schemas, all  dataQuery  messages must include the schema keys:  {\n   \u201cid\u201d: \u201c{client_generated_id}\u201d,\n   \u201ctype\u201d: \u201cdataQuery\u201d, \n   \u201cdata\u201d: {\n     \u201ccontext\u201d: {\n       \u201cschemaKeys\u201d { //optional\n    \u201c{keyName1}\u201d: \u201c{keyValue1}\u201d,\n    \u201c{keyName2}\u201d: \u201c{keyValue2}\u201d \n       }\n     }\n     {rest of the query}\n   }\n   \u201ccountdown\u201d: \u201c{countdown}\u201d,\n   \u201cincompleteResultOK\u201d: true/false\n}  This feature is supported by the DimensionsStore operator and is being used by the App Data Tracker.", 
            "title": "Schema Keys"
        }, 
        {
            "location": "/app_data_framework/#dimensions-schema-additional-values-for-combinations", 
            "text": "For the Dimensions Schema supported by the DimensionsStore operator, there can be more complex schema specifications which have results for values only for certain combinations. This is the schema for yet another example about aggregation of advertisement data in the DataTorrent RTS example repository.  {\n    topic : AdsQueryGenericResultDemo.0.08170037856325507 ,\n    data :{\n       id : 0.08170037856325507 ,\n       type : schemaResult ,\n       data :[\n         {\n             schemaType : dimensions ,\n             schemaVersion : 1.0 ,\n             time :{\n                buckets :[ 1m , 1h , 1d ],\n                slidingAggregateSupported :true,\n                from : 1459987200000 ,\n                to : 1460768100000 \n            },\n             keys :[\n               {\n                   name : publisher ,\n                   type : string ,\n                   enumValues :[ twitter , facebook , yahoo , google , bing , amazon \n                  ]\n               },\n               {\n                   name : advertiser ,\n                   type : string ,\n                   enumValues :[ starbucks , safeway , mcdonalds , macys , taco bell ,\n                      walmart , kohls , san diego zoo , pandas , jack in the box ,\n                      tomatina , ron swanson \n                  ]\n               },\n               {\n                   name : location ,\n                   type : string ,\n                   enumValues :[\n                      N , LREC , SKY , AL , AK , AZ , AR , CA , CO , CT , DE , FL ,\n                      GA , HI , ID \n                  ]\n               }\n            ],\n             values :[\n               {\n                   name : impressions:COUNT ,\n                   type : long \n               },\n               {\n                   name : impressions:SUM ,\n                   type : long \n               },\n               {\n                   name : impressions:AVG ,\n                   type : double \n               },\n               {\n                   name : clicks:COUNT ,\n                   type : long \n               },\n               {\n                   name : clicks:SUM ,\n                   type : long \n               },\n               {\n                   name : clicks:AVG ,\n                   type : double \n               },\n               {\n                   name : cost:COUNT ,\n                   type : long \n               },\n               {\n                   name : cost:SUM ,\n                   type : double \n               },\n               {\n                   name : cost:AVG ,\n                   type : double \n               },\n               {\n                   name : revenue:COUNT ,\n                   type : long \n               },\n               {\n                   name : revenue:SUM ,\n                   type : double \n               },\n               {\n                   name : revenue:AVG ,\n                   type : double \n               }\n            ],\n             dimensions :[\n               {\n                   combination :[\n\n                  ]\n               },\n               {\n                   combination :[\n                      location \n                  ]\n               },\n               {\n                   combination :[\n                      advertiser \n                  ],\n                   additionalValues :[\n                     {\n                         name : impressions:MAX ,\n                         type : long \n                     },\n                     {\n                         name : impressions:MIN ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MAX ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MIN ,\n                         type : long \n                     },\n                     {\n                         name : cost:MAX ,\n                         type : double \n                     },\n                     {\n                         name : cost:MIN ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MAX ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MIN ,\n                         type : double \n                     }\n                  ]\n               },\n               {\n                   combination :[\n                      publisher \n                  ],\n                   additionalValues :[\n                     {\n                         name : impressions:MAX ,\n                         type : long \n                     },\n                     {\n                         name : impressions:MIN ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MAX ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MIN ,\n                         type : long \n                     },\n                     {\n                         name : cost:MAX ,\n                         type : double \n                     },\n                     {\n                         name : cost:MIN ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MAX ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MIN ,\n                         type : double \n                     }\n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      advertiser \n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      publisher \n                  ]\n               },\n               {\n                   combination :[\n                      advertiser ,\n                      publisher \n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      advertiser ,\n                      publisher \n                  ]\n               }\n            ]\n         }\n      ]\n   },\n    type : data \n}  Here you can see that a combination can also have  additionalValues  specified. Additional values are values which are only available for a specific combination of keys.  The code for the ads example is available  here .", 
            "title": "Dimensions Schema: Additional Values for Combinations"
        }, 
        {
            "location": "/app_data_framework/#ui-widgets_1", 
            "text": "Coming soon", 
            "title": "UI Widgets"
        }, 
        {
            "location": "/app_data_framework/#app-data-tracker", 
            "text": "App Data Tracker is discussed  here .", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/dtgateway_api/", 
            "text": "DataTorrent dtGateway API v2 Specification\n\n\nREST API\n\n\nReturn codes\n\n\n\n\n200\n: OK\n\n\n400\n: The request is not in the format that the server expects\n\n\n404\n: The resource is not found\n\n\n500\n: Something is wrong on the server side\n\n\n\n\nREST URI Specification\n\n\nGET /ws/v2/about\n\n\nFunction:\n\n\nReturn:\n\n\n{\n    \nbuildVersion\n: \n{Apex build version}\n,\n    \nbuildDate\n: \n{Apex build date and time}\n,\n    \nbuildRevision\n: \n{Apex revision}\n,\n    \nbuildUser\n: \n{Apex build user}\n,\n    \nversion\n: \n{Apex version}\n,\n    \nrtsBuildVersion\n: \n{RTS build version}\n,\n    \nrtsBuildDate\n: \n{RTS build date and time}\n,\n    \nrtsBuildRevision\n: \n{RTS revision}\n,\n    \nrtsBuildUser\n: \n{RTS build user}\n,\n    \nrtsVersion\n: \n{RTS version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \njavaVersion\n: \n{java_version}\n,\n    \nhadoopLocation\n: \n{hadoop_location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhostname\n: \n{hostname}\n,\n    \nhadoopIsSecurityEnabled\n: \n{true/false}\n\n}\n\n\n\n\nGET /ws/v2/cluster/metrics\n\n\nFunction: List metrics that are relevant to the entire cluster\n\n\nReturn:\n\n\n{\n    \naverageAge\n: \n{average running application age in milliseconds}\n,\n    \ncpuPercentage\n: \n{cpuPercentage}\n,\n    \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n    \nmaxMemoryAllocatedMB\n: \n{maxMemoryAllocatedMB}\n,\n    \nnumAppsFailed\n: \n{numAppsFailed}\n,\n    \nnumAppsFinished\n: \n{numAppsFinished}\n,\n    \nnumAppsKilled\n: \n{numAppsKilled}\n,\n    \nnumAppsPending\n: \n{numAppsPending}\n,\n    \nnumAppsRunning\n: \n{numAppsRunning}\n,\n    \nnumAppsSubmitted\n: \n{numAppsSubmitted}\n,\n    \nnumContainers\n: \n{numContainers}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications[?states={STATE_FILTER}\nname={NAME_FILTER}\nuser={USER_FILTER]\n\n\nFunction: List IDs of all streaming applications\n\n\nReturn:\n\n\n{\n    \napps\n: [\n        {\n            \ndiagnostics\n: \n{diagnostics}\n,\n            \nelapsedTime\n: \n{elapsedTime}\n,\n            \nfinalStatus\n: \n{finalStatus}\n,\n            \nfinishedTime\n: \n{finishedTime}\n,\n            \nid\n: \n{appId}\n,\n            \nname\n: \n{name}\n,\n            \nqueue\n: \n{queue}\n,\n            \nstartedTime\n: \n{startedTime}\n,\n            \nstate\n: \n{state}\n,\n            \ntrackingUrl\n: \n{trackingUrl}\n,\n            \nuser\n: \n{user}\n\n        },  \n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: Get the information for the specified application\n\n\nReturn:\n\n\n{\n    \nid\n: \n{appid}\n,\n    \nname\n: \n{name}\n,\n    \nstate\n: \n{state}\n,\n    \ntrackingUrl\n: \n{tracking url}\n,\n    \nfinalStatus\n: {finalStatus},\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nstartedTime\n: \n{startTime}\n,\n    \nuser\n: \n{user}\n,\n    \nversion\n: \n{stram version}\n,\n    \nremainingLicensedMB\n: \n{remainingLicensedMB}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nattributes\n: {\n           \n{attributeName}\n: \n{attributeValue}\n, \n           \n{attributeName-n}\n: \n{attributeValue-n}\n, \n    },\n    \nstats\n: {\n        \nallocatedContainers\n: \n{allocatedContainer}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nlatency\n: \n{overall latency}\n,\n        \ncriticalPath\n: \n{list of operator id that represents the critical path}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n:\n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n:\n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n:\n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan\n\n\nFunction: Return the physical plan for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nisUnifier\n: true/false\n        },\n         \u2026\n     ],\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators\n\n\nFunction: Return list of operators for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncounters\n: {\n                \n{counterName}\n: \n{counterValue}\n, \n                ...\n             },\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nunifierClass\n: \n{unifierClass}\n\n        },\n         \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/streams\n\n\nFunction: Return physical streams\n\n\nReturn:\n\n\n{\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}\n\n\nFunction: Return information of the given operator for the given application\n\n\nReturn:\n\n\n{\n    \nclassName\n: \n{className}\n,\n    \ncontainer\n: \n{containerId}\n,\n    \ncounters\n: {\n      \n{counterName}: \n{counterValue}\n, ...            \n    }\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \ncurrentWindowId\n: \n{currentWindowId}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \nports\n: [\n       {\n          \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n          \nname\n: \n{name}\n,\n          \ntotalTuples\n: \n{totalTuples}\n,\n          \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n          \ntype\n: \ninput/output\n\n       }, ...\n    ],\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,\n    \nname\n: \n{name}\n,\n    \nrecordingStartTime\n: \n{recordingStartTime}\n,\n    \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n    \nstatus\n: \n{status}\n,\n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory\n\n\nFunction: Return container deploy history of this operator\nSince: 1.0.6\n\n\nReturn:\n\n\n{\n   \ncontainers\n: [  \n        {  \n            \ncontainer\n: \n{containerId}\n,   \n            \nstartTime\n: \n{startTime}\n  \n        }, ...  \n    ],   \n    \nname\n: \n{operatorName}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports\n\n\nFunction: Get the information of all ports of the given operator of the\ngiven application\n\n\nReturn:\n\n\n{  \n    \nports\n: [\n        {  \n            \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,  \n            \ntotalTuples\n: \n{totalTuples}\n,   \n            \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n            \ntype\n: \noutput\n  \n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}\n\n\nFunction: Get the information of a specified port\n\n\nReturn:\n\n\n{  \n    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n    \nname\n: \n{name}\n,   \n    \ntotalTuples\n: \n{totalTuples}\n,   \n    \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n    \ntype\n: \n{type}\n  \n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}\nq={searchTerm}\npackagePrefixes={comma-separated-package-prefixes}]\n\n\nFunction: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        { \nname\n:\n{className}\n },\n       \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}\n\n\nFunction: Get the description of the given operator class\n\n\nReturn:\n\n\n{\n    \ninputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n          ...\n    ],\n    \noutputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n        \u2026\n    ],\n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/shutdown\n\n\nFunction: Shut down the application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/kill\n\n\nFunction: Kill the given application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start\n\n\nFunction: Start recording on operator\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop\n\n\nFunction: Stop recording on operator\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start\n\n\nFunction: Start recording on port\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop\n\n\nFunction: Stop recording on port\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]\n\n\nFunction: Return the list of containers for this application\n\n\nReturn:\n\n\n{\n    \ncontainers\n: [\n        {\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \nnumOperators\n: \n{numOperators}\n,\n            \noperators:\n {\n                \nid1\n: \nname1\n,\n                \nid2\n: \nname2\n,\n                \nid3\n: \nname3\n\n            },\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstate\n: \n{state}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction: Return the information of the specified container\n\n\nReturn:\n\n\n{\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \noperators:\n {\n        \nid1\n: \nname1\n,\n        \nid2\n: \nname2\n,\n        \nid3\n: \nname3\n\n    },\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstate\n: \n{state}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction: Return the container log list\n\n\nReturn:\n\n\n{\n    \nlogs\n: [\n        {\n            \nlength\n: \n{log length}\n,\n            \nname\n: \n{logName}\n\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/stackTrace\n\n\nSince: 3.4.0 \n\n\nFunction: Return the container stack trace\n\n\nReturn:\n\n\n{\n    \nthreads\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nstate\n: \n{state}\n,\n            \nid\n: \n{id}\n,\n            \nstackTraceElements\n: [\n                \n{line1}\n,\n                \n{line2}\n, ...\n            ]\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}]\n\n\nFunction: Return the raw log\n\n\nReturn: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\n{\n    \nlines\n: [\n        { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \n{line}\n }, \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill\n\n\nFunction: Kill this container\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Return the logical plan of this application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n      {\n        \nname\n: \n{name}\n,\n        \nattributes\n: {attributeMap},\n        \nclass\n: \n{class}\n,\n        \nports\n: {\n           [\n            {\n                \nname\n: \n{name}\n,\n                \nattributes\n: {attributeMap},\n                \ntype\n: \ninput/output\n\n            }, ...\n           ]\n         },\n         \nproperties\n: {\n            \nclass\n: \n{class}\n\n         }\n      }, ...\n    ],\n    \nstreams\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/attributes\n\n\nFunction: Return the application attributes\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators\n\n\nFunction: Return the list of info of the logical operator\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n\n\nFunction: Return the info of the logical operator\n\n\nReturn:\n\n\n{\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Return the properties of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Set the properties of the logical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Return the properties of the physical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Set the properties of the physical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes\n\n\nFunction: Get the attributes of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes\n\n\nFunction:  Get the attributes of the port\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Change logical plan of this application\nPayload:\n\n\n{\n    \nrequests\n: [\n        {\n            \nrequestType\n: \nAddStreamSinkRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nCreateOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \noperatorFQCN\n: \n{operatorFQCN}\n,\n        },\n        {\n            \nrequestType\n: \nCreateStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsourceOperatorName\n: \n{sourceOperatorName}\n,\n            \nsourceOperatorPortName\n: \n{sourceOperatorPortName}\n\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nRemoveOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n        },\n        {\n            \nrequestType\n: \nRemoveStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n        },\n        {\n            \nrequestType\n: \nSetOperatorPropertyRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \npropertyName\n: \n{propertyName}\n,\n            \npropertyValue\n: \n{propertyValue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta\n\n\nFunction: Return the meta information about the statistics stored for\nthis operator\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \noperatorName\n: \n{operatorName}\n,\n    \noperatorIds\n: [ {opid}, \u2026 ],\n    \nstartTime\n: \n{startTime}\n,\n    \nendTime\n: \n{endTime}\n,\n    \ncount\n: \n{count}\n,\n    \nended\n: \n{boolean}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the statistics stored for this logical operator\n\n\n{\n    \noperatorStats\n: [\n        {\n            \noperatorId\n: \n{operatorId}\n,\n            \ntimestamp\n: \n{timestamp}\n,\n            \nstats\n: {\n                \ncontainer\n: \ncontainerId\n,\n                \nhost\n: \nhost\n,\n                \ntotalTuplesProcessed\n, \n{totalTuplesProcessed}\n,\n                \ntotalTuplesEmitted\n, \n{totalTuplesEmitted}\n,\n                \ntuplesProcessedPSMA\n, \n{tuplesProcessedPSMA}\n,\n                \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n                \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n                \nlatencyMA\n: \n{latencyMA}\n,\n                \nports\n: [ {\n                    \nname\n: \n{name}\n,\n                    \ntype\n:\n{input/output}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n, \n{tuplesPSMA}\n,\n                    \nbufferServerBytesPSMA\n, \n{bufferServerBytesPSMA}\n\n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta\n\n\nFunction: Return the meta information about the container statistics\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \ncontainers\n: {\n        \n{containerId}\n: {\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nhost\n: \n{host}\n,\n            \nmemoryMBAllocated\n, \n{memoryMBAllocated}\n\n        },\n        \u2026\n    },\n    \nstartTime\n: \n{startTime}\n\n    \nendTime\n: \n{endTime}\n\n    \ncount\n: \n{count}\n\n    \nended\n: {boolean}\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the container statistics stored for this application\n\n\n{\n    \ncontainerStats\n: [\n        {\n            \ncontainerId\n: \n{containerId}\n\n            \ntimestamp\n: \n{timestamp}\n\n            \nstats\n: {\n                \nnumOperators\n: \n{numOperators}\n,\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/recordings\n\n\nFunction: Get the list of all recordings for this application\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [{\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings\n\n\nFunction: Get the list of recordings on this operator\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [ {\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Get the information about the recording\n\n\nReturn:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nstartTime\n: \n{startTime}\n,\n    \nappId\n: \n{appId}\n,\n    \noperatorId\n: \n{operatorId}\n,\n    \ncontainerId\n: \n{containerId}\n,\n    \ntotalTuples\n: \n{totalTuples}\n,\n    \nports\n: [ {\n       \nname\n: \n{portName}\n,\n       \nstreamName\n: \n{streamName}\n,\n       \ntype\n: \n{type}\n,\n       \nid\n: \n{index}\n,\n       \ntupleCount\n: \n{tupleCount}\n\n     } \u2026 ],\n    \nended\n: {boolean},\n    \nwindowIdRanges\n: [ {\n       \nlow\n: \n{lowId}\n,\n       \nhigh\n: \n{highId}\n\n     } \u2026 ],\n    \nproperties\n: {\n       \nname\n: \nvalue\n, ...\n     }\n}\n\n\n\n\nDELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Deletes the specified recording\n\n\nSince: 1.0.4\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples\n\n\nQuery Parameters:\n\n\noffset\nstartWindow\nlimit\nports\nexecuteEmptyWindow\n\n\n\nFunction: Get the tuples\n\n\nReturn:\n\n\n{\n    \nstartOffset\n: \n{startOffset}\n,\n    \ntuples\n: [ {\n        \nwindowId\n: \n{windowId}\n,\n        \ntuples\n: [ {\n            \nportId\n: \n{portId}\n,\n            \ndata\n: \n{tupleData}\n\n        }, \u2026 ]\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/events?from={fromTime}\nto={toTime}\noffset={offset}\nlimit={limit}\n\n\nFunction: Get the events\n\n\nReturn:\n\n\n{\n    \nevents\n: [ {\n           \nid\n: \n{id}\n,\n        \ntimestamp\n: \n{timestamp}\n,\n        \ntype\n: \n{type}\n,\n        \ndata\n: {\n            \nname\n: \nvalue\n, \u2026\n        }\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/user\n\n\nFunction: Get the user profile information, list of roles and list of\npermissions given the user\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/settings\n\n\nFunction: Get the current user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}\n\n\nFunction: Get the specified user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Get the specified user's setting key\n\n\nReturn:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nPUT /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Set the specified user's setting key\nPayload:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nGET /ws/v2/auth/roles\n\n\nFunction: Get the list of roles the system has\n\n\nReturn:\n\n\n{\n    \nroles\n: [\n       {\n         \nname\n: \n{role1}\n,\n         \npermissions\n: [ \n{permission1}\n, \u2026 ]\n       }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/auth/roles/{role}\n\n\nFunction: Get the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/auth/roles/{role}\n\n\nFunction: create or edit the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/auth/restoreDefaultRoles\n\n\nFunction: Restores default roles\n\n\nDELETE /ws/v2/auth/roles/{role}\n\n\nFunction: delete the given role\n\n\nGET /ws/v2/auth/permissions\n\n\nFunction: Get the list of possible permissions\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ {\n       \nname\n: \n{permissionName}\n,\n       \nadminOnly\n: true/false\n    }, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/applications/{appid}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPOST /ws/v2/licenses\n\n\nFunction: Add a license to the registry\n\n\nPayload: The license file content\n\n\nReturn:\n\n\n{\n  \nid\n: \n{licenseId}\n,\n  \nexpireTime\n: {unixTimeMillis},\n  \nexpirationTimeNotificationPeriod\n: {timeMillis},\n  \nnodesAllowed\n: {nodesAllowed},\n  \nmemoryMBAllowed\n: {memoryMBAllowed},\n  \nexceedGracePeriod\n: {timeMillis},\n  \ncontextType\n: \n{contextType}\n,\n  \ntype\n: \n{type}\n,\n  \nfeatures\n: [ \n{feature1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/licenses/current\n\n\nFunction: Get info on the current license\n\n\n{\n      \nid\n: \n{licenseId}\n,\n      \ncurrentTime\n: {unixTimeMillis},\n      \nexpireTime\n: {unixTimeMillis},\n      \nnodesAllowed\n: {nodesAllowed},\n      \nnodesUsed\n: {nodesUsed},\n      \nmemoryMBAllowed\n: {memoryMBAllowed},\n      \nmemoryMBUsed\n: {memoryMBUsed},\n      \nexceedGracePeriod\n: {timeMillis}, // memory exceed grace period\n      \nexceedRemainingTime\n: {timeMillis},  // (optional)\n      \nviolation\n: \nmemory\n, // returns violation type (optional)\n      \ncontextType\n: \n{community|standard|enterprise}\n,\n      \ntype\n: \n{evaluation|non_production|production}\n\n      \nfeatures\n: [ \n{feature1}\n, \u2026 ], // for community, empty array\n      \ncurrent\n: true/false,\n      \nexpirationTimeNotificationLevel\n: \n{INFO|WARN|ERROR}\n, // (optional)\n      \nvalid\n: true/false // true, if the license is valid\n}\n\n\n\n\nGET /ws/v2/config/installMode\n\n\nFunction: returns the install mode\n\n\n{\n  \ninstallMode\n: \n{evaluation|community|app}\n,\n  \nappPackageName\n: \n{optionalAppPackageName}\n,\n  \nappPackageVersion\n: \n{optionalAppPackageVersion}\n\n}\n\n\n\n\nGET /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction: returns the download type\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nPUT /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction:\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nFeature List:  \n\n\n\n\nSYSTEM_APPS\n\n\nSYSTEM_ALERTS\n\n\nAPP_DATA_DASHBOARDS\n\n\nRUNTIME_DAG_CHANGE\n\n\nRUNTIME_PROPERTY_CHANGE\n\n\nAPP_CONTAINER_LOGS\n\n\nLOGGING_LEVELS\n\n\nAPP_DATA_TRACKER\n\n\nJAAS_LDAP_AUTH\n\n\nAPP_BUILDER\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns list of properties from dt-site.xml.\n\n\nReturn:\n\n\n{\n    \n{name}\n: {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n    }\n\n}\n\n\n\n\nGET /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Returns single property from dt-site.xml, specify by name\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nPOST /ws/v2/config/properties\n\n\nFunction: Overwrites all specified properties in dt-site.xml\n\n\nPayload:\n\n\n{\n    \nproperties\n: [\n        {\n            \nname\n: \n{name}\n\n            \nvalue\n: \n{PROPERTY_VALUE}\n,\n            \nlocal\n: true/false,\n                    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nPUT /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Overwrites or creates new property in dt-site.xml\nPayload:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \nlocal\n: true/false,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nDELETE /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Deletes a property from dt-site.xml. \n\n\nGET /ws/v2/config/hadoopExecutable\n\n\nFunction: Returns the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nPUT /ws/v2/config/hadoopExecutable\n\n\nFunction: Sets the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nGET /ws/v2/config/issues\n\n\nFunction: Returns list of potential issues with environment\n\n\nReturn:\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        {...},\n        {...}\n    ]    \n}\n\n\n\n\nGET /ws/v2/config/ipAddresses\n\n\nFunction: Returns list of ip addresses the gateway can listen to\n\n\nReturn:\n\n\n{\n    \nipAddresses\n: [\n      \n1.2.3.4\n, ...\n    ]    \n}\n\n\n\n\nPOST /ws/v2/config/restart\n\n\nFunction: Restarts the gateway\n\n\nPayload: none\n\n\nGET /proxy/rm/v1/\u2026\n\n\nPOST /proxy/rm/v1/\u2026\n\n\nFunction: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.\n\n\nGET /proxy/stram/v2/...\n\n\nPOST /proxy/stram/v2/\u2026\n\n\nPUT /proxy/stram/v2/\u2026\n\n\nDELETE /proxy/stram/v2/\u2026\n\n\nFunction: Proxy calls to Stram Web Services.\n\n\nPOST /ws/v2/applications/{appid}/loggers\n\n\nFunction: Set the logger levels of packages/classes.\n\n\nPayload:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers\n\n\nFunction: Gets the logger levels of packages/classes.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"\n\n\nFunction: searches for all classes that match the pattern.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nname\n : \n{fully qualified class name}\n,\n            \nlevel\n: \n{logger level}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/restart[?queue={queue}]\n\n\nSince: 3.4.0\nFunction: Restart the terminated application. Payload is optional.\nPayload:\n\n\n{\n  \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n  \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of appPackages the user can view in the system\n\n\n{\n    \nappPackages\n: [\n        {\n                 \nappPackageName\n: \n{appPackageName}\n,\n                 \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nowner\n: \n{owner}\n,\n        }, ...\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages?merge={replace|fail|ours|theirs}\n\n\nSince: 1.0.4\n\n\nFunction: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package\n\n\nPayload: the raw zip file\n\n\nReturn: The information of the app package\n\n\nGET /ws/v2/appPackages/{owner}/{name}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of versions of appPackages with the given name in the system owned by the specified user\n\n\n{\n    \nversions\n: [\n        \n1.0-SNAPSHOT\n\n    ]\n}\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the appPackage\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download\n\n\nSince: 1.0.4\n\n\nFunction: Downloads the appPackage zip file\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns:\n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nmodificationTime\n:  \n{modificationTime}\n,\n    \nowner\n: \n{owner}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs\n\n\nSince: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:\n\n\n{\n    \nconfigs\n: [\n        \nmy-app-conf1.xml\n\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the properties XML of the specified config\n\n\nReturns:\n\n\nconfiguration\n\n        \nproperty\n\n                \nname\n...\n/name\n\n                \nvalue\n...\n/value\n\n        \n/property\n\n        \u2026\n\n/configuration\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Creates or replaces the specified config with the property parameters specified payload\n\n\nPayload: configuration in XML\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the specified config\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of applications in the appPackage\n\n\nReturns:\n\n\n{\n    \napplications\n: [\n        {\n            \ndag\n: {dag in json format},\n            \nfile\n: \n{fileName}\n,\n            \nname\n: \n{name}\n,\n            \ntype\n: \n{type}\n,\n            \nerror\n: \n{error}\n,\n            \nfileContent\n: {originalFileContentForJSONTypeApp}\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta data for that application\n\n\nReturns:\n\n\n{\n    \nfile\n: \n{fileName}\n,\n    \nname\n: \n{name}\n,\n    \ntype\n: \n{json/class/properties}\n,\n    \nerror\n: \n{error}\n\n    \ndag\n: {\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n         }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n    },\n    \nfileContent\n: {originalFileContentForJSONTypeApp}\n}\n\n\n\n\nPOST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge\n\n\nFunction: Merge the configuration, json apps, and resources files from the app package specified by user/name/version from the payload to the specified app package in the url, without overwriting any existing file in the specified app package. If replaceExisting is true, the files in the app, conf and resources directory of the app package will be replaced by the ones in the app package specified in the payload. Otherwise, they will not be replaced. The fields user, name and replaceExisting in the payload are optional. If user and name are not specified, they are default to be the same as in the URI path. replaceExisting's default is false.\n\n\nPayload:\n\n\n{\n \nuser\n: \n{user}\n,\n \nname\n: \n{name}\n,\n \nversion\n: \n{versionToMergeFrom}\n,\n \nreplaceExisting\n: \n{true/false}\n\n}\n\n\n\n\nPOST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}\noriginalAppId={originalAppId}\nqueue={queueName}]\n\n\nSince: 1.0.4\n\n\nFunction: Launches the application with the given configuration specified in the POST payload\n\n\nPayload:\n\n\n{\n    \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}\n\n\nSince: 1.0.4\n\n\nFunction: Get the properties of the operator given the classname in the jar\n\n\n{  \n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]\n\n\nFunction: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app package\n\n\nPayload:\n\n\n{\n        \ndisplayName\n: \n{displayName}\n,\n        \ndescription\n: \n{description}\n,\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n          }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n}\n\n\n\n\nReturn:\n\n\n{\n        \nerror\n: \n{error}\n\n}\n\n\n\n\nAvailable port attributes to set: \n\n\n\n\nAUTO_RECORD\n\n\nIS_OUTPUT_UNIFIED\n\n\nPARTITION_PARALLEL\n\n\nQUEUE_CAPACITY\n\n\nSPIN_MILLIS\n\n\nSTREAM_CODEC\n\n\nUNIFIER_LIMIT\n\n\n\n\nAvailable locality options to set: \n\n\n\n\nTHREAD_LOCAL\n\n\nCONTAINER_LOCAL\n\n\nNODE_LOCAL\n\n\nRACK_LOCAL\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}\n\n\nSince: 1.0.5\n\n\nFunction: Deletes non-jar based application in the app package\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators\n\n\nSince: 1.0.5\n\n\nFunction: Get the classes of operators from specified app package.\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        {\n            \nname\n:\n{fullyQualifiedClassName}\n, \n            \ntitle\n: \n{title}\n,\n            \nshortDesc\n: \n{description}\n,\n            \nlongDesc\n: \n{description}\n,\n            \ncategory\n: \n{categoryName}\n,\n            \ndoclink\n: \n{doc url}\n,\n            \ntags\n: [ \n{tag}\n, \n{tag}\n, \u2026 ],\n            \ninputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ]\n            \noutputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ],\n            \nproperties\n: [  \n                {\n                    \nname\n:\n{propertyName}\n,\n                    \ncanGet\n: {canGet},\n                    \ncanSet\n: {canSet},\n                    \ntype\n:\n{type}\n,\n                    \ndescription\n:\n{description}\n,\n                    \nproperties\n: ...\n                }, \u2026\n            ],\n            \ndefaultValue\n: {\n                \n{propertyName}\n: [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/import\n\n\nFunction: List the importable app packages on Gateway's local file\nsystem\n\n\nReturn:\n\n\n{\n    \nappPackages: [\n        {\n            \nfile\n: \n{file}\n,\n            \nname\n: \n{name}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \nversion\n: \n{version}\n,\n            \ndescription\n: \n{description}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages/import\n\n\nFunction: Import app package from Gateway's local file system\n\n\nPayload:\n\n\n{\n        \nfiles\n: [\n{file}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Creates or replaces the specified system alert. The condition has access to an object in its scope called \n_topic\n. An example alert might take the form of the following:\n\n\n_topic[\"applications.application_1400294100000_0001\"].allocatedContainers \n 5\n\n\n\nPayload:\n\n\n{\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n        \ntimeThresholdMillis\n:\n{time}\n\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Deletes the specified system alert\n\n\nGET /ws/v2/systemAlerts/alerts?inAlert={true/false}\n\n\nFunction: Gets the created alerts\n\n\nReturn:\n\n\n{\n    \nalerts\n: [{\n        \nname\n: \n{alertName}\n,\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n        \ntimeThresholdMillis\n:\n{time}\n,\n        \nalertStatus\n: {\n            \nisInAlert\n:{true/false}\n            \ninTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }\n    }, \u2026  ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Gets the specified system alert\n\n\nReturn:\n\n\n{\n    \nname\n: \n{alertName}\n,\n    \ncondition\n:\n{condition in javascript}\n,        \n    \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n    \ntimeThresholdMillis\n:\n{time}\n,\n    \nalertStatus\n: {\n        \nisInAlert\n:{true/false}\n        \ninTime\n: \n{time}\n,\n        \nmessage\n: \n{message}\n,\n        \nemailSent\n: {true/false}\n    }\n}\n\n\n\n\nGET /ws/v2/systemAlerts/history\n\n\nFunction: Gets the history of alerts\n\n\nReturn:\n\n\n{\n    \nhistory\n: [\n        {\n            \nname\n:\n{alertName}\n,\n            \ninTime\n:\n{time}\n,\n            \noutTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\nFunction: Gets the topic data that is used for evaluating alert\ncondition\n\n\nReturn:\n\n\n{\n     \n{topicName}\n: {json object data}, ...\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Creates or replaces the specified system alert template.\n\n\nPayload:\n\n\n{\n    \nisSystemTemplate\n: true,\n    \ndescription\n: \n{description}\n,\n    \nparameters\n: [\n        {\n          \nvariable\n: \n{replacement variable in Javascript block}\n,\n          \nlabel\n: \n{input label}\n,\n          \ntype\n: \n{number/text}\n,\n          \nplaceholder\n: \n{input placeholder}\n,\n          \ntooltip\n: \n{input tooltip}\n,\n          \nrequired\n: {true/false},\n          \ndefault\n: \n{default value}\n,     // optional\n          \nvalues\n: {                       // optional\n            \n{key}\n: \n{value}\n,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n    \nscript\n: \n{Javascript block}\n\n}\n\n\n\n\nExample:\n\n\n{\n    \ntemplates\n: [\n        {\n            \nisSystemTemplate\n: true,\n            \ndescription\n: \nAn alert template example.\n,\n            \nparameters\n: [\n                {\n                  \nvariable\n: \ncomparison\n,\n                  \nlabel\n: \nComparison\n,\n                  \ntype\n: \ntext\n,\n                  \nplaceholder\n: \nSelect a comparison\n,\n                  \ntooltip\n: \nChoose the comparison to use.\n,\n                  \nrequired\n: true,\n                  \ndefault\n: \n,\n                  \nvalues\n: {\n                    \n: \nless than\n,\n                    \n===\n: \nequals to\n,\n                    \n: \ngreater than\n\n                  }\n                },\n                {\n                  \nvariable\n: \ncount\n,\n                  \nlabel\n: \nNumber of Killed Containers\n,\n                  \ntype\n: \nnumber\n,\n                  \nplaceholder\n: \nEnter a valid number\n,\n                  \ntooltip\n: \nEnter the number.\n,\n                  \nrequired\n: false\n                }\n            ],\n            \nscript\n: \n/* Alert when number of killed containers is {{comparison}} {{count}} */\n\n                _topic['cluster.metrics'].numContainers {{comparison.key}} ({{count}} !== null ? {{count}} : 0);\n\n        }\n    ]\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Deletes the specified system alert template.\n\n\nGET /ws/v2/systemAlerts/templates/system\n\n\nFunction: Gets the created system alert templates\n\n\nReturn:\n\n\n{\n    \ntemplates\n: [\n        {\n            \nisSystemTemplate\n: true,\n            \ndescription\n: \n{description}\n,\n            \nparameters\n: [\n                {\n                  \nvariable\n: \n{replacement variable in Javascript block}\n,\n                  \nlabel\n: \n{input label}\n,\n                  \ntype\n: \n{number/text}\n,\n                  \nplaceholder\n: \n{input placeholder}\n,\n                  \ntooltip\n: \n{input tooltip}\n,\n                  \nrequired\n: {true/false},\n                  \ndefault\n: \n{default value}\n,     // optional\n                  \nvalues\n: {                       // optional\n                    \n{key}\n: \n{value}\n,\n                    \u2026\n                  }\n                },\n                \u2026\n            ],\n            \nscript\n: \n{Javascript block}\n\n        },\n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Gets the specified system alert template\n\n\nReturn:\n\n\n{\n    \nisSystemTemplate\n: true,\n    \ndescription\n: \n{description}\n,\n    \nparameters\n: [\n        {\n          \nvariable\n: \n{replacement variable in Javascript block}\n,\n          \nlabel\n: \n{input label}\n,\n          \ntype\n: \n{number/text}\n,\n          \nplaceholder\n: \n{input placeholder}\n,\n          \ntooltip\n: \n{input tooltip}\n,\n          \nrequired\n: {true/false},\n          \ndefault\n: \n{default value}\n,     // optional\n          \nvalues\n: {                       // optional\n            \n{key}\n: \n{value}\n,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n    \nscript\n: \n{Javascript block}\n\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/validate/script\n\n\nFunction: Validates Java script.\n\n\nPayload:\n\n\n{\n    \nscript\n: {\nscript\n}\n}\n\n\n\n\nGET /ws/v2/auth/users/{user}\n\n\nFunction: Gets the info of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPOST /ws/v2/auth/users/{user}\n\n\nFunction: Changes password and/or roles of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \noldPassword\n: \n{oldPassword}\n,\n    \nnewPassword\n: \n{newPassword}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPUT /ws/v2/auth/users/{user}\n\n\nFunction: Creates new user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nDELETE /ws/v2/auth/users/{user}\n\n\nFunction: Deletes the specified user\n\n\nGET /ws/v2/auth/users\n\n\nFunction: Gets the list of users\n\n\nReturn:\n\n\n{\n    \nusers\n: [ {\n       \nuserName\n: \n{username1}\n,\n       \nroles\n: [ \n{role1}\n, \u2026 ],\n       \npermissions\n: [ \n{permission1}\n, \u2026 ]\n    }\n}\n\n\n\n\nPOST /ws/v2/login\n\n\nFunction: Login\nPayload:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n\n}\n\n\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/logout\n\n\nFunction: Log out the current user\n\n\nReturn:\n\n\n{\n}\n\n\n\n\nPUT /ws/v2/config/auth\n\n\nFunction: Configure authentication. \nThe request specifies the type of authentication to setup such as password, kerberos, ldap etc and the configuration \nparameters for the authentication. The web service sets up the appropriate configuration files for the authentication \nas described in authentication section of \ndtgateway_security\n document. Gateway needs to be \nrestarted for the new authentication to take effect. This can be done by making the gateway restart web service request.\n\n\npayload:\n\n\n{\n    \ntype\n: \n{authenticationType}\n,\n    \nconfiguration\n:{ }\n}\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe web request \nGET /ws/v2/config/auth\n returns payload body that was sent in \nPUT\n request as the response verbatim. \n\n\nPassword\n\n\nFunction: Configure Password authentication \n\n\n{\n    \ntype\n: \npassword\n,\n    \nconfiguration\n:{ }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe web request \nGET /ws/v2/config/auth\n returns payload body that was sent in \nPUT\n request as the response verbatim. \n\n\n{\n    \ntype\n: \npassword\n,\n    \nconfiguration\n:{ }\n}\n\n\n\n\n\nKerberos with no group mapping\n\n\nFunction: Configure Kerberos authentication with no group mapping\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nTwo of the properties are mandatory, they are \"kerberosPrincipal\" \n \"kerberosKeytab\". A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should be \nspecified as \"false\".\n\n\n{\n    \ntype\n:\nkerberos\n,\n       \nconfiguration\n:{  \n         \ngroupSupport\n:\nfalse\n,\n         \nkerberosPrincipal\n:\n{kerberosPrincipal}\n,\n         \nkerberosKeytab\n:\n{Keytab}\n,\n         \ntokenValidity\n:\n{tokenValidity}\n,\n         \ncookieDomain\n:\n{cookieDomain}\n,\n         \ncookiePath\n:\n{cookiePath}\n,      \n         \nsignatureSecret\n:\n{signatureSecret}\n\n         }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nKerberos with group mapping\n\n\nFunction: Configure Kerberos authentication with group mapping \nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nTwo of these properties are mandatory, they are \"kerberosPrincipal\" \n \"kerberosKeytab\". A \"groupSupport\" property \nspecifies whether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should \nbe specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that \ncontains the mapping from kerberos groups to roles.\n\n\n\n{  \n   \ntype\n: \nkerberos\n,\n   \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nkerberosPrincipal\n: \n{kerberosPrincipal}\n,\n        \nkerberosKeytab\n: \n{Keytab}\n,\n        \ntokenValidity\n: \n{Validity}\n,\n        \ncookieDomain\n : \n{cookieDomain}\n,\n        \ncookiePath\n: \n{cookiePath}\n\n        \nsignatureSecret\n: \n{signatureSecret}\n\n        }\n    \ngroupMapping\n: [\n        { \n             \ngroup\n: \nusers\n,\n             \nroles\n: [\ndevelopers\n, \nadmins\n, \nqa\n] \n        },\n        {  \n             \ngroup\n: \nops\n,\n             \nroles\n: [\noperators\n]\n        }\n     ]  \n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nLDAP\n\n\nConfigure LDAP authentication. There are different configurations possible based on how the LDAP server is configured.\n\n\nAnonymous search allowed and no group mapping needed\n\n\nFunction: Configure LDAP authentication with anonymous search available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows LDAP groups\nto be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \ntype\n: \nldap\n,\n   \nconfiguration\n: {  \n       \ngroupSupport\n: \nfalse\n,\n       \nServer\n: \n{Server}\n,\n       \nPort\n: {port}\n +\n       \nuserBaseDn\n: \n{usserBaseDn}\n,\n       \nuserIdAttribute\n: \n{userIdAttribute}\n\n   }\n}\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed and no group mapping needed\n\n\nFunction: Configure LDAP authentication when anonymous search is not available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".\n\n\n\n{  \n    \ntype\n: \nldap\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \nfalse\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n, \n        \nuserObjectClass\n: \n{userObjectClass}\n\n    }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed but group mapping needed\n\n\nFunction: Configure LDAP authentication when anonymous search is not available on LDAP server but group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from LDAP groups to roles.\n\n\n{ \n    \ntype\n: \nldap\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n, \n        \nroleBaseDn\n: \n{roleBaseDn}\n,\n        \nuserRdnAttribute\n:\n{userRdnAttribute}\n, \n        \nroleNameAttribute\n: \n{roleNameAttribute}\n, \n        \nroleObjectClass\n: \n{roleObjectClass}\n, \n        \nuserObjectClass\n: \n{userObjectClass}\n\n    },\n    \ngroupMapping\n: [ \n        { \n            \ngroup\n: \nusers\n,\n            \nroles\n:[\ndevelopers\n] \n        },\n        { \n            \ngroup\n: \nops\n,\n            \nroles\n: [\noperators\n]\n        }\n    ]  \n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nActive Directory\n\n\nConfigure Active Directory authentication. There are different configurations possible based on how the Active Directory \nserver is configured.\n\n\nAnonymous search allowed and no group mapping needed\n\n\nFunction: Configure Active Directory authentication with anonymous search available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows \nActive Directory groups to be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \n   \ntype\n: \nad\n,\n   \nconfiguration\n: {  \n       \ngroupSupport \n: \nfalse\n,\n       \nServer\n: \n{server}\n,\n       \nPort\n: {port},\n       \nuserSearchFilter\n: \n{userSearchFilter}\n,\n       \nuserBaseDn\n: \n{userBaseDn}\n,\n       \nuserIdAttribute\n: \n{userIdAttribute}\n,\n       \nuserDomain\n : \n{userDomain}\n    \n   }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed and no group mapping needed\n\n\nFunction: Configure Active Directory authentication when anonymous search is not available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".\n\n\n\n{  \n   \ntype\n: \nad\n,\n   \nconfiguration\n: {  \n   \ngroupSupport\n: \nfalse\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n,\n        \nuserObjectClass\n: \n{userObjectClass}\n\n    }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed but group mapping needed\n\n\nFunction: Configure Active Directory authentication when anonymous search is not available on Active Directory server but group mapping is needed\nThe configuration comprises different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from Active Directory groups to roles.\n\n\n{ \n    \ntype\n: \nad\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nServer\n: \n{Server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n,\n        \nroleBaseDn\n: \n{roleBaseDn}\n,\n        \nuserRdnAttribute\n: \n{userRdnAttribute}\n,\n        \nroleNameAttribute\n: \nroleNameAttribute\n,\n        \nroleObjectClass\n: \nroleObjectClass\n,\n        \nuserObjectClass\n: \n{userObjectClass}\n,\n     },\n     \ngroupMapping\n: [ ]  \n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPAM\n\n\nConfigure PAM or Pluggable Authentication Mechanism. PAM is the de-facto authentication available on Linux systems.\n\n\nPAM with no group mapping\n\n\nFunction: Configure PAM authentication with no group mapping\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nIn PAM, service name is mandatory \n there is no configuration. A \"groupSupport\" property specifies whether group mapping \nshould be enabled. Group mapping allows PAM groups to be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \n   \ntype\n:\npam\n,\n   \nconfiguration\n:{  \n      \ngroupSupport\n:\nfalse\n,\n      \nserviceName\n:\n{serviceName}\n\n   }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPAM with group mapping\n\n\nFunction: Configure PAM authentication with group mapping \nThe configuration comprises of different properties as described in the \ndtgateway_security\n document.\nIn PAM, service name is mandatory \n there is no configuration. Group mapping allows PAM groups to be mapped to roles. It \nshould be specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be \nspecified that contains the mapping from PAM groups to roles.\n\n\n\n{  \n   \ntype\n:\npam\n,\n   \nconfiguration\n:{  \n      \ngroupSupport\n:\ntrue\n,\n      \nserviceName\n:\n{serviceName}\n\n   },\n\n   \ngroupMapping\n: [ \n       { \n          \ngroup\n: \nusers\n,\n          \nroles\n:[\ndevelopers\n] \n       },\n       { \n          \ngroup\n: \nops\n,\n          \nroles\n: [\noperators\n]\n       }\n   ]  \n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPUT /ws/v2/config/groupMapping\n\n\nFunction: Specify group to role mapping\nSet or update mapping from groups of configured authentication mechanism to RTS roles.\n\n\n\n{\n   \ngroupMapping\n : [\n        {\n           \ngroup\n : \nusers\n,\n           \nrole\n : [\ndevelopers\n, \nadmins\n, \nqa\n, \ninterns\n]\n        },\n        {\n           \ngroup\n: \nops\n,\n           \nrole\n : [\noperators\n]\n        }\n   ]\n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPublisher-Subscriber WebSocket Protocol\n\n\ndtGateway provides a light-weight pubsub websocket service.\nThe URL of dtGateway's pubsub websocket service is: \nws://{dtGateway-host-port}/pubsub\n.\nFor example: \nws://localhost:9090/pubsub\n\n\nInput\n\n\nPublishing\n\n\n{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nSubscribing\n\n\n{\"type\":\"subscribe\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing\n\n\n{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing from the number of subscribers of a topic\n\n\n{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nOutput\n\n\nNormal Published Data\n\n\n{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nNumber of Subscribers:\n\n\n{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}\n\n\n\nAuto publish topics\n\n\ndata that gets published every one second:\n\n\n\n\napplications\n - list of streaming applications running in the cluster\n\n\napplications.[appid]\n - information about a particular application\n\n\napplications.[appid].containers\n - information about containers of a particular application\n\n\napplications.[appid].physicalOperators\n - information about operators of a particular application\n\n\napplications.[appid].logicalOperators\n - information about logical operators of a particular application\n\n\napplications.[appid].events\n - events from the AM of a particularapplication\n\n\n\n\ndata that gets published every five seconds:\n\n\n\n\ncluster.metrics\n - metrics of the cluster", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#datatorrent-dtgateway-api-v2-specification", 
            "text": "", 
            "title": "DataTorrent dtGateway API v2 Specification"
        }, 
        {
            "location": "/dtgateway_api/#rest-api", 
            "text": "", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#return-codes", 
            "text": "200 : OK  400 : The request is not in the format that the server expects  404 : The resource is not found  500 : Something is wrong on the server side", 
            "title": "Return codes"
        }, 
        {
            "location": "/dtgateway_api/#rest-uri-specification", 
            "text": "", 
            "title": "REST URI Specification"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2about", 
            "text": "Function:  Return:  {\n     buildVersion :  {Apex build version} ,\n     buildDate :  {Apex build date and time} ,\n     buildRevision :  {Apex revision} ,\n     buildUser :  {Apex build user} ,\n     version :  {Apex version} ,\n     rtsBuildVersion :  {RTS build version} ,\n     rtsBuildDate :  {RTS build date and time} ,\n     rtsBuildRevision :  {RTS revision} ,\n     rtsBuildUser :  {RTS build user} ,\n     rtsVersion :  {RTS version} ,\n     gatewayUser :  {user} ,\n     javaVersion :  {java_version} ,\n     hadoopLocation :  {hadoop_location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hostname :  {hostname} ,\n     hadoopIsSecurityEnabled :  {true/false} \n}", 
            "title": "GET /ws/v2/about"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2clustermetrics", 
            "text": "Function: List metrics that are relevant to the entire cluster  Return:  {\n     averageAge :  {average running application age in milliseconds} ,\n     cpuPercentage :  {cpuPercentage} ,\n     currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n     maxMemoryAllocatedMB :  {maxMemoryAllocatedMB} ,\n     numAppsFailed :  {numAppsFailed} ,\n     numAppsFinished :  {numAppsFinished} ,\n     numAppsKilled :  {numAppsKilled} ,\n     numAppsPending :  {numAppsPending} ,\n     numAppsRunning :  {numAppsRunning} ,\n     numAppsSubmitted :  {numAppsSubmitted} ,\n     numContainers :  {numContainers} ,\n     numOperators :  {numOperators} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/cluster/metrics"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsstatesstate_filternamename_filteruseruser_filter", 
            "text": "Function: List IDs of all streaming applications  Return:  {\n     apps : [\n        {\n             diagnostics :  {diagnostics} ,\n             elapsedTime :  {elapsedTime} ,\n             finalStatus :  {finalStatus} ,\n             finishedTime :  {finishedTime} ,\n             id :  {appId} ,\n             name :  {name} ,\n             queue :  {queue} ,\n             startedTime :  {startedTime} ,\n             state :  {state} ,\n             trackingUrl :  {trackingUrl} ,\n             user :  {user} \n        },  \n        \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications[?states={STATE_FILTER}&amp;name={NAME_FILTER}&amp;user={USER_FILTER]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappid", 
            "text": "Function: Get the information for the specified application  Return:  {\n     id :  {appid} ,\n     name :  {name} ,\n     state :  {state} ,\n     trackingUrl :  {tracking url} ,\n     finalStatus : {finalStatus},\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     elapsedTime :  {elapsedTime} ,\n     startedTime :  {startTime} ,\n     user :  {user} ,\n     version :  {stram version} ,\n     remainingLicensedMB :  {remainingLicensedMB} ,\n     allocatedMB :  {allocatedMB} ,\n     gatewayConnected :  true/false ,\n     connectedToThisGateway :  true/false ,\n     attributes : {\n            {attributeName} :  {attributeValue} , \n            {attributeName-n} :  {attributeValue-n} , \n    },\n     stats : {\n         allocatedContainers :  {allocatedContainer} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         latency :  {overall latency} ,\n         criticalPath :  {list of operator id that represents the critical path} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         plannedContainers :  {plannedContainers} ,\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed : {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA : {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted : {sum of operators:totalTuplesEmitted} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} \n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplan", 
            "text": "Function: Return the physical plan for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             isUnifier : true/false\n        },\n         \u2026\n     ],\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperators", 
            "text": "Function: Return list of operators for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             counters : {\n                 {counterName} :  {counterValue} , \n                ...\n             },\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             unifierClass :  {unifierClass} \n        },\n         \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanstreams", 
            "text": "Function: Return physical streams  Return:  {\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/streams"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopid", 
            "text": "Function: Return information of the given operator for the given application  Return:  {\n     className :  {className} ,\n     container :  {containerId} ,\n     counters : {\n       {counterName}:  {counterValue} , ...            \n    }\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     currentWindowId :  {currentWindowId} ,\n     failureCount :  {failureCount} ,\n     host :  {host} ,\n     id :  {id} ,\n     ports : [\n       {\n           bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n           name :  {name} ,\n           totalTuples :  {totalTuples} ,\n           tuplesPSMA :  {tuplesPSMA} ,\n           type :  input/output \n       }, ...\n    ],\n     lastHeartbeat :  {lastHeartbeat} ,\n     latencyMA :  {latencyMA} ,\n     name :  {name} ,\n     recordingStartTime :  {recordingStartTime} ,\n     recoveryWindowId :  {recoveryWindowId} ,\n     status :  {status} ,\n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopiddeployhistory", 
            "text": "Function: Return container deploy history of this operator\nSince: 1.0.6  Return:  {\n    containers : [  \n        {  \n             container :  {containerId} ,   \n             startTime :  {startTime}   \n        }, ...  \n    ],   \n     name :  {operatorName} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidports", 
            "text": "Function: Get the information of all ports of the given operator of the\ngiven application  Return:  {  \n     ports : [\n        {  \n             bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,  \n             totalTuples :  {totalTuples} ,   \n             tuplesPSMA :  {tuplesPSMA} ,   \n             type :  output   \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidportsportname", 
            "text": "Function: Get the information of a specified port  Return:  {  \n     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n     name :  {name} ,   \n     totalTuples :  {totalTuples} ,   \n     tuplesPSMA :  {tuplesPSMA} ,   \n     type :  {type}   \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesparentparentqsearchtermpackageprefixescomma-separated-package-prefixes", 
            "text": "Function: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent  Return:  {  \n     operatorClasses : [  \n        {  name : {className}  },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}&amp;q={searchTerm}&amp;packagePrefixes={comma-separated-package-prefixes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesoperatorclass", 
            "text": "Function: Get the description of the given operator class  Return:  {\n     inputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n          ...\n    ],\n     outputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n        \u2026\n    ],\n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidshutdown", 
            "text": "Function: Shut down the application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/shutdown"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidkill", 
            "text": "Function: Kill the given application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/kill"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstart", 
            "text": "Function: Start recording on operator  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstop", 
            "text": "Function: Stop recording on operator  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstart", 
            "text": "Function: Start recording on port  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstop", 
            "text": "Function: Stop recording on port  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatesnewallocatedactivekilled", 
            "text": "Function: Return the list of containers for this application  Return:  {\n     containers : [\n        {\n             host :  {host} ,\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat :  {lastHeartbeat} ,\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             numOperators :  {numOperators} ,\n             operators:  {\n                 id1 :  name1 ,\n                 id2 :  name2 ,\n                 id3 :  name3 \n            },\n             containerLogsUrl :  {containerLogsUrl} ,\n             state :  {state} \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontainerid", 
            "text": "Function: Return the information of the specified container  Return:  {\n     host :  {host} ,\n     id :  {id} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :  {lastHeartbeat} ,\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     numOperators :  {numOperators} ,\n     operators:  {\n         id1 :  name1 ,\n         id2 :  name2 ,\n         id3 :  name3 \n    },\n     containerLogsUrl :  {containerLogsUrl} ,\n     state :  {state} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogs", 
            "text": "Function: Return the container log list  Return:  {\n     logs : [\n        {\n             length :  {log length} ,\n             name :  {logName} \n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridstacktrace", 
            "text": "Since: 3.4.0   Function: Return the container stack trace  Return:  {\n     threads : [\n        {\n             name :  {name} ,\n             state :  {state} ,\n             id :  {id} ,\n             stackTraceElements : [\n                 {line1} ,\n                 {line2} , ...\n            ]\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/stackTrace"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogslognamestartstartposendendposgrepregexpincludeoffsettruefalse", 
            "text": "Function: Return the raw log  Return: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):  {\n     lines : [\n        {  byteOffset : {byteOffset} ,  line :  {line}  }, \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}&amp;end={endPos}&amp;grep={regexp}&amp;includeOffset={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplancontainerscontaineridkill", 
            "text": "Function: Kill this container  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplan", 
            "text": "Function: Return the logical plan of this application  Return:  {\n     operators : [\n      {\n         name :  {name} ,\n         attributes : {attributeMap},\n         class :  {class} ,\n         ports : {\n           [\n            {\n                 name :  {name} ,\n                 attributes : {attributeMap},\n                 type :  input/output \n            }, ...\n           ]\n         },\n          properties : {\n             class :  {class} \n         }\n      }, ...\n    ],\n     streams : [\n        {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanattributes", 
            "text": "Function: Return the application attributes  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperators", 
            "text": "Function: Return the list of info of the logical operator  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopname", 
            "text": "Function: Return the info of the logical operator  Return:  {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Return the properties of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Set the properties of the logical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Return the properties of the physical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Set the properties of the physical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameattributes", 
            "text": "Function: Get the attributes of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameportsportnameattributes", 
            "text": "Function:  Get the attributes of the port  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplan", 
            "text": "Function: Change logical plan of this application\nPayload:  {\n     requests : [\n        {\n             requestType :  AddStreamSinkRequest ,\n             streamName :  {streamName} ,\n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  CreateOperatorRequest ,\n             operatorName :  {operatorName} ,\n             operatorFQCN :  {operatorFQCN} ,\n        },\n        {\n             requestType :  CreateStreamRequest ,\n             streamName :  {streamName} ,\n             sourceOperatorName :  {sourceOperatorName} ,\n             sourceOperatorPortName :  {sourceOperatorPortName} \n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  RemoveOperatorRequest ,\n             operatorName :  {operatorName} ,\n        },\n        {\n             requestType :  RemoveStreamRequest ,\n             streamName :  {streamName} ,\n        },\n        {\n             requestType :  SetOperatorPropertyRequest ,\n             operatorName :  {operatorName} ,\n             propertyName :  {propertyName} ,\n             propertyValue :  {propertyValue} \n        },\n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsmeta", 
            "text": "Function: Return the meta information about the statistics stored for\nthis operator  Return:  {\n     appId :  {appId} ,\n     operatorName :  {operatorName} ,\n     operatorIds : [ {opid}, \u2026 ],\n     startTime :  {startTime} ,\n     endTime :  {endTime} ,\n     count :  {count} ,\n     ended :  {boolean} \n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the statistics stored for this logical operator  {\n     operatorStats : [\n        {\n             operatorId :  {operatorId} ,\n             timestamp :  {timestamp} ,\n             stats : {\n                 container :  containerId ,\n                 host :  host ,\n                 totalTuplesProcessed ,  {totalTuplesProcessed} ,\n                 totalTuplesEmitted ,  {totalTuplesEmitted} ,\n                 tuplesProcessedPSMA ,  {tuplesProcessedPSMA} ,\n                 tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n                 cpuPercentageMA :  {cpuPercentageMA} ,\n                 latencyMA :  {latencyMA} ,\n                 ports : [ {\n                     name :  {name} ,\n                     type : {input/output} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA ,  {tuplesPSMA} ,\n                     bufferServerBytesPSMA ,  {bufferServerBytesPSMA} \n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsmeta", 
            "text": "Function: Return the meta information about the container statistics  {\n     appId :  {appId} ,\n     containers : {\n         {containerId} : {\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             host :  {host} ,\n             memoryMBAllocated ,  {memoryMBAllocated} \n        },\n        \u2026\n    },\n     startTime :  {startTime} \n     endTime :  {endTime} \n     count :  {count} \n     ended : {boolean}\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the container statistics stored for this application  {\n     containerStats : [\n        {\n             containerId :  {containerId} \n             timestamp :  {timestamp} \n             stats : {\n                 numOperators :  {numOperators} ,\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidrecordings", 
            "text": "Function: Get the list of all recordings for this application  Return:  {\n     recordings : [{\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordings", 
            "text": "Function: Get the list of recordings on this operator  Return:  {\n     recordings : [ {\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Get the information about the recording  Return:  {\n     id :  {id} ,\n     startTime :  {startTime} ,\n     appId :  {appId} ,\n     operatorId :  {operatorId} ,\n     containerId :  {containerId} ,\n     totalTuples :  {totalTuples} ,\n     ports : [ {\n        name :  {portName} ,\n        streamName :  {streamName} ,\n        type :  {type} ,\n        id :  {index} ,\n        tupleCount :  {tupleCount} \n     } \u2026 ],\n     ended : {boolean},\n     windowIdRanges : [ {\n        low :  {lowId} ,\n        high :  {highId} \n     } \u2026 ],\n     properties : {\n        name :  value , ...\n     }\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Deletes the specified recording  Since: 1.0.4", 
            "title": "DELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsidtuples", 
            "text": "Query Parameters:  offset\nstartWindow\nlimit\nports\nexecuteEmptyWindow  Function: Get the tuples  Return:  {\n     startOffset :  {startOffset} ,\n     tuples : [ {\n         windowId :  {windowId} ,\n         tuples : [ {\n             portId :  {portId} ,\n             data :  {tupleData} \n        }, \u2026 ]\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappideventsfromfromtimetototimeoffsetoffsetlimitlimit", 
            "text": "Function: Get the events  Return:  {\n     events : [ {\n            id :  {id} ,\n         timestamp :  {timestamp} ,\n         type :  {type} ,\n         data : {\n             name :  value , \u2026\n        }\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/events?from={fromTime}&amp;to={toTime}&amp;offset={offset}&amp;limit={limit}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profileuser", 
            "text": "Function: Get the user profile information, list of roles and list of\npermissions given the user  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/profile/user"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettings", 
            "text": "Function: Get the current user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuser", 
            "text": "Function: Get the specified user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuserkey", 
            "text": "Function: Get the specified user's setting key  Return:  {\n     value : {value}\n}", 
            "title": "GET /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2profilesettingsuserkey", 
            "text": "Function: Set the specified user's setting key\nPayload:  {\n     value : {value}\n}", 
            "title": "PUT /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authroles", 
            "text": "Function: Get the list of roles the system has  Return:  {\n     roles : [\n       {\n          name :  {role1} ,\n          permissions : [  {permission1} , \u2026 ]\n       }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/auth/roles"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authrolesrole", 
            "text": "Function: Get the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authrolesrole", 
            "text": "Function: create or edit the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "PUT /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authrestoredefaultroles", 
            "text": "Function: Restores default roles", 
            "title": "POST /ws/v2/auth/restoreDefaultRoles"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authrolesrole", 
            "text": "Function: delete the given role", 
            "title": "DELETE /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authpermissions", 
            "text": "Function: Get the list of possible permissions  Return:  {\n     permissions : [ {\n        name :  {permissionName} ,\n        adminOnly : true/false\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2applicationsappidpermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidpermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownernamepermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownernamepermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2licenses", 
            "text": "Function: Add a license to the registry  Payload: The license file content  Return:  {\n   id :  {licenseId} ,\n   expireTime : {unixTimeMillis},\n   expirationTimeNotificationPeriod : {timeMillis},\n   nodesAllowed : {nodesAllowed},\n   memoryMBAllowed : {memoryMBAllowed},\n   exceedGracePeriod : {timeMillis},\n   contextType :  {contextType} ,\n   type :  {type} ,\n   features : [  {feature1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/licenses"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2licensescurrent", 
            "text": "Function: Get info on the current license  {\n       id :  {licenseId} ,\n       currentTime : {unixTimeMillis},\n       expireTime : {unixTimeMillis},\n       nodesAllowed : {nodesAllowed},\n       nodesUsed : {nodesUsed},\n       memoryMBAllowed : {memoryMBAllowed},\n       memoryMBUsed : {memoryMBUsed},\n       exceedGracePeriod : {timeMillis}, // memory exceed grace period\n       exceedRemainingTime : {timeMillis},  // (optional)\n       violation :  memory , // returns violation type (optional)\n       contextType :  {community|standard|enterprise} ,\n       type :  {evaluation|non_production|production} \n       features : [  {feature1} , \u2026 ], // for community, empty array\n       current : true/false,\n       expirationTimeNotificationLevel :  {INFO|WARN|ERROR} , // (optional)\n       valid : true/false // true, if the license is valid\n}", 
            "title": "GET /ws/v2/licenses/current"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configinstallmode", 
            "text": "Function: returns the install mode  {\n   installMode :  {evaluation|community|app} ,\n   appPackageName :  {optionalAppPackageName} ,\n   appPackageVersion :  {optionalAppPackageVersion} \n}", 
            "title": "GET /ws/v2/config/installMode"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function: returns the download type  {\n   value :  true/false \n}", 
            "title": "GET /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function:  {\n   value :  true/false \n}  Feature List:     SYSTEM_APPS  SYSTEM_ALERTS  APP_DATA_DASHBOARDS  RUNTIME_DAG_CHANGE  RUNTIME_PROPERTY_CHANGE  APP_CONTAINER_LOGS  LOGGING_LEVELS  APP_DATA_TRACKER  JAAS_LDAP_AUTH  APP_BUILDER", 
            "title": "PUT /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configproperties", 
            "text": "Function: Returns list of properties from dt-site.xml.  Return:  {\n     {name} : {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} \n    }\n\n}", 
            "title": "GET /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesproperty_name", 
            "text": "Function: Returns single property from dt-site.xml, specify by name  Return:  {\n     value :  {PROPERTY_VALUE} ,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "GET /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configproperties", 
            "text": "Function: Overwrites all specified properties in dt-site.xml  Payload:  {\n     properties : [\n        {\n             name :  {name} \n             value :  {PROPERTY_VALUE} ,\n             local : true/false,\n                     description :  {PROPERTY_DESCRIPTION} \n        }, \u2026\n    ]\n}", 
            "title": "POST /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesproperty_name", 
            "text": "Function: Overwrites or creates new property in dt-site.xml\nPayload:  {\n     value :  {PROPERTY_VALUE} ,\n     local : true/false,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "PUT /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2configpropertiesproperty_name", 
            "text": "Function: Deletes a property from dt-site.xml.", 
            "title": "DELETE /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2confighadoopexecutable", 
            "text": "Function: Returns the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "GET /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2confighadoopexecutable", 
            "text": "Function: Sets the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "PUT /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configissues", 
            "text": "Function: Returns list of potential issues with environment  Return:  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        {...},\n        {...}\n    ]    \n}", 
            "title": "GET /ws/v2/config/issues"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configipaddresses", 
            "text": "Function: Returns list of ip addresses the gateway can listen to  Return:  {\n     ipAddresses : [\n       1.2.3.4 , ...\n    ]    \n}", 
            "title": "GET /ws/v2/config/ipAddresses"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configrestart", 
            "text": "Function: Restarts the gateway  Payload: none", 
            "title": "POST /ws/v2/config/restart"
        }, 
        {
            "location": "/dtgateway_api/#get-proxyrmv1", 
            "text": "", 
            "title": "GET /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-proxyrmv1", 
            "text": "Function: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.", 
            "title": "POST /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#get-proxystramv2", 
            "text": "", 
            "title": "GET /proxy/stram/v2/..."
        }, 
        {
            "location": "/dtgateway_api/#post-proxystramv2", 
            "text": "", 
            "title": "POST /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#put-proxystramv2", 
            "text": "", 
            "title": "PUT /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#delete-proxystramv2", 
            "text": "Function: Proxy calls to Stram Web Services.", 
            "title": "DELETE /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidloggers", 
            "text": "Function: Set the logger levels of packages/classes.  Payload:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggers", 
            "text": "Function: Gets the logger levels of packages/classes.  Return:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggerssearchpatternpattern", 
            "text": "Function: searches for all classes that match the pattern.  Return:  {\n     loggers  : [\n        {\n             name  :  {fully qualified class name} ,\n             level :  {logger level} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\""
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidrestartqueuequeue", 
            "text": "Since: 3.4.0\nFunction: Restart the terminated application. Payload is optional.\nPayload:  {\n   {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n   appId :  {appId} \n}", 
            "title": "POST /ws/v2/applications/{appid}/restart[?queue={queue}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackages", 
            "text": "Since: 1.0.4  Function: Gets the list of appPackages the user can view in the system  {\n     appPackages : [\n        {\n                  appPackageName :  {appPackageName} ,\n                  appPackageVersion :  {appPackageVersion} ,\n             modificationTime :  {modificationTime} ,\n             owner :  {owner} ,\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/appPackages"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesmergereplacefailourstheirs", 
            "text": "Since: 1.0.4  Function: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package  Payload: the raw zip file  Return: The information of the app package", 
            "title": "POST /ws/v2/appPackages?merge={replace|fail|ours|theirs}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownername", 
            "text": "Since: 1.0.4  Function: Gets the list of versions of appPackages with the given name in the system owned by the specified user  {\n     versions : [\n         1.0-SNAPSHOT \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Deletes the appPackage", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversiondownload", 
            "text": "Since: 1.0.4  Function: Downloads the appPackage zip file", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Gets the meta information of the app package  Returns:  {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     modificationTime :   {modificationTime} ,\n     owner :  {owner} ,\n    ...\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigs", 
            "text": "Since: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:  {\n     configs : [\n         my-app-conf1.xml \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Gets the properties XML of the specified config  Returns:  configuration \n         property \n                 name ... /name \n                 value ... /value \n         /property \n        \u2026 /configuration", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Creates or replaces the specified config with the property parameters specified payload  Payload: configuration in XML", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Deletes the specified config", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplications", 
            "text": "Since: 1.0.4  Function: Gets the list of applications in the appPackage  Returns:  {\n     applications : [\n        {\n             dag : {dag in json format},\n             file :  {fileName} ,\n             name :  {name} ,\n             type :  {type} ,\n             error :  {error} ,\n             fileContent : {originalFileContentForJSONTypeApp}\n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplicationsappname", 
            "text": "Since: 1.0.4  Function: Gets the meta data for that application  Returns:  {\n     file :  {fileName} ,\n     name :  {name} ,\n     type :  {json/class/properties} ,\n     error :  {error} \n     dag : {\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n         }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n    },\n     fileContent : {originalFileContentForJSONTypeApp}\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesuserapppackagenameapppackageversionmerge", 
            "text": "Function: Merge the configuration, json apps, and resources files from the app package specified by user/name/version from the payload to the specified app package in the url, without overwriting any existing file in the specified app package. If replaceExisting is true, the files in the app, conf and resources directory of the app package will be replaced by the ones in the app package specified in the payload. Otherwise, they will not be replaced. The fields user, name and replaceExisting in the payload are optional. If user and name are not specified, they are default to be the same as in the URI path. replaceExisting's default is false.  Payload:  {\n  user :  {user} ,\n  name :  {name} ,\n  version :  {versionToMergeFrom} ,\n  replaceExisting :  {true/false} \n}", 
            "title": "POST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesownerpackagenamepackageversionapplicationsappnamelaunchconfigconfignameoriginalappidoriginalappidqueuequeuename", 
            "text": "Since: 1.0.4  Function: Launches the application with the given configuration specified in the POST payload  Payload:  {\n     {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n     appId :  {appId} \n}", 
            "title": "POST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}&amp;originalAppId={originalAppId}&amp;queue={queueName}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperatorsclassname", 
            "text": "Since: 1.0.4  Function: Get the properties of the operator given the classname in the jar  {  \n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationnameerrorifexiststruefalse", 
            "text": "Function: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app package  Payload:  {\n         displayName :  {displayName} ,\n         description :  {description} ,\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n          }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n}  Return:  {\n         error :  {error} \n}  Available port attributes to set:    AUTO_RECORD  IS_OUTPUT_UNIFIED  PARTITION_PARALLEL  QUEUE_CAPACITY  SPIN_MILLIS  STREAM_CODEC  UNIFIER_LIMIT   Available locality options to set:    THREAD_LOCAL  CONTAINER_LOCAL  NODE_LOCAL  RACK_LOCAL", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationname", 
            "text": "Since: 1.0.5  Function: Deletes non-jar based application in the app package", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperators", 
            "text": "Since: 1.0.5  Function: Get the classes of operators from specified app package.  Return:  {  \n     operatorClasses : [  \n        {\n             name : {fullyQualifiedClassName} , \n             title :  {title} ,\n             shortDesc :  {description} ,\n             longDesc :  {description} ,\n             category :  {categoryName} ,\n             doclink :  {doc url} ,\n             tags : [  {tag} ,  {tag} , \u2026 ],\n             inputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ]\n             outputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ],\n             properties : [  \n                {\n                     name : {propertyName} ,\n                     canGet : {canGet},\n                     canSet : {canSet},\n                     type : {type} ,\n                     description : {description} ,\n                     properties : ...\n                }, \u2026\n            ],\n             defaultValue : {\n                 {propertyName} : [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesimport", 
            "text": "Function: List the importable app packages on Gateway's local file\nsystem  Return:  {\n     appPackages: [\n        {\n             file :  {file} ,\n             name :  {name} ,\n             displayName :  {displayName} ,\n             version :  {version} ,\n             description :  {description} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesimport", 
            "text": "Function: Import app package from Gateway's local file system  Payload:  {\n         files : [ {file} , \u2026 ]\n}", 
            "title": "POST /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertsalertsname", 
            "text": "Function: Creates or replaces the specified system alert. The condition has access to an object in its scope called  _topic . An example alert might take the form of the following:  _topic[\"applications.application_1400294100000_0001\"].allocatedContainers   5  Payload:  {\n         condition : {condition in javascript} ,\n         email : {email} ,\n     description :  {description} ,\n         timeThresholdMillis : {time} \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2systemalertsalertsname", 
            "text": "Function: Deletes the specified system alert", 
            "title": "DELETE /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsinalerttruefalse", 
            "text": "Function: Gets the created alerts  Return:  {\n     alerts : [{\n         name :  {alertName} ,\n         condition : {condition in javascript} ,\n         email : {email} ,\n     description :  {description} ,\n         timeThresholdMillis : {time} ,\n         alertStatus : {\n             isInAlert :{true/false}\n             inTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }\n    }, \u2026  ]\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts?inAlert={true/false}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsname", 
            "text": "Function: Gets the specified system alert  Return:  {\n     name :  {alertName} ,\n     condition : {condition in javascript} ,        \n     email : {email} ,\n     description :  {description} ,\n     timeThresholdMillis : {time} ,\n     alertStatus : {\n         isInAlert :{true/false}\n         inTime :  {time} ,\n         message :  {message} ,\n         emailSent : {true/false}\n    }\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertshistory", 
            "text": "Function: Gets the history of alerts  Return:  {\n     history : [\n        {\n             name : {alertName} ,\n             inTime : {time} ,\n             outTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/systemAlerts/history"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstopicdata", 
            "text": "Function: Gets the topic data that is used for evaluating alert\ncondition  Return:  {\n      {topicName} : {json object data}, ...\n}", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertstemplatessystemname", 
            "text": "Function: Creates or replaces the specified system alert template.  Payload:  {\n     isSystemTemplate : true,\n     description :  {description} ,\n     parameters : [\n        {\n           variable :  {replacement variable in Javascript block} ,\n           label :  {input label} ,\n           type :  {number/text} ,\n           placeholder :  {input placeholder} ,\n           tooltip :  {input tooltip} ,\n           required : {true/false},\n           default :  {default value} ,     // optional\n           values : {                       // optional\n             {key} :  {value} ,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n     script :  {Javascript block} \n}  Example:  {\n     templates : [\n        {\n             isSystemTemplate : true,\n             description :  An alert template example. ,\n             parameters : [\n                {\n                   variable :  comparison ,\n                   label :  Comparison ,\n                   type :  text ,\n                   placeholder :  Select a comparison ,\n                   tooltip :  Choose the comparison to use. ,\n                   required : true,\n                   default :  ,\n                   values : {\n                     :  less than ,\n                     === :  equals to ,\n                     :  greater than \n                  }\n                },\n                {\n                   variable :  count ,\n                   label :  Number of Killed Containers ,\n                   type :  number ,\n                   placeholder :  Enter a valid number ,\n                   tooltip :  Enter the number. ,\n                   required : false\n                }\n            ],\n             script :  /* Alert when number of killed containers is {{comparison}} {{count}} */\n\n                _topic['cluster.metrics'].numContainers {{comparison.key}} ({{count}} !== null ? {{count}} : 0); \n        }\n    ]\n}", 
            "title": "PUT /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2systemalertstemplatessystemname", 
            "text": "Function: Deletes the specified system alert template.", 
            "title": "DELETE /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstemplatessystem", 
            "text": "Function: Gets the created system alert templates  Return:  {\n     templates : [\n        {\n             isSystemTemplate : true,\n             description :  {description} ,\n             parameters : [\n                {\n                   variable :  {replacement variable in Javascript block} ,\n                   label :  {input label} ,\n                   type :  {number/text} ,\n                   placeholder :  {input placeholder} ,\n                   tooltip :  {input tooltip} ,\n                   required : {true/false},\n                   default :  {default value} ,     // optional\n                   values : {                       // optional\n                     {key} :  {value} ,\n                    \u2026\n                  }\n                },\n                \u2026\n            ],\n             script :  {Javascript block} \n        },\n        \u2026\n    ]\n}", 
            "title": "GET /ws/v2/systemAlerts/templates/system"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstemplatessystemname", 
            "text": "Function: Gets the specified system alert template  Return:  {\n     isSystemTemplate : true,\n     description :  {description} ,\n     parameters : [\n        {\n           variable :  {replacement variable in Javascript block} ,\n           label :  {input label} ,\n           type :  {number/text} ,\n           placeholder :  {input placeholder} ,\n           tooltip :  {input tooltip} ,\n           required : {true/false},\n           default :  {default value} ,     // optional\n           values : {                       // optional\n             {key} :  {value} ,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n     script :  {Javascript block} \n}", 
            "title": "GET /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertsvalidatescript", 
            "text": "Function: Validates Java script.  Payload:  {\n     script : { script }\n}", 
            "title": "PUT /ws/v2/systemAlerts/validate/script"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusersuser", 
            "text": "Function: Gets the info of the given user  Return:  {\n     userName :  {userName} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "GET /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authusersuser", 
            "text": "Function: Changes password and/or roles of the given user  Return:  {\n     userName :  {userName} ,\n     oldPassword :  {oldPassword} ,\n     newPassword :  {newPassword} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "POST /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authusersuser", 
            "text": "Function: Creates new user  Return:  {\n     userName :  {userName} ,\n     password :  {password} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "PUT /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authusersuser", 
            "text": "Function: Deletes the specified user", 
            "title": "DELETE /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusers", 
            "text": "Function: Gets the list of users  Return:  {\n     users : [ {\n        userName :  {username1} ,\n        roles : [  {role1} , \u2026 ],\n        permissions : [  {permission1} , \u2026 ]\n    }\n}", 
            "title": "GET /ws/v2/auth/users"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2login", 
            "text": "Function: Login\nPayload:  {\n     userName :  {userName} ,\n     password :  {password} \n}  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/login"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2logout", 
            "text": "Function: Log out the current user  Return:  {\n}", 
            "title": "POST /ws/v2/logout"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configauth", 
            "text": "Function: Configure authentication. \nThe request specifies the type of authentication to setup such as password, kerberos, ldap etc and the configuration \nparameters for the authentication. The web service sets up the appropriate configuration files for the authentication \nas described in authentication section of  dtgateway_security  document. Gateway needs to be \nrestarted for the new authentication to take effect. This can be done by making the gateway restart web service request.  payload:  {\n     type :  {authenticationType} ,\n     configuration :{ }\n}  Returns:  {\n}  The web request  GET /ws/v2/config/auth  returns payload body that was sent in  PUT  request as the response verbatim.", 
            "title": "PUT /ws/v2/config/auth"
        }, 
        {
            "location": "/dtgateway_api/#password", 
            "text": "Function: Configure Password authentication   {\n     type :  password ,\n     configuration :{ }\n};  Returns:  {\n}  The web request  GET /ws/v2/config/auth  returns payload body that was sent in  PUT  request as the response verbatim.   {\n     type :  password ,\n     configuration :{ }\n}", 
            "title": "Password"
        }, 
        {
            "location": "/dtgateway_api/#kerberos-with-no-group-mapping", 
            "text": "Function: Configure Kerberos authentication with no group mapping\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nTwo of the properties are mandatory, they are \"kerberosPrincipal\"   \"kerberosKeytab\". A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should be \nspecified as \"false\".  {\n     type : kerberos ,\n        configuration :{  \n          groupSupport : false ,\n          kerberosPrincipal : {kerberosPrincipal} ,\n          kerberosKeytab : {Keytab} ,\n          tokenValidity : {tokenValidity} ,\n          cookieDomain : {cookieDomain} ,\n          cookiePath : {cookiePath} ,      \n          signatureSecret : {signatureSecret} \n         }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Kerberos with no group mapping"
        }, 
        {
            "location": "/dtgateway_api/#kerberos-with-group-mapping", 
            "text": "Function: Configure Kerberos authentication with group mapping \nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nTwo of these properties are mandatory, they are \"kerberosPrincipal\"   \"kerberosKeytab\". A \"groupSupport\" property \nspecifies whether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should \nbe specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that \ncontains the mapping from kerberos groups to roles.  \n{  \n    type :  kerberos ,\n    configuration : {  \n         groupSupport :  true ,\n         kerberosPrincipal :  {kerberosPrincipal} ,\n         kerberosKeytab :  {Keytab} ,\n         tokenValidity :  {Validity} ,\n         cookieDomain  :  {cookieDomain} ,\n         cookiePath :  {cookiePath} \n         signatureSecret :  {signatureSecret} \n        }\n     groupMapping : [\n        { \n              group :  users ,\n              roles : [ developers ,  admins ,  qa ] \n        },\n        {  \n              group :  ops ,\n              roles : [ operators ]\n        }\n     ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Kerberos with group mapping"
        }, 
        {
            "location": "/dtgateway_api/#ldap", 
            "text": "Configure LDAP authentication. There are different configurations possible based on how the LDAP server is configured.", 
            "title": "LDAP"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-allowed-and-no-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication with anonymous search available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows LDAP groups\nto be mapped to roles. It should be specified as \"false\".  \n{   type :  ldap ,\n    configuration : {  \n        groupSupport :  false ,\n        Server :  {Server} ,\n        Port : {port}  +\n        userBaseDn :  {usserBaseDn} ,\n        userIdAttribute :  {userIdAttribute} \n   }\n}  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-and-no-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication when anonymous search is not available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".  \n{  \n     type :  ldap ,\n     configuration : {  \n         groupSupport :  false ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} , \n         userObjectClass :  {userObjectClass} \n    }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-but-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication when anonymous search is not available on LDAP server but group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from LDAP groups to roles.  { \n     type :  ldap ,\n     configuration : {  \n         groupSupport :  true ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} , \n         roleBaseDn :  {roleBaseDn} ,\n         userRdnAttribute : {userRdnAttribute} , \n         roleNameAttribute :  {roleNameAttribute} , \n         roleObjectClass :  {roleObjectClass} , \n         userObjectClass :  {userObjectClass} \n    },\n     groupMapping : [ \n        { \n             group :  users ,\n             roles :[ developers ] \n        },\n        { \n             group :  ops ,\n             roles : [ operators ]\n        }\n    ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed but group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#active-directory", 
            "text": "Configure Active Directory authentication. There are different configurations possible based on how the Active Directory \nserver is configured.", 
            "title": "Active Directory"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-allowed-and-no-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication with anonymous search available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows \nActive Directory groups to be mapped to roles. It should be specified as \"false\".  \n{  \n    type :  ad ,\n    configuration : {  \n        groupSupport  :  false ,\n        Server :  {server} ,\n        Port : {port},\n        userSearchFilter :  {userSearchFilter} ,\n        userBaseDn :  {userBaseDn} ,\n        userIdAttribute :  {userIdAttribute} ,\n        userDomain  :  {userDomain}     \n   }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-and-no-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication when anonymous search is not available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".  \n{  \n    type :  ad ,\n    configuration : {  \n    groupSupport :  false ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} ,\n         userObjectClass :  {userObjectClass} \n    }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-but-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication when anonymous search is not available on Active Directory server but group mapping is needed\nThe configuration comprises different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from Active Directory groups to roles.  { \n     type :  ad ,\n     configuration : {  \n         groupSupport :  true ,\n         Server :  {Server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} ,\n         roleBaseDn :  {roleBaseDn} ,\n         userRdnAttribute :  {userRdnAttribute} ,\n         roleNameAttribute :  roleNameAttribute ,\n         roleObjectClass :  roleObjectClass ,\n         userObjectClass :  {userObjectClass} ,\n     },\n      groupMapping : [ ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed but group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#pam", 
            "text": "Configure PAM or Pluggable Authentication Mechanism. PAM is the de-facto authentication available on Linux systems.", 
            "title": "PAM"
        }, 
        {
            "location": "/dtgateway_api/#pam-with-no-group-mapping", 
            "text": "Function: Configure PAM authentication with no group mapping\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nIn PAM, service name is mandatory   there is no configuration. A \"groupSupport\" property specifies whether group mapping \nshould be enabled. Group mapping allows PAM groups to be mapped to roles. It should be specified as \"false\".  \n{  \n    type : pam ,\n    configuration :{  \n       groupSupport : false ,\n       serviceName : {serviceName} \n   }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PAM with no group mapping"
        }, 
        {
            "location": "/dtgateway_api/#pam-with-group-mapping", 
            "text": "Function: Configure PAM authentication with group mapping \nThe configuration comprises of different properties as described in the  dtgateway_security  document.\nIn PAM, service name is mandatory   there is no configuration. Group mapping allows PAM groups to be mapped to roles. It \nshould be specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be \nspecified that contains the mapping from PAM groups to roles.  \n{  \n    type : pam ,\n    configuration :{  \n       groupSupport : true ,\n       serviceName : {serviceName} \n   },\n\n    groupMapping : [ \n       { \n           group :  users ,\n           roles :[ developers ] \n       },\n       { \n           group :  ops ,\n           roles : [ operators ]\n       }\n   ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PAM with group mapping"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configgroupmapping", 
            "text": "Function: Specify group to role mapping\nSet or update mapping from groups of configured authentication mechanism to RTS roles.  \n{\n    groupMapping  : [\n        {\n            group  :  users ,\n            role  : [ developers ,  admins ,  qa ,  interns ]\n        },\n        {\n            group :  ops ,\n            role  : [ operators ]\n        }\n   ]\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PUT /ws/v2/config/groupMapping"
        }, 
        {
            "location": "/dtgateway_api/#publisher-subscriber-websocket-protocol", 
            "text": "dtGateway provides a light-weight pubsub websocket service.\nThe URL of dtGateway's pubsub websocket service is:  ws://{dtGateway-host-port}/pubsub .\nFor example:  ws://localhost:9090/pubsub", 
            "title": "Publisher-Subscriber WebSocket Protocol"
        }, 
        {
            "location": "/dtgateway_api/#input", 
            "text": "", 
            "title": "Input"
        }, 
        {
            "location": "/dtgateway_api/#publishing", 
            "text": "{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Publishing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing", 
            "text": "{\"type\":\"subscribe\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing"
        }, 
        {
            "location": "/dtgateway_api/#unsubscribing", 
            "text": "{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}", 
            "title": "Unsubscribing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing-to-the-number-of-subscribers-of-a-topic", 
            "text": "{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing to the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#unsubscribing-from-the-number-of-subscribers-of-a-topic", 
            "text": "{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Unsubscribing from the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#output", 
            "text": "", 
            "title": "Output"
        }, 
        {
            "location": "/dtgateway_api/#normal-published-data", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Normal Published Data"
        }, 
        {
            "location": "/dtgateway_api/#number-of-subscribers", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}", 
            "title": "Number of Subscribers:"
        }, 
        {
            "location": "/dtgateway_api/#auto-publish-topics", 
            "text": "data that gets published every one second:   applications  - list of streaming applications running in the cluster  applications.[appid]  - information about a particular application  applications.[appid].containers  - information about containers of a particular application  applications.[appid].physicalOperators  - information about operators of a particular application  applications.[appid].logicalOperators  - information about logical operators of a particular application  applications.[appid].events  - events from the AM of a particularapplication   data that gets published every five seconds:   cluster.metrics  - metrics of the cluster", 
            "title": "Auto publish topics"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/", 
            "text": "How to import and launch an app-template\n\n\nThis document has step-by-step guide on how to import and launch app-template.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the verticals for different use-cases is displayed. Navigating to the desired vertical shows available suites of applications for that vertical.\n\n\n\n\n\n\nNavigating to the desired suite shows available applications in the suite.\n\n\n\n\n\n\n\n\nClick on \nimport\n link for the desired application.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Applications in this application package will be displayed in the table. Click on the \nlaunch\n button for the desired application.\n\n\n\n\n\n\nLaunch dialogue for the application is displayed.  \n\n    Fill in the required properties for the application. For more details, please refer to properties section in the documentation for the respective app.\n\n\n\n\n\n\nAdvanced properties for the application have preset default values. Thus, changing these values is optional. But, if required you can change these properties for fine tuning the application.\n\n   Under optional properties section, click on down arrow button and then click on \nadd default properties\n.\n\n\n\n\n\n\n\nClick on the \nLaunch\n button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nphysical\n tab shows the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nTo look at the details of specific container and to monitor logs, stout, stderr etc. container details page can be reached from the container link in the physical tab.", 
            "title": "Import and Launch App-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/#how-to-import-and-launch-an-app-template", 
            "text": "This document has step-by-step guide on how to import and launch app-template.", 
            "title": "How to import and launch an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n       Page listing the verticals for different use-cases is displayed. Navigating to the desired vertical shows available suites of applications for that vertical.    Navigating to the desired suite shows available applications in the suite.     Click on  import  link for the desired application.    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Applications in this application package will be displayed in the table. Click on the  launch  button for the desired application.    Launch dialogue for the application is displayed.   \n    Fill in the required properties for the application. For more details, please refer to properties section in the documentation for the respective app.    Advanced properties for the application have preset default values. Thus, changing these values is optional. But, if required you can change these properties for fine tuning the application. \n   Under optional properties section, click on down arrow button and then click on  add default properties .    Click on the  Launch  button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       physical  tab shows the status of physical instances of the operator, containers etc.\n       To look at the details of specific container and to monitor logs, stout, stderr etc. container details page can be reached from the container link in the physical tab.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/", 
            "text": "How to customize an app-template\n\n\nThis document describes how to customize an app-template.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0.5 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.6.0)+\n\n\n\n\n\n\n\n\nUse following command to clone the repository:\n\n\ngit clone git@github.com:DataTorrent/moodI.git\n\n\n\n\n\n\nChange directory to containing app-template you wish to customize:\n\n\ncd moodI/app-templates/kafka-to-hdfs-filter-transform/\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.\n\n\n\n\n\n\nNote on customization\n\n\n\n\n\n\nApplication.java\n defines the Application class which defines the application pipeline. Application is represented as directed acyclic graph (DAG). Vertices of the graph represents operators or computational units. Edges represents data flow or streams.\n\n\n\n\n\n\nApplication\n class implements \nStreamingApplication\n interface by defining implementation for \npopulateDAG\n method .\n\n\n\n\n\n\npopulateDAG\n methods receives an argument with an instance of DAG object. App developers can add operators, streams to this dag object to define the pipeline based on the need.\n\n\n\n\n\n\nSecond argument to \npopulateDAG\n method is an instance of Configuration object.\nAll the properties specified in \nproperties.xml\n will be available through this configuration object.\n\n\n\n\n\n\nMost of the commonly used functionality, connectors are available in \nmoodI\n, \nmalhar\n operator library. If required functionality is not available in the operator library; one can implement own operators for custom computations.\n\n\n\n\n\n\nAdd the operators to the DAG using \ndag.addOperator()\n API and connect the operators with upstream, downstream operators using \ndag.addStream()\n API.\n\n\nFor example, suppose one needs to modify \nkafka-to-hdfs-filter-transform\n such that count of tuples discarded by the filter operator should be displayed on the console. This can be achived by adding following code to \npopulateDAG\n method in \nApplication.java\n:\n\n\n\n\n\n\n\nCounter counter = dag.addOperator(\ncounter\n, Counter.class);\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, ConsoleOutputOperator.class);\ndag.addStream(\nFilteredOut\n, filterOperator.falsePort, counter.input);\ndag.addStream(\nFilteredOutToConsole\n, counter.output, console.input);\n\n\n\n\nFollow the \ncustomization steps\n for applying your changes.", 
            "title": "Customizing an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#how-to-customize-an-app-template", 
            "text": "This document describes how to customize an app-template.", 
            "title": "How to customize an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0.5 +  git  : 1.7 +  Hadoop  (Apache-2.6.0)+     Use following command to clone the repository:  git clone git@github.com:DataTorrent/moodI.git    Change directory to containing app-template you wish to customize:  cd moodI/app-templates/kafka-to-hdfs-filter-transform/    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#note-on-customization", 
            "text": "Application.java  defines the Application class which defines the application pipeline. Application is represented as directed acyclic graph (DAG). Vertices of the graph represents operators or computational units. Edges represents data flow or streams.    Application  class implements  StreamingApplication  interface by defining implementation for  populateDAG  method .    populateDAG  methods receives an argument with an instance of DAG object. App developers can add operators, streams to this dag object to define the pipeline based on the need.    Second argument to  populateDAG  method is an instance of Configuration object.\nAll the properties specified in  properties.xml  will be available through this configuration object.    Most of the commonly used functionality, connectors are available in  moodI ,  malhar  operator library. If required functionality is not available in the operator library; one can implement own operators for custom computations.    Add the operators to the DAG using  dag.addOperator()  API and connect the operators with upstream, downstream operators using  dag.addStream()  API.  For example, suppose one needs to modify  kafka-to-hdfs-filter-transform  such that count of tuples discarded by the filter operator should be displayed on the console. This can be achived by adding following code to  populateDAG  method in  Application.java :    \nCounter counter = dag.addOperator( counter , Counter.class);\nConsoleOutputOperator console = dag.addOperator( console , ConsoleOutputOperator.class);\ndag.addStream( FilteredOut , filterOperator.falsePort, counter.input);\ndag.addStream( FilteredOutToConsole , counter.output, console.input);  Follow the  customization steps  for applying your changes.", 
            "title": "Note on customization"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/", 
            "text": "Database to Database Sync Application\n\n\nSummary\n\n\nThis application demonstrates continuous archival of big data from database tables.\nApplication connects to the source database table over JDBC connection and continuously polls for new data. This database rows from source table are archieved in the destination database in scalable, fault-tolerent fashion. Application also allows to do custom transformations on the rows before they are copied to the destination.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nJdbc Input Database Url\n\n\nString\n\n\njdbc:postgresql://node1 .corp1.com:5432/testdb\n\n\nConnection URL for the source database.\n\n\n\n\n\n\nJdbc Input Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for source database\n\n\n\n\n\n\nJdbc Input Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for source database\n\n\n\n\n\n\nJdbc Input Table Name\n\n\nString\n\n\ntest_event_input_table\n\n\nSource table name\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:postgresql://dest1 .corp1.com:5432/testdb\n\n\nConnection URL for the destination database.\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for destination database\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for destination database\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\ntest_event_output_table\n\n\nDestination table name\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBatch Size For Jdbc Input\n\n\n300\n\n\nint\n\n\n1000\n\n\nNo of rows to fetch in single query for input.\n\n\n\n\n\n\nColumns Expressions\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nString\n\n\ncolumn1_name, column2_name, column3_name\n\n\nComma separated list of columns to select from the source table.\n\n\n\n\n\n\nJdbc Input Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\norg.postgresql .Driver\n\n\nFQCN for jdbc driver class for source database\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\norg.postgresql .Driver\n\n\nFQCN for jdbc driver class for destination database\n\n\n\n\n\n\nNumber Of Partition Required\n\n\n4\n\n\nint\n\n\n2\n\n\nNumber of partitions for reading data from JDBC source database. There will one additional partition used for polling the databse.\n\n\n\n\n\n\nTuple Class Name For Jdbc Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input database row\n\n\n\n\n\n\nTuple Class Name For Jdbc Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input database row.\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nUnique Key For Range Queries\n\n\nACCOUNT_NO\n\n\nString\n\n\nid\n\n\nColumn to be used for partitioning.\n\n\n\n\n\n\nWhere Condition\n\n\n\n\nString\n\n\n\n\nWhere condition used for filtering rows from source table. Empty string indicates select all rows.\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\nApplication is configured for pre-defined schema mentioned \nhere\n. For using this application for custom schema objects; changes will be needed in \nApplication.java\n. One can define field info mapping for the input fields in \naddInputFieldInfos()\n. Simillarly,\nfield info mapping for the output fields can be defined in  \naddOutputFieldInfos()\n.\n\n\n\n\n\n\nPojoEvent\n is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath. \nConfiguration package\n can be used for this purpose.\n\n\n\n\n\n\nThis application has been tested with PostgreSQL database. But, this can be used on other databases like MySQL, Oracle etc provided that JDBC client jar for the same is available in the classpath. Please specify \nJdbc Input Database Driver\n, \nJdbc Output Database Driver\n properties to point to respective driver class for the database you are connecting to.", 
            "title": "Database-to-database-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#database-to-database-sync-application", 
            "text": "", 
            "title": "Database to Database Sync Application"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#summary", 
            "text": "This application demonstrates continuous archival of big data from database tables.\nApplication connects to the source database table over JDBC connection and continuously polls for new data. This database rows from source table are archieved in the destination database in scalable, fault-tolerent fashion. Application also allows to do custom transformations on the rows before they are copied to the destination.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Jdbc Input Database Url  String  jdbc:postgresql://node1 .corp1.com:5432/testdb  Connection URL for the source database.    Jdbc Input Store Password  String  postgres  Password for source database    Jdbc Input Store Username  String  postgres  Username for source database    Jdbc Input Table Name  String  test_event_input_table  Source table name    Jdbc Output Database Url  String  jdbc:postgresql://dest1 .corp1.com:5432/testdb  Connection URL for the destination database.    Jdbc Output Store Password  String  postgres  Password for destination database    Jdbc Output Store Username  String  postgres  Username for destination database    Jdbc Output Table Name  String  test_event_output_table  Destination table name", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Batch Size For Jdbc Input  300  int  1000  No of rows to fetch in single query for input.    Columns Expressions  ACCOUNT_NO, NAME, AMOUNT  String  column1_name, column2_name, column3_name  Comma separated list of columns to select from the source table.    Jdbc Input Database Driver  org.postgresql .Driver  String  org.postgresql .Driver  FQCN for jdbc driver class for source database    Jdbc Output Database Driver  org.postgresql .Driver  String  org.postgresql .Driver  FQCN for jdbc driver class for destination database    Number Of Partition Required  4  int  2  Number of partitions for reading data from JDBC source database. There will one additional partition used for polling the databse.    Tuple Class Name For Jdbc Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input database row    Tuple Class Name For Jdbc Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input database row.    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing output for transform operator (if included in the DAG. Not included by default).    Unique Key For Range Queries  ACCOUNT_NO  String  id  Column to be used for partitioning.    Where Condition   String   Where condition used for filtering rows from source table. Empty string indicates select all rows.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#notes", 
            "text": "Application is configured for pre-defined schema mentioned  here . For using this application for custom schema objects; changes will be needed in  Application.java . One can define field info mapping for the input fields in  addInputFieldInfos() . Simillarly,\nfield info mapping for the output fields can be defined in   addOutputFieldInfos() .    PojoEvent  is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath.  Configuration package  can be used for this purpose.    This application has been tested with PostgreSQL database. But, this can be used on other databases like MySQL, Oracle etc provided that JDBC client jar for the same is available in the classpath. Please specify  Jdbc Input Database Driver ,  Jdbc Output Database Driver  properties to point to respective driver class for the database you are connecting to.", 
            "title": "Notes"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/", 
            "text": "HDFS line copy Application\n\n\nSummary\n\n\nThis application demonstrates data preparation pipeline which reads lines from files on source HDFS. It performs customized filtering, transformations on the line and writes them into destination HDFS.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\nOutput File Name\n\n\nString\n\n\noutput.txt\n\n\nName of the output file. This name will be appended with suffix for each part.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{      \"separator\": \"\n\",      \"quoteChar\": \"\\\"\",      \"lineDelimiter\":\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }\n\n\nString\n\n\nJSON string defining schema for CSV formatter\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{      \"separator\": \"\n\",      \"quoteChar\": \"\\\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }\n\n\nString\n\n\nJSON string defining schema for CSV parser\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.\n\n\n\n\n\n\nNumber Of Readers For Partitioning\n\n\n2\n\n\nint\n\n\nBlocks reader operator would be partioned into these many partitions.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be emitted by CSV Parser\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be consumed by CSV formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be consumed by Transform operator\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be emitted by Transform operator\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\nApplication is pre-configured for pre-defined schema mentioned \nhere\n. For using this application for custom schema objects; changes will be needed in the configuration.\n\n\n\n\n\n\nPojoEvent\n is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath. \nConfiguration package\n can be used for this purpose.", 
            "title": "HDFS-line-copy"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#hdfs-line-copy-application", 
            "text": "", 
            "title": "HDFS line copy Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#summary", 
            "text": "This application demonstrates data preparation pipeline which reads lines from files on source HDFS. It performs customized filtering, transformations on the line and writes them into destination HDFS.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.    Output File Name  String  output.txt  Name of the output file. This name will be appended with suffix for each part.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Csv Formatter Schema  {      \"separator\": \" \",      \"quoteChar\": \"\\\"\",      \"lineDelimiter\":\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }  String  JSON string defining schema for CSV formatter    Csv Parser Schema  {      \"separator\": \" \",      \"quoteChar\": \"\\\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }  String  JSON string defining schema for CSV parser    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.    Number Of Readers For Partitioning  2  int  Blocks reader operator would be partioned into these many partitions.    Tuple Class Name For Csv Parser Output  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be emitted by CSV Parser    Tuple Class Name For Formatter Input  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be consumed by CSV formatter    Tuple Class Name For Transform Input  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be consumed by Transform operator    Tuple Class Name For Transform Output  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be emitted by Transform operator", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#notes", 
            "text": "Application is pre-configured for pre-defined schema mentioned  here . For using this application for custom schema objects; changes will be needed in the configuration.    PojoEvent  is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath.  Configuration package  can be used for this purpose.", 
            "title": "Notes"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/", 
            "text": "HDFS part file copy Application\n\n\nSummary\n\n\nThis application demonstrates continuous big data archival from HDFS.\nIt ingests files as blocks for backup on remote HDFS cluster.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nMinimum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.", 
            "title": "HDFS-part-file-copy"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#hdfs-part-file-copy-application", 
            "text": "", 
            "title": "HDFS part file copy Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#summary", 
            "text": "This application demonstrates continuous big data archival from HDFS.\nIt ingests files as blocks for backup on remote HDFS cluster.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Maximum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Minimum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/", 
            "text": "HDFS to HDFS filter transform Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data preparation while reading data from a source Hadoop cluster. This data is considered to be in delimited format, which is further filtered, transformed based on configurable properties. Finally, this prepared data is written back in a desired format to the destination Hadoop cluster. This could be easily utilized and extended by any developer to create a fast, fault tolerant and scalable Big Data Application to serve business with rich data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nMinimum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.", 
            "title": "HDFS-to-HDFS-filter-transform"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#hdfs-to-hdfs-filter-transform-application", 
            "text": "", 
            "title": "HDFS to HDFS filter transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#summary", 
            "text": "This application template demonstrates continuous big data preparation while reading data from a source Hadoop cluster. This data is considered to be in delimited format, which is further filtered, transformed based on configurable properties. Finally, this prepared data is written back in a desired format to the destination Hadoop cluster. This could be easily utilized and extended by any developer to create a fast, fault tolerant and scalable Big Data Application to serve business with rich data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Maximum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Minimum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/", 
            "text": "Kafka to Cassandra Filter Transform Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data preparation while reading data messages from a configured Kafka topic. This data is considered to be in JSON format, which is further filtered, transformed based on configurable properties. Finally, this prepared message is written to a Cassandra  .\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nCassandra Node Name\n\n\nString\n\n\nlocalhost\n\n\nHost name for cassandra server\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKey Space On Cassandra\n\n\nString\n\n\ntestdb\n\n\nKeyspace to be used for output\n\n\n\n\n\n\nTable Name\n\n\nString\n\n\ntest_input\n\n\nQuasi java expression\n\n\n\n\n\n\nJson Parser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.", 
            "title": "Kafka-to-Cassandra-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#kafka-to-cassandra-filter-transform-application", 
            "text": "", 
            "title": "Kafka to Cassandra Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#summary", 
            "text": "This application template demonstrates continuous big data preparation while reading data messages from a configured Kafka topic. This data is considered to be in JSON format, which is further filtered, transformed based on configurable properties. Finally, this prepared message is written to a Cassandra  .", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Cassandra Node Name  String  localhost  Host name for cassandra server    Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Key Space On Cassandra  String  testdb  Keyspace to be used for output    Table Name  String  test_input  Quasi java expression    Json Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/", 
            "text": "Kafka to Database Sync Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kafka topic. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nCsv Parser Schema\n\n\nString\n\n\n{  \"separator\": \"\n\",  \"quoteChar\": \"\\\"\",   \"fields\": [{\"name\": \"accountNumber\",\"type\": \"Integer\"} ,{\"name\": \"name\",\"type\": \"String\"},{\"name\": \"amount\",\"type\": \"Integer\"}]}\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:postgresql://dest1 .corp1.com:5432/testdb\n\n\nConnection URL for the destination database.\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for destination database\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for destination database\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\ntest_event_output_table\n\n\nDestination table name\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\n\n\nFQCN for jdbc driver class for destination database\n\n\n\n\n\n\nTransform Expression Info\n\n\n{\"accountNumber\":\"{$.accountNumber}\", \"name\":\"{$.name}.toUpperCase()\", \"amount\":\"{$.amount}\"}\n\n\nString\n\n\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\n{\"accountNumber\":\"INTEGER\", \"name\":\"STRING\", \"amount\":\"INTEGER\"}\n\n\nString\n\n\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.", 
            "title": "Kafka-to-Database-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#kafka-to-database-sync-application", 
            "text": "", 
            "title": "Kafka to Database Sync Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kafka topic. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Csv Parser Schema  String  {  \"separator\": \" \",  \"quoteChar\": \"\\\"\",   \"fields\": [{\"name\": \"accountNumber\",\"type\": \"Integer\"} ,{\"name\": \"name\",\"type\": \"String\"},{\"name\": \"amount\",\"type\": \"Integer\"}]}  JSON representing schema to be used by CSV parser    Jdbc Output Database Url  String  jdbc:postgresql://dest1 .corp1.com:5432/testdb  Connection URL for the destination database.    Jdbc Output Store Password  String  postgres  Password for destination database    Jdbc Output Store Username  String  postgres  Username for destination database    Jdbc Output Table Name  String  test_event_output_table  Destination table name    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.    Jdbc Output Database Driver  org.postgresql .Driver  String   FQCN for jdbc driver class for destination database    Transform Expression Info  {\"accountNumber\":\"{$.accountNumber}\", \"name\":\"{$.name}.toUpperCase()\", \"amount\":\"{$.amount}\"}  String   JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  {\"accountNumber\":\"INTEGER\", \"name\":\"STRING\", \"amount\":\"INTEGER\"}  String   JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/", 
            "text": "Kafka to HDFS Filter Transform Application\n\n\nSummary\n\n\nThis application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/dtuser/output/dir1\nhdfs://node1.corp1.com/user/dtuser/output\n\n\nHDFS path (absolute or relative)\n\n\n\n\n\n\nParser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.", 
            "title": "Kafka-to-HDFS-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#kafka-to-hdfs-filter-transform-application", 
            "text": "", 
            "title": "Kafka to HDFS Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#summary", 
            "text": "This application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa    Output Directory Path  String  /user/dtuser/output/dir1 hdfs://node1.corp1.com/user/dtuser/output  HDFS path (absolute or relative)    Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/", 
            "text": "Kafka to Kafka Filter Transform Application\n\n\nSummary\n\n\nThis application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKafka Output Topic Name\n\n\nString\n\n\noutput_transactions\n\n\nOutput topic name on Kakfa\n\n\n\n\n\n\nJson Parser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Input Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic name on Kakfa\n\n\n\n\n\n\nKafka Producer Properties\n\n\nString\n\n\nserializer.class =kafka.serializer. StringEncoder ,producer.type=async ,metadata.broker.list=localhost:9092\n\n\nProducer properties for Kafka output\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.\n\n\n\n\n\n\nNumber Of Partitions For Kafka Consumer\n\n\ncom.datatorrent.common .partitioner.StatelessPartitioner:1\n\n\nString\n\n\ncom.datatorrent.common .partitioner.StatelessPartitioner:16\n\n\nParallel instances for Kafka operator", 
            "title": "Kafka-to-Kafka-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#kafka-to-kafka-filter-transform-application", 
            "text": "", 
            "title": "Kafka to Kafka Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#summary", 
            "text": "This application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Kafka Output Topic Name  String  output_transactions  Output topic name on Kakfa    Json Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Input Topic Name  String  transactions  Topic name on Kakfa    Kafka Producer Properties  String  serializer.class =kafka.serializer. StringEncoder ,producer.type=async ,metadata.broker.list=localhost:9092  Producer properties for Kafka output    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.    Number Of Partitions For Kafka Consumer  com.datatorrent.common .partitioner.StatelessPartitioner:1  String  com.datatorrent.common .partitioner.StatelessPartitioner:16  Parallel instances for Kafka operator", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/", 
            "text": "Kinesis to Redshift application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nKinesis Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nKinesis Endpoint\n\n\nString\n\n\nkinesis.us-east-1.amazonaws.com\n\n\nAWS service end-point for Kinesis\n\n\n\n\n\n\nKinesis Secret Key\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret Key for accessing Kinesis.\n\n\n\n\n\n\nKinesis Stream Name\n\n\nString\n\n\nevents\n\n\nStream name to read data from Kinesis\n\n\n\n\n\n\nRedshift Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for redshift\n\n\n\n\n\n\nRedshift Secret Key\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret Key for accessing Redshift.\n\n\n\n\n\n\nRegion Of Input File\n\n\nString\n\n\nap-southeast-1\n\n\nRegion for the intermediate S3 storage.\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\nString\n\n\ncom.amazon.redshift .jdbc4.Driver\n\n\nFQCN for Redshift JDBC output database driver.\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev\n\n\nConnection URL for redshift\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npassword\n\n\nPassword for redshift store instance.\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\nusername\n\n\nUsername for redshift store instance.\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\nevents\n\n\ntable to be used from redshift store instance.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nEmr Cluster Id\n\n\nEMR-CLUSTERID\n\n\nString\n\n\n\n\nCluster id for EMR.\n\n\n\n\n\n\nRedshift Delimiter\n\n\n\n\nString\n\n\n\n\nDelimiter to be used for redshift output\n\n\n\n\n\n\nRedshift Reader Mode\n\n\nREAD_FROM_S3\n\n\nString\n\n\nREAD_FROM_S3\nREAD_FROM_EMR\n\n\nChoose preferred intermediate store. S3 or HDFS.\n\n\n\n\n\n\nS3Bucket Name\n\n\nS3_BUCKET_NAME\n\n\nString\n\n\n\n\nBucket name for intermediate storage\n\n\n\n\n\n\nS3Directory Name\n\n\nS3_DIRECTORY_NAME\n\n\nString\n\n\n\n\nDirectory name for intermediate storage\n\n\n\n\n\n\nBatch Size For Jdbc Output\n\n\n500\n\n\nString\n\n\n\n\nNumber rows to push into redshift in single query\n\n\n\n\n\n\nMax Length Of Rolling File\n\n\n1048576 (1 MB)\n\n\nString\n\n\n\n\nMaximums size in bytes for files on intermediate storage.\n\n\n\n\n\n\nTuple Class Name For Jdbc Output\n\n\ncom.datatorrent .apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent .apps.PojoEvent\n\n\nFQCN for tuples written on redshift", 
            "title": "Kinesis-to-Redshift"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#kinesis-to-redshift-application", 
            "text": "", 
            "title": "Kinesis to Redshift application"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Kinesis Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Kinesis Endpoint  String  kinesis.us-east-1.amazonaws.com  AWS service end-point for Kinesis    Kinesis Secret Key  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret Key for accessing Kinesis.    Kinesis Stream Name  String  events  Stream name to read data from Kinesis    Redshift Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for redshift    Redshift Secret Key  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret Key for accessing Redshift.    Region Of Input File  String  ap-southeast-1  Region for the intermediate S3 storage.    Jdbc Output Database Driver  String  com.amazon.redshift .jdbc4.Driver  FQCN for Redshift JDBC output database driver.    Jdbc Output Database Url  String  jdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev  Connection URL for redshift    Jdbc Output Store Password  String  password  Password for redshift store instance.    Jdbc Output Store Username  String  username  Username for redshift store instance.    Jdbc Output Table Name  String  events  table to be used from redshift store instance.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Emr Cluster Id  EMR-CLUSTERID  String   Cluster id for EMR.    Redshift Delimiter   String   Delimiter to be used for redshift output    Redshift Reader Mode  READ_FROM_S3  String  READ_FROM_S3 READ_FROM_EMR  Choose preferred intermediate store. S3 or HDFS.    S3Bucket Name  S3_BUCKET_NAME  String   Bucket name for intermediate storage    S3Directory Name  S3_DIRECTORY_NAME  String   Directory name for intermediate storage    Batch Size For Jdbc Output  500  String   Number rows to push into redshift in single query    Max Length Of Rolling File  1048576 (1 MB)  String   Maximums size in bytes for files on intermediate storage.    Tuple Class Name For Jdbc Output  com.datatorrent .apps.PojoEvent  String  com.datatorrent .apps.PojoEvent  FQCN for tuples written on redshift", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/", 
            "text": "Kinesis to S3 application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 output\n\n\n\n\n\n\nS3Output Directory Path\n\n\nString\n\n\nkinesis_to_s3\n\n\nOutput directory for AWS S3\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3 output\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nString\n\n\nkinesis.us-east-1.amazonaws.com\n\n\nAWS service end-point for Kinesis\n\n\n\n\n\n\nKinesis Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 output.\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing Kinesis.\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\nString\n\n\nevents\n\n\nStream name to read data from Kinesis\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSize Of S3File In Bytes\n\n\n134217728 (128 MB)\n\n\nlong\n\n\nOutput files on S3 will be rolled over to new files after this size is reached.\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{\"separator\":\"\n\" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{\"separator\":\"\n\",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\nJSON representing schema for objects given to CSV formatter.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input row\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input row for formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Kinesis-to-S3"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#kinesis-to-s3-application", 
            "text": "", 
            "title": "Kinesis to S3 application"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      S3Output Bucket Name  String  com.example.app.s3  Bucket name for AWS S3 output    S3Output Directory Path  String  kinesis_to_s3  Output directory for AWS S3    Access Key For Kinesis Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Access Key For S3Output  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3 output    End Point For Kinesis Input  String  kinesis.us-east-1.amazonaws.com  AWS service end-point for Kinesis    Kinesis Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Secret Access Key For S3Output  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 output.    Secret Key For Kinesis Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing Kinesis.    Stream Name For Kinesis Input  String  events  Stream name to read data from Kinesis", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Size Of S3File In Bytes  134217728 (128 MB)  long  Output files on S3 will be rolled over to new files after this size is reached.    Csv Parser Schema  {\"separator\":\" \" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}  String  JSON representing schema to be used by CSV parser    Csv Formatter Schema  {\"separator\":\" \",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}  String  JSON representing schema for objects given to CSV formatter.    Tuple Class Name For Csv Parser Output  com.datatorrent. apps.PojoEvent  String  POJO class representing input row    Tuple Class Name For Formatter Input  com.datatorrent. apps.PojoEvent  String  POJO class representing input row for formatter    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  POJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/", 
            "text": "S3 to HDFS application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading blocks of data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAccess Key For S3Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3\n\n\n\n\n\n\nBucket Name For S3Input\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 input\n\n\n\n\n\n\nInput Directory Or File Path On S3\n\n\nString\n\n\n/path/to/input\n\n\nDirectory path within S3 bucket\n\n\n\n\n\n\nOutput Directory Path On HDFS\n\n\nString\n\n\n/user/dtuser /output/dir1\nhdfs://node1.corp1.com /user/dtuser/output\n\n\nHDFS path (absolute or relative)\n\n\n\n\n\n\nSecret Key For S3Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 input.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n4\n\n\nint\n\n\nMaximum allowed partitions for Block Reader. Allocating more partitions is for scaling the application by allocating more resources to handle larger dataset\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n16\n\n\nint\n\n\nLimit to control number of blocks emitted to downstream operators.", 
            "title": "S3-to-HDFS-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#s3-to-hdfs-application", 
            "text": "", 
            "title": "S3 to HDFS application"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading blocks of data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Access Key For S3Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3    Bucket Name For S3Input  String  com.example.app.s3  Bucket name for AWS S3 input    Input Directory Or File Path On S3  String  /path/to/input  Directory path within S3 bucket    Output Directory Path On HDFS  String  /user/dtuser /output/dir1 hdfs://node1.corp1.com /user/dtuser/output  HDFS path (absolute or relative)    Secret Key For S3Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 input.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Maximum Readers For Dynamic Partitioning  4  int  Maximum allowed partitions for Block Reader. Allocating more partitions is for scaling the application by allocating more resources to handle larger dataset    Number Of Blocks Per Window  16  int  Limit to control number of blocks emitted to downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/", 
            "text": "S3 to Redshift application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while scanning data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync, Preparation or Retention Application to serve business with rich continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAccess Key For S3Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3\n\n\n\n\n\n\nBucket Name For S3Input\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 input\n\n\n\n\n\n\nFiles For Scanning\n\n\nString\n\n\n/path/to/input\n\n\nDirectory path within S3 bucket\n\n\n\n\n\n\nJdbc Driver Of Redshift\n\n\nString\n\n\ncom.amazon.redshift .jdbc4.Driver\n\n\nFQCN for Redshift JDBC output database driver.\n\n\n\n\n\n\nJdbc Url Of Redshift\n\n\nString\n\n\njdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev\n\n\nConnection URL for redshift\n\n\n\n\n\n\nRegion For S3Or EMR\n\n\nString\n\n\nap-southeast-1\n\n\nRegion for S3 used for intermediate storage\n\n\n\n\n\n\nSecret Key For S3Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 input.\n\n\n\n\n\n\nTable Name For Redshift\n\n\nString\n\n\nevents\n\n\ntable to be used from redshift store instance.\n\n\n\n\n\n\nUser Name To Connect Redshift\n\n\nString\n\n\nusername\n\n\nUsername for redshift store instance.\n\n\n\n\n\n\nUser Password To Connect Redshift\n\n\nString\n\n\npassword\n\n\nPassword for redshift store instance.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nRedshift Delimiter\n\n\n\n\nString\n\n\n,\n\n\nDelimiter to be used for redshift output\n\n\n\n\n\n\nRedshift Reader Mode\n\n\nREAD_FROM_S3\n\n\nString\n\n\nREAD_FROM_S3\nREAD_FROM_EMR\n\n\nChoose preferred intermediate store. S3 or HDFS.\n\n\n\n\n\n\nBlock Reader Size For S3Input\n\n\n1048576 (1 MB)\n\n\nlong\n\n\n\n\nSize of data chunk to be read in single request from S3.\n\n\n\n\n\n\nBucket Name For Redshift In S3\n\n\ndummyBucket\n\n\nString\n\n\n\n\nBucket name for intermediate storage\n\n\n\n\n\n\nCluster Id For Redshift\n\n\nEMR-CLUSTERID\n\n\nString\n\n\n\n\nCluster id for EMR.\n\n\n\n\n\n\nS3Directory Name\n\n\nS3_DIRECTORY_NAME\n\n\nString\n\n\n\n\nDirectory name for intermediate storage\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{\"separator\":\"\n\" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{\"separator\":\"\n\",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\n\n\nJSON representing schema for objects given to CSV formatter.\n\n\n\n\n\n\nMax Length Of Rolling File\n\n\n1048576 (1 MB)\n\n\nString\n\n\n\n\nMaximums size in bytes for files on intermediate storage.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\n\n\nLimit to control number of blocks emitted to downstream operators.\n\n\n\n\n\n\nNumber Of Readers For Partitioning\n\n\n2\n\n\nint\n\n\n\n\nLimit to control number of blocks emitted to downstream operators.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input row\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input row for formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "S3-to-HDFS-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#s3-to-redshift-application", 
            "text": "", 
            "title": "S3 to Redshift application"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while scanning data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync, Preparation or Retention Application to serve business with rich continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Access Key For S3Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3    Bucket Name For S3Input  String  com.example.app.s3  Bucket name for AWS S3 input    Files For Scanning  String  /path/to/input  Directory path within S3 bucket    Jdbc Driver Of Redshift  String  com.amazon.redshift .jdbc4.Driver  FQCN for Redshift JDBC output database driver.    Jdbc Url Of Redshift  String  jdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev  Connection URL for redshift    Region For S3Or EMR  String  ap-southeast-1  Region for S3 used for intermediate storage    Secret Key For S3Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 input.    Table Name For Redshift  String  events  table to be used from redshift store instance.    User Name To Connect Redshift  String  username  Username for redshift store instance.    User Password To Connect Redshift  String  password  Password for redshift store instance.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Redshift Delimiter   String  ,  Delimiter to be used for redshift output    Redshift Reader Mode  READ_FROM_S3  String  READ_FROM_S3 READ_FROM_EMR  Choose preferred intermediate store. S3 or HDFS.    Block Reader Size For S3Input  1048576 (1 MB)  long   Size of data chunk to be read in single request from S3.    Bucket Name For Redshift In S3  dummyBucket  String   Bucket name for intermediate storage    Cluster Id For Redshift  EMR-CLUSTERID  String   Cluster id for EMR.    S3Directory Name  S3_DIRECTORY_NAME  String   Directory name for intermediate storage    Csv Parser Schema  {\"separator\":\" \" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}  String   JSON representing schema to be used by CSV parser    Csv Formatter Schema  {\"separator\":\" \",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}  String   JSON representing schema for objects given to CSV formatter.    Max Length Of Rolling File  1048576 (1 MB)  String   Maximums size in bytes for files on intermediate storage.    Number Of Blocks Per Window  1  int   Limit to control number of blocks emitted to downstream operators.    Number Of Readers For Partitioning  2  int   Limit to control number of blocks emitted to downstream operators.    Tuple Class Name For Csv Parser Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input row    Tuple Class Name For Formatter Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input row for formatter    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/", 
            "text": "Database dump to HDFS Sync application\n\n\nSummary\n\n\nIngest records from a PostgreSQL Database table to hadoop HDFS. This application reads messages from configured PostgreSQL table and writes each record as a comma-separated line in HDFS files.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/database-to-hdfs\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n   \n\n\nClick on import button for \nDatabase dump to HDFS Sync App\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nDatabase-to-HDFS-Sync\n application.\n\n\n\n\n\n\nLaunch Database-to-HDFS-Sync\n dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option to display a list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n    \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n   \n\n\n\nFor example, suppose we wish to process all rows from the table \ntest_event_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \ndatabase-node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n, and we wish to write the output to the HDFS file\n\n/user/appuser/output/output.txt\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.databaseUrl\n\n\njdbc:postgresql://database-node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.password\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.userName\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.tableName\n\n\ntest_event_table\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.whereCondition\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\noutput.txt\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://database-node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.tableName\n\n\nTable name for input records\n\n\nString\n\n\ntest_event_table\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.whereCondition\n\n\nWhere clause condition (if any) for input records. Keep blank to fetch all records.\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.partitionCount\n\n\nNumber of JDBC input partitions for parallel reading.\n\n\n4\n\n\n1\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.batchSize\n\n\nBatch size to read data from JDBC.\n\n\n300\n\n\n300\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.key\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO\n\n\nACCOUNT_NO\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.columnsExpression\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\n\n\n\n\ndt.operator.JdbcPoller.port.outputPort.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.pollInterval\n\n\nPoll interval for scanning new records in milisec\n\n\n1000\n\n\n1000\n\n\n\n\n\n\ndt.operator.formatter.prop.schema\n\n\nSchema for CSV formatter\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter. port.in.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/database-to-hdfs\n:\n\n\ncd examples/tutorials/database-to-hdfs\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Database dump to HDFS App"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#database-dump-to-hdfs-sync-application", 
            "text": "", 
            "title": "Database dump to HDFS Sync application"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#summary", 
            "text": "Ingest records from a PostgreSQL Database table to hadoop HDFS. This application reads messages from configured PostgreSQL table and writes each record as a comma-separated line in HDFS files.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/database-to-hdfs .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n     Click on import button for  Database dump to HDFS Sync App    Notification is displayed on the top right corner after application package is successfully imported.\n       Click on the link in the notification which navigates to the page for this application package.\n     Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Database-to-HDFS-Sync  application.    Launch Database-to-HDFS-Sync  dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option to display a list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n        Select  Specify custom properties  option. Click on  add default properties  button.\n       This expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n     \nFor example, suppose we wish to process all rows from the table  test_event_table  in a\nPostgreSQL database named  testdb  accessible at  database-node.com  port  5432  with credentials\nusername= postgres , password= postgres , and we wish to write the output to the HDFS file /user/appuser/output/output.txt . Properties should be set as follows:     Name  Value      dt.operator.JdbcPoller.prop.store.databaseUrl  jdbc:postgresql://database-node.com:5432/testdb    dt.operator.JdbcPoller.prop.store.password  postgres    dt.operator.JdbcPoller.prop.store.userName  postgres    dt.operator.JdbcPoller.prop.tableName  test_event_table    dt.operator.JdbcPoller.prop.whereCondition     dt.operator.fileOutput.prop.filePath  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  output.txt     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n    \n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.JdbcPoller.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcPoller.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://database-node.com:5432/testdb    dt.operator.JdbcPoller.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcPoller.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcPoller.prop.tableName  Table name for input records  String  test_event_table    dt.operator.JdbcPoller.prop.whereCondition  Where clause condition (if any) for input records. Keep blank to fetch all records.  String     dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.JdbcPoller.prop.partitionCount  Number of JDBC input partitions for parallel reading.  4  1    dt.operator.JdbcPoller.prop.batchSize  Batch size to read data from JDBC.  300  300    dt.operator.JdbcPoller.prop.key  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO  ACCOUNT_NO    dt.operator.JdbcPoller.prop.columnsExpression  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO, NAME, AMOUNT  ACCOUNT_NO, NAME, AMOUNT    dt.operator.JdbcPoller.port.outputPort.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcPoller.prop.pollInterval  Poll interval for scanning new records in milisec  1000  1000    dt.operator.formatter.prop.schema  Schema for CSV formatter  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter. port.in.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/database-to-hdfs :  cd examples/tutorials/database-to-hdfs    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/", 
            "text": "Database to Database Sync application\n\n\nSummary\n\n\nIngest records from a source PostgreSQL Database table to destination PostgreSQL database table. This application reads messages from configured PostgreSQL table and writes each record to output PostgreSQL table.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/database-to-database-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n   \n\n\nClick on import button for \nDatabase to Database Sync App\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nDatabase-to-Database-Sync\n application.\n\n\n\n\n\n\nLaunch Database-to-Database-Sync\n dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option to display a list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n    \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n   \n\n\n\nFor example, suppose we wish to process all rows from the table \ntest_event_input_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \nsource-database-node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n, and we wish to write the output records to other\nPostgreSQL database table \ntest_event_output_table\n accessible at \ndestination-database-node.com\n port \n5432\n with same creditials listed above. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseUrl\n\n\njdbc:postgresql://source-database-node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.password\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.userName\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.tableName\n\n\ntest_event_input_table\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.whereCondition\n\n\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\njdbc:postgresql://destination-database-node:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tablename\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n   \n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\nMeta-data\n table is required for Jdbc Output operator for transactional data and application consistency.\n\n\n\n\n\n\n\n\nTable Name\n\n\nColumn Names\n\n\n\n\n\n\n\n\n\n\ndt_meta\n\n\ndt_app_id (VARCHAR)\ndt_operator_id (INT) \ndt_window (BIGINT)\n\n\n\n\n\n\n\n\nQuery for \nMeta-data\n table creation:\n\n\nCREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://node1.company.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.tableName\n\n\nTable name for input records\n\n\nString\n\n\ntest_event_input_table\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.whereCondition\n\n\nWhere clause condition (if any) for input records. Keep blank to fetch all records.\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://node2.company.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tableName\n\n\nTable name for output records\n\n\nString\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.partitionCount\n\n\nNumber of JDBC input partitions for parallel reading.\n\n\n4\n\n\n1\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.batchSize\n\n\nBatch size to read data from JDBC.\n\n\n300\n\n\n300\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.key\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO\n\n\nACCOUNT_NO\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.columnsExpression\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\n\n\n\n\ndt.operator.JdbcInput.port.outputPort.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.pollInterval\n\n\nPoll interval for scanning new records in milisec\n\n\n1000\n\n\n1000\n\n\n\n\n\n\ndt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/database-to-database-sync\n:\n\n\ncd examples/tutorials/database-to-database-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Database to Database Sync App"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#database-to-database-sync-application", 
            "text": "", 
            "title": "Database to Database Sync application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#summary", 
            "text": "Ingest records from a source PostgreSQL Database table to destination PostgreSQL database table. This application reads messages from configured PostgreSQL table and writes each record to output PostgreSQL table.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/database-to-database-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n     Click on import button for  Database to Database Sync App    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n     Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Database-to-Database-Sync  application.    Launch Database-to-Database-Sync  dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option to display a list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n        Select  Specify custom properties  option. Click on  add default properties  button.\n       This expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n     \nFor example, suppose we wish to process all rows from the table  test_event_input_table  in a\nPostgreSQL database named  testdb  accessible at  source-database-node.com  port  5432  with credentials\nusername= postgres , password= postgres , and we wish to write the output records to other\nPostgreSQL database table  test_event_output_table  accessible at  destination-database-node.com  port  5432  with same creditials listed above. Properties should be set as follows:     Name  Value      dt.operator.JdbcInput.prop.store.databaseDriver  org.postgresql.Driver    dt.operator.JdbcInput.prop.store.databaseUrl  jdbc:postgresql://source-database-node.com:5432/testdb    dt.operator.JdbcInput.prop.store.password  postgres    dt.operator.JdbcInput.prop.store.userName  postgres    dt.operator.JdbcInput.prop.tableName  test_event_input_table    dt.operator.JdbcInput.prop.whereCondition     dt.operator.JdbcOuput.prop.store.databaseDriver  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  jdbc:postgresql://destination-database-node:5432/testdb    dt.operator.JdbcOuput.prop.store.password  postgres    dt.operator.JdbcOuput.prop.store.userName  postgres    dt.operator.JdbcOuput.prop.tablename  test_event_output_table     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#prerequistes", 
            "text": "Meta-data  table is required for Jdbc Output operator for transactional data and application consistency.     Table Name  Column Names      dt_meta  dt_app_id (VARCHAR) dt_operator_id (INT)  dt_window (BIGINT)     Query for  Meta-data  table creation:  CREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.JdbcInput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcInput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://node1.company.com:5432/testdb    dt.operator.JdbcInput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcInput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcInput.prop.tableName  Table name for input records  String  test_event_input_table    dt.operator.JdbcInput.prop.whereCondition  Where clause condition (if any) for input records. Keep blank to fetch all records.  String     dt.operator.JdbcOuput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://node2.company.com:5432/testdb    dt.operator.JdbcOuput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.tableName  Table name for output records  String  test_event_output_table", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.JdbcInput.prop.partitionCount  Number of JDBC input partitions for parallel reading.  4  1    dt.operator.JdbcInput.prop.batchSize  Batch size to read data from JDBC.  300  300    dt.operator.JdbcInput.prop.key  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO  ACCOUNT_NO    dt.operator.JdbcInput.prop.columnsExpression  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO, NAME, AMOUNT  ACCOUNT_NO, NAME, AMOUNT    dt.operator.JdbcInput.port.outputPort.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcInput.prop.pollInterval  Poll interval for scanning new records in milisec  1000  1000    dt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/database-to-database-sync :  cd examples/tutorials/database-to-database-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-sync/", 
            "text": "HDFS Sync App\n\n\nSummary\n\n\nIngest and backup hadoop HDFS data from one cluster to another in a fault tolerant way for use cases such as disaster recovery. This application copies files from the configured source path to the destination file path. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n  \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for HDFS to see all applications related to HDFS.\n\n\n\n\n\n\n\n\nClick on import button for \nHDFS Sync App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS-Sync-App\n\n   application.\n\n\n\n\n\n\nLaunch HDFS-Sync-App\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n   \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n  \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n   \n\n   \n\n For example, suppose we wish to copy all files in \n/user/appuser/input\n from \nremote-cluster\n to \n/user/appuser/archive\n on the host cluster (on which app is running). Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\n/user/appuser/archive\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.files\n\n\nhdfs://remote-cluster/user/appuser/input\n\n\n\n\n\n\n\n\nThis application is tuned for better performance if reading data from remote cluster to host cluster.\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on \nLaunch\n button on bottom right corner to launch the application.\n   Notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (these properties are all strings and\nare HDFS paths: the first is the destination and the second the source).\n\n\n\n\n\n\n\n\nProperty\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\n/user/appuser/output/dir1\nhdfs://node1.corp1.com/user/appuser/output\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.files\n\n\n/user/appuser/input/dir1\n/user/appuser/input/dir2/file1.log\nhdfs://node1.corp1.com/user/appuser/input\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for \ncluster-\nmemory- \nconf.xml\n\n\nDefault for  \nsandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.blocksThreshold\n\n\nRate at which block metadata is emitted per second\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-sync\n:\n\n\ncd examples/tutorials/hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. This application is for copying files from source to destination. Thus, \nApplication.java\n does not involve any processing operator in between.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#hdfs-sync-app", 
            "text": "", 
            "title": "HDFS Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#summary", 
            "text": "Ingest and backup hadoop HDFS data from one cluster to another in a fault tolerant way for use cases such as disaster recovery. This application copies files from the configured source path to the destination file path. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n      Page listing the applications available on AppHub is displayed.\nSearch for HDFS to see all applications related to HDFS.     Click on import button for  HDFS Sync App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n     Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  HDFS-Sync-App \n   application.    Launch HDFS-Sync-App  dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option. This displays list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n       Select  Specify custom properties  option. Click on  add default properties  button.\n      This expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n    \n    \n For example, suppose we wish to copy all files in  /user/appuser/input  from  remote-cluster  to  /user/appuser/archive  on the host cluster (on which app is running). Properties should be set as follows:     name  value      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  /user/appuser/archive    dt.operator.HDFSInputModule.prop.files  hdfs://remote-cluster/user/appuser/input     This application is tuned for better performance if reading data from remote cluster to host cluster.\nDetails about configuration options are available in  Configuration options  section.    Click on  Launch  button on bottom right corner to launch the application.\n   Notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status. The  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties (these properties are all strings and\nare HDFS paths: the first is the destination and the second the source).     Property  Example      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  /user/appuser/output/dir1 hdfs://node1.corp1.com/user/appuser/output    dt.operator.HDFSInputModule.prop.files  /user/appuser/input/dir1 /user/appuser/input/dir2/file1.log hdfs://node1.corp1.com/user/appuser/input", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster- memory-  conf.xml  Default for   sandbox- memory  -conf.xml      dt.operator.HDFSInputModule.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.HDFSInputModule.prop.blocksThreshold  Rate at which block metadata is emitted per second  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-sync :  cd examples/tutorials/hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. This application is for copying files from source to destination. Thus,  Application.java  does not involve any processing operator in between.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/", 
            "text": "HDFS to HDFS line copy application\n\n\nSummary\n\n\nIngest and backup hadoop HDFS data as lines from one cluster to another in a fault tolerant way. This application reads lines from the configured source path and writes them to the destination file path. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-line-copy\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for HDFS to see all applications related to HDFS.\n   \n\n\n\n\n\n\nClick on import button for \nHDFS to HDFS Line Copy App\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS line copy\n\n   application.\n\n\n\n\n\n\nLaunch HDFS-line-copy\n dialogue is displayed. One can configure name of this instance of the application after from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n   \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n   \n\n\nFor example, suppose we wish to process lines from all the files in \n/user/appuser/input\n from \nsource-cluster\n and send the output to \noutput.txt\n in \n/user/appuser/output\n in the \ndestination-cluster\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nhdfs://destination-cluster/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.recordReader.prop.files\n\n\nhdfs://source-cluster/user/appuser/input\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on \nLaunch\n button on bottom right corner to launch the application.\nNotification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage with listing of all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (all properties are strings).\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.files\n\n\nHDFS path for input file or directory\n\n\n/user/appuser/input/directory1\n/user/appuser/input/file2.log\nhdfs://node1.corp1.com/user/user1/input\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nName of the output file. This name will be appended with suffix for each part.\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster\n-memory\n-conf.xml\n\n\nDefault for\nsandbox\n-memory\n-conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.minReaders\n\n\nMinimum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n1\n\n\n1\n\n\n\n\n\n\ndt.operator.recordReader.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-to-kafka-sync\n:\n\n\ncd examples/tutorials/hdfs-line-copy\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with the \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS to HDFS Line Copy App"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#hdfs-to-hdfs-line-copy-application", 
            "text": "", 
            "title": "HDFS to HDFS line copy application"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#summary", 
            "text": "Ingest and backup hadoop HDFS data as lines from one cluster to another in a fault tolerant way. This application reads lines from the configured source path and writes them to the destination file path. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-line-copy .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#mandatory-properties", 
            "text": "End user must specify the values for these properties (all properties are strings).     Property  Description  Example      dt.operator.recordReader.prop.files  HDFS path for input file or directory  /user/appuser/input/directory1 /user/appuser/input/file2.log hdfs://node1.corp1.com/user/user1/input    dt.operator.fileOutput.prop.outputFileName  Name of the output file. This name will be appended with suffix for each part.  output.txt    dt.operator.fileOutput.prop.filePath  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.  /user/appuser/output", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster -memory -conf.xml  Default for sandbox -memory -conf.xml      dt.operator.recordReader.prop.minReaders  Minimum number of BlockReader partitions for parallel reading.  int  1  1    dt.operator.recordReader.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-to-kafka-sync :  cd examples/tutorials/hdfs-line-copy    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with the  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/", 
            "text": "HDFS to Kafka Sync App\n\n\nSummary\n\n\nThis application reads lines from configured HDFS path and writes each line as a message in configured Apache Kafka topic.\nThis document illustrates step by step guide to launch, configure, customize\nthis application.The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-to-kafka-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n   \n\n    Click on import button for \nHDFS to Kafka Sync App\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n    Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS to Kafka Sync\n application.\n\n\n\n\n\n\nLaunch HDFS-to-Kafka-Sync\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n   \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis a expands key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n   \n\n   \n\n   For example, suppose we wish to process lines from all files in \n/user/appuser/input\n from \nsource-cluster\n and send the output to kafka on \nkafka-server-node\n with topic \ntest\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaOutput.prop.producerProperties\n\n\nserializer.class=kafka.serializer. DefaultEncoder,\nproducer.type= async,\nmetadata.broker. list=kafka-server-node:9092\n\n\n\n\n\n\ndt.operator.kafkaOutput.prop.topic\n\n\ntest\n\n\n\n\n\n\ndt.operator.recordReader.prop.files\n\n\n/user/appuser/input\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on \nLaunch\n button on lower right corner to launch the application.\nNotification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaOutput. prop.producerProperties\n\n\nProperties for Kafka producer\n\n\nComma separated String\n\n\nserializer.class=kafka.serializer.DefaultEncoder, producer.type=async,\nmetadata.broker.list=kafka-server-node:9092\n\n\n\n\n\n\ndt.operator.kafkaOutput .prop.topic\n\n\nKafka topic for output records\n\n\nString\n\n\ntest\n\n\n\n\n\n\ndt.operator.recordReader\nprop.files\n\n\nHDFS path for input file or directory\n\n\nString\n\n\n/user/appuser/input/directory1\n/user/appuser/input/file2.log\nhdfs://node1.corp1.com/user/appuser/input\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nCluster default\n\n\nSandbox default\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.minReaders\n\n\nMinimum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n1\n\n\n1\n\n\n\n\n\n\ndt.operator.recordReader.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.kafkaOutput.attr.PARTITIONER\n\n\nPartitoning for Kafka output operator\n\n\nString\n\n\nSee\n (1)\n\n\nSee\n (2)\n\n\n\n\n\n\n\n\n\n\nCluster default\n: \ncom.datatorrent.common.partitioner.StatelessPartitioner:16\n\n\nSandbox default\n: \ncom.datatorrent.common.partitioner.StatelessPartitioner:1\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-to-kafka-sync\n:\n\n\ncd examples/tutorials/hdfs-to-kafka-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n    \nmvn clean package\n\n\nThis will generate the application package with the \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS to Kafka Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#hdfs-to-kafka-sync-app", 
            "text": "", 
            "title": "HDFS to Kafka Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#summary", 
            "text": "This application reads lines from configured HDFS path and writes each line as a message in configured Apache Kafka topic.\nThis document illustrates step by step guide to launch, configure, customize\nthis application.The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-to-kafka-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n    \n    Click on import button for  HDFS to Kafka Sync App    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n    Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  HDFS to Kafka Sync  application.    Launch HDFS-to-Kafka-Sync  dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option. This displays list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n       Select  Specify custom properties  option. Click on  add default properties  button.\n       This a expands key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n    \n    \n   For example, suppose we wish to process lines from all files in  /user/appuser/input  from  source-cluster  and send the output to kafka on  kafka-server-node  with topic  test . Properties should be set as follows:     name  value      dt.operator.kafkaOutput.prop.producerProperties  serializer.class=kafka.serializer. DefaultEncoder, producer.type= async, metadata.broker. list=kafka-server-node:9092    dt.operator.kafkaOutput.prop.topic  test    dt.operator.recordReader.prop.files  /user/appuser/input     Details about configuration options are available in  Configuration options  section.    Click on  Launch  button on lower right corner to launch the application.\nNotification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n   The  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.kafkaOutput. prop.producerProperties  Properties for Kafka producer  Comma separated String  serializer.class=kafka.serializer.DefaultEncoder, producer.type=async, metadata.broker.list=kafka-server-node:9092    dt.operator.kafkaOutput .prop.topic  Kafka topic for output records  String  test    dt.operator.recordReader prop.files  HDFS path for input file or directory  String  /user/appuser/input/directory1 /user/appuser/input/file2.log hdfs://node1.corp1.com/user/appuser/input", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Cluster default  Sandbox default      dt.operator.recordReader.prop.minReaders  Minimum number of BlockReader partitions for parallel reading.  int  1  1    dt.operator.recordReader.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.kafkaOutput.attr.PARTITIONER  Partitoning for Kafka output operator  String  See  (1)  See  (2)      Cluster default :  com.datatorrent.common.partitioner.StatelessPartitioner:16  Sandbox default :  com.datatorrent.common.partitioner.StatelessPartitioner:1   You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-to-kafka-sync :  cd examples/tutorials/hdfs-to-kafka-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:\n     mvn clean package  This will generate the application package with the  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/", 
            "text": "Kafka To Database Sync application\n\n\nSummary\n\n\nIngest string messages seperated by '|' from configured kafka topic and writes each message as a record in DataBase. This application uses PoJoEvent as an example schema, this can be customized to use custom schema based on specific needs.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-database-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n   \n\n\nClick on import button for \nKafka to Database Sync App\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-Database-Sync\n application.\n\n\n\n\n\n\nLaunch Kafka-to-Database-Sync\n dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option to display a list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n    \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n   \n\n\n\nFor example, suppose we wish string messages seperated with '|' to the table \ntest_event_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \ntarget-database.node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nkafka-source.node.com:9092\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\ntransactions\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nEARLIEST\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\njdbc:postgresql://target-database.node .com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tablename\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n   \n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\n\n\nKafka configured with version 0.9.\n\n\nMeta-data\n table is required for Jdbc Output operator for transactional data and application consistency.\n\n\n\n\n\n\n\n\n\n\nTable Name\n\n\nColumn Names\n\n\n\n\n\n\n\n\n\n\ndt_meta\n\n\ndt_app_id (VARCHAR)\ndt_operator_id (INT) \ndt_window (BIGINT)\n\n\n\n\n\n\n\n\nQuery for \nMeta-data\n table creation:\n\n\nCREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nList of Brokers for kafka input\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nKafka topics for input\n\n\nString\n\n\ntransactions\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nKafka input offset\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://target-database.node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tableName\n\n\nTable name for output records\n\n\nString\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.csvParser.port.out.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by Kafka input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.csvParser.prop.schema\n\n\nSchema for CSV parser\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/kafka-to-database-sync\n:\n\n\ncd examples/tutorials/kafka-to-database-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Kafka to Database Sync App"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#kafka-to-database-sync-application", 
            "text": "", 
            "title": "Kafka To Database Sync application"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#summary", 
            "text": "Ingest string messages seperated by '|' from configured kafka topic and writes each message as a record in DataBase. This application uses PoJoEvent as an example schema, this can be customized to use custom schema based on specific needs.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/kafka-to-database-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Database to see all applications related to Database.\n     Click on import button for  Kafka to Database Sync App    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n     Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-Database-Sync  application.    Launch Kafka-to-Database-Sync  dialogue is displayed. One can configure the name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option to display a list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n        Select  Specify custom properties  option. Click on  add default properties  button.\n       This expands a key-value editor pre-populated with mandatory properties for this\napplication. Change values as needed.\n     \nFor example, suppose we wish string messages seperated with '|' to the table  test_event_table  in a\nPostgreSQL database named  testdb  accessible at  target-database.node.com  port  5432  with credentials\nusername= postgres , password= postgres . Properties should be set as follows:     Name  Value      dt.operator.kafkaInput.prop.clusters  kafka-source.node.com:9092    dt.operator.kafkaInput.prop.topics  transactions    dt.operator.kafkaInput.prop.initialOffset  EARLIEST    dt.operator.JdbcOuput.prop.store.databaseDriver  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  jdbc:postgresql://target-database.node .com:5432/testdb    dt.operator.JdbcOuput.prop.store.password  postgres    dt.operator.JdbcOuput.prop.store.userName  postgres    dt.operator.JdbcOuput.prop.tablename  test_event_output_table     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button at the lower right corner of the dialog to launch the\napplication.\nA notification is displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#prerequistes", 
            "text": "Kafka configured with version 0.9.  Meta-data  table is required for Jdbc Output operator for transactional data and application consistency.      Table Name  Column Names      dt_meta  dt_app_id (VARCHAR) dt_operator_id (INT)  dt_window (BIGINT)     Query for  Meta-data  table creation:  CREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.kafkaInput.prop.clusters  List of Brokers for kafka input  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.topics  Kafka topics for input  String  transactions    dt.operator.kafkaInput.prop.initialOffset  Kafka input offset  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.JdbcOuput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://target-database.node.com:5432/testdb    dt.operator.JdbcOuput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.tableName  Table name for output records  String  test_event_output_table", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.csvParser.port.out.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by Kafka input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.csvParser.prop.schema  Schema for CSV parser  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/kafka-to-database-sync :  cd examples/tutorials/kafka-to-database-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/", 
            "text": "Kafka to HDFS Filter Application\n\n\nSummary\n\n\nIngest filtered messages from kafka to hadoop HDFS for continuous ingestion to hadoop.\nThe source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n   \n\n   Click on import button for \nKafka to HDFS Filter App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-HDFS-Filter\n application.\n\n\n\n\n\n\nLaunch Kafka-to-HDFS-Filter\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n   \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n   \n\n   \n\n   For example, suppose we wish to process all messages seperated by '|' from topic \ntransactions\n at the kafka server running on \nkafka-source.node.com\n with port \n9092\n, filters the messages based on the filter criteria \n({$}.getAmount() \n= 20000)\n and write them to \noutput.txt\n under \n/user/appuser/output\n on HDFS.\n   Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.filter.prop.condition\n\n\nFilter condition\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nkafka-source.node.com:9092\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nEARLIEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\ntransactions\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\nKafka configured with version 0.9.\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.filter.prop.condition\n\n\nFilter condition\n\n\nCondition\n\n\n({$}.getAmount() \n= 20000)\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nComma separated list of kafka-brokers\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nInitial offset to read from Kafka\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nTopics to read from Kafka\n\n\nString\n\n\nevent_data\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster-\nmemory\n- conf.xml\n\n\nDefault for\n sandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.maxLength\n\n\nMaximum length for output file after which file is rotated\n\n\nlong\n\n\nLong.MAX_VALUE\n\n\nLong.MAX_VALUE\n\n\n\n\n\n\ndt.operator.csvParser.prop.schema\n\n\nSchema for CSV Parser\n\n\nSchema\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter.prop.schema\n\n\nSchema for CSV formatter\n\n\nSchema\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter. port.in.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter\n\n\nPOJO\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.filter.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to Filter\n\n\nPOJO\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kafka-to-hdfs-filter':\n\n\ncd examples/tutorials/kafka-to-hdfs-filter\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kafka to HDFS Filter App"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#kafka-to-hdfs-filter-application", 
            "text": "", 
            "title": "Kafka to HDFS Filter Application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#summary", 
            "text": "Ingest filtered messages from kafka to hadoop HDFS for continuous ingestion to hadoop.\nThe source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n    \n   Click on import button for  Kafka to HDFS Filter App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-HDFS-Filter  application.    Launch Kafka-to-HDFS-Filter  dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option. This displays list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n       Select  Specify custom properties  option. Click on  add default properties  button.\n       This expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n    \n    \n   For example, suppose we wish to process all messages seperated by '|' from topic  transactions  at the kafka server running on  kafka-source.node.com  with port  9092 , filters the messages based on the filter criteria  ({$}.getAmount()  = 20000)  and write them to  output.txt  under  /user/appuser/output  on HDFS.\n   Properties should be set as follows:     name  value      dt.operator.fileOutput.prop.filePath  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  output.txt    dt.operator.filter.prop.condition  Filter condition    dt.operator.kafkaInput.prop.clusters  kafka-source.node.com:9092    dt.operator.kafkaInput.prop.initialOffset  EARLIEST    dt.operator.kafkaInput.prop.topics  transactions     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#prerequistes", 
            "text": "Kafka configured with version 0.9.", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt    dt.operator.filter.prop.condition  Filter condition  Condition  ({$}.getAmount()  = 20000)    dt.operator.kafkaInput.prop.clusters  Comma separated list of kafka-brokers  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.initialOffset  Initial offset to read from Kafka  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.kafkaInput.prop.topics  Topics to read from Kafka  String  event_data", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Type  Default for  cluster- memory - conf.xml  Default for  sandbox- memory  -conf.xml      dt.operator.fileOutput.prop.maxLength  Maximum length for output file after which file is rotated  long  Long.MAX_VALUE  Long.MAX_VALUE    dt.operator.csvParser.prop.schema  Schema for CSV Parser  Schema  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter.prop.schema  Schema for CSV formatter  Schema  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter. port.in.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter  POJO  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.filter.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to Filter  POJO  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kafka-to-hdfs-filter':  cd examples/tutorials/kafka-to-hdfs-filter    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/", 
            "text": "Kafka to HDFS Sync Application\n\n\nSummary\n\n\nIngest messages from kafka to hadoop HDFS for continuous ingestion to hadoop. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n   \n\n   Click on import button for \nKafka to HDFS Sync App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-HDFS-Sync\n application.\n\n\n\n\n\n\nLaunch Kafka-to-HDFS-Sync\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n   \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n   \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n   \n\n   \n\n   For example, suppose we wish to process all messages from topic \ntransactions\n at the kafka server running on localhost port 9092\n   and write them to \noutput.txt\n under \n/user/appuser/output\n on HDFS. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nlocalhost:9092\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nEARLIEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\ntransactions\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nComma separated list of kafka-brokers\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nInitial offset to read from Kafka\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nTopics to read from Kafka\n\n\nString\n\n\nevent_data\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster-\nmemory\n- conf.xml\n\n\nDefault for\n sandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.maxLength\n\n\nMaximum length for output file after which file is rotated\n\n\nlong\n\n\nLong.MAX_VALUE\n\n\nLong.MAX_VALUE\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kafka-to-hdfs-sync':\n\n\ncd examples/tutorials/kafka-to-hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kafka to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#kafka-to-hdfs-sync-application", 
            "text": "", 
            "title": "Kafka to HDFS Sync Application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#summary", 
            "text": "Ingest messages from kafka to hadoop HDFS for continuous ingestion to hadoop. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Kafka to see all applications related to Kafka.\n    \n   Click on import button for  Kafka to HDFS Sync App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-HDFS-Sync  application.    Launch Kafka-to-HDFS-Sync  dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n       Select  Use saved configuration  option. This displays list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n       Select  Specify custom properties  option. Click on  add default properties  button.\n       This expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n    \n    \n   For example, suppose we wish to process all messages from topic  transactions  at the kafka server running on localhost port 9092\n   and write them to  output.txt  under  /user/appuser/output  on HDFS. Properties should be set as follows:     name  value      dt.operator.fileOutput.prop.filePath  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  output.txt    dt.operator.kafkaInput.prop.clusters  localhost:9092    dt.operator.kafkaInput.prop.initialOffset  EARLIEST    dt.operator.kafkaInput.prop.topics  transactions     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt    dt.operator.kafkaInput.prop.clusters  Comma separated list of kafka-brokers  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.initialOffset  Initial offset to read from Kafka  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.kafkaInput.prop.topics  Topics to read from Kafka  String  event_data", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster- memory - conf.xml  Default for  sandbox- memory  -conf.xml      dt.operator.fileOutput.prop.maxLength  Maximum length for output file after which file is rotated  long  Long.MAX_VALUE  Long.MAX_VALUE     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kafka-to-hdfs-sync':  cd examples/tutorials/kafka-to-hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/", 
            "text": "Kinesis to S3 Application\n\n\nSummary\n\n\nIngest messages from kinesis and write to S3 bucket.\nThe source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kinesis-to-s3.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed.\nSearch for Kinesis to see all applications related to Kinesis.\n   \n\n   Click on import button for \nKinesis to S3 App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nkinesis-to-S3\n application.\n\n\n\n\n\n\nLaunch Kinesis-to-S3\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n   \n\n\n\n\n\n\nSelect \nSpecify Launch properties\n option. This expands a key-value editor with mandatory properties for this application. \n   \n\n\n\n\n\n\nSpecify the mandatory properties under \nSpecify Launch Properties\n\n   \n\n   \n\n   For example, suppose we wish to process all messages from Kinesis stream \ntransactions\n \n      and write them to \noutput.txt\n under \n/user/appuser/output\n on S3. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\ntransactions\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nKINESIS_ACCESS_KEY\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nKINESIS_SECRET_KEY\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nKINESIS_END_POINT\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nS3_ACCESS_KEY\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nS3_SECRET_KEY\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nS3_BUCKET_NAME\n\n\n\n\n\n\nS3Output Directory Path\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\nName of the stream from where the records to be fetched\n\n\nString\n\n\ntransactions\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nIndicates the accessKey which have read access to the Kinesis stream\n\n\nString\n\n\nKINESIS_ACCESS_KEY\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nIndicates the secret AccessKey which have read access to the Kinesis stream\n\n\nString\n\n\nKINESIS_SECRET_KEY\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nIndicates the endpoint of the kinesis stream\n\n\nString\n\n\nKINESIS_END_POINT\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nAWS access key which have write access to the S3 bucket\n\n\nString\n\n\nS3_ACCESS_KEY\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nAWS Secret key which have write access to the S3 bucket\n\n\nString\n\n\nS3_SECRET_KEY\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nName of S3 bucket\n\n\nSting\n\n\nS3_BUCKET_NAME\n\n\n\n\n\n\nS3Output Directory Path\n\n\nOutput path for S3 files\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kinesis-to-s3':\n\n\ncd examples/tutorials/kinesis-to-s3\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kinesis to S3 App"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#kinesis-to-s3-application", 
            "text": "", 
            "title": "Kinesis to S3 Application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#summary", 
            "text": "Ingest messages from kinesis and write to S3 bucket.\nThe source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kinesis-to-s3.  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n       Page listing the applications available on AppHub is displayed.\nSearch for Kinesis to see all applications related to Kinesis.\n    \n   Click on import button for  Kinesis to S3 App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  kinesis-to-S3  application.    Launch Kinesis-to-S3  dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n       Select  Specify Launch properties  option. This expands a key-value editor with mandatory properties for this application. \n       Specify the mandatory properties under  Specify Launch Properties \n    \n    \n   For example, suppose we wish to process all messages from Kinesis stream  transactions  \n      and write them to  output.txt  under  /user/appuser/output  on S3. Properties should be set as follows:     name  value      Stream Name For Kinesis Input  transactions    Access Key For Kinesis Input  KINESIS_ACCESS_KEY    Secret Key For Kinesis Input  KINESIS_SECRET_KEY    End Point For Kinesis Input  KINESIS_END_POINT    Access Key For S3Output  S3_ACCESS_KEY    Secret Access Key For S3Output  S3_SECRET_KEY    S3Output Bucket Name  S3_BUCKET_NAME    S3Output Directory Path  /user/appuser/output     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      Stream Name For Kinesis Input  Name of the stream from where the records to be fetched  String  transactions    Access Key For Kinesis Input  Indicates the accessKey which have read access to the Kinesis stream  String  KINESIS_ACCESS_KEY    Secret Key For Kinesis Input  Indicates the secret AccessKey which have read access to the Kinesis stream  String  KINESIS_SECRET_KEY    End Point For Kinesis Input  Indicates the endpoint of the kinesis stream  String  KINESIS_END_POINT    Access Key For S3Output  AWS access key which have write access to the S3 bucket  String  S3_ACCESS_KEY    Secret Access Key For S3Output  AWS Secret key which have write access to the S3 bucket  String  S3_SECRET_KEY    S3Output Bucket Name  Name of S3 bucket  Sting  S3_BUCKET_NAME    S3Output Directory Path  Output path for S3 files  String  /user/appuser/output", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kinesis-to-s3':  cd examples/tutorials/kinesis-to-s3    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/", 
            "text": "S3 to HDFS Sync App\n\n\nSummary\n\n\nIngest and backup Amazon S3 data to hadoop HDFS for data download from Amazon to hadoop. This application copies files from the configured S3 location to the destination path in HDFS. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/s3-to-hdfs-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppHub tab from the top navigation bar.\n    \n\n\n\n\n\n\nPage listing the applications available on AppHub is displayed. Search for S3 to see all applications related to S3.\n    \n\n    Click on import button for \nS3 to HDFS Sync App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n    \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n    \n\n    Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nS3-to-HDFS-Sync\n application.\n\n\n\n\n\n\nLaunch S3-to-HDFS-Sync\n dialogue is displayed. One can configure name of this instance of the application from this dialogue.\n\n\n\n\n\n\n\nSelect \nUse saved configuration\n option. This displays a list of pre-saved configurations.\nPlease select \nsandbox-memory-conf.xml\n or \ncluster-memory-conf.xml\n depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n    \n\n\n\n\n\n\nSelect \nSpecify custom properties\n option. Click on \nadd default properties\n button.\n    \n\n\n\n\n\n\nThis expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n    \n\n    \n\n    For example, suppose we wish to copy from all files in \ndirectory1\n from \ncom.example.s3test\n using \nACCESS_KEY_ID:SECRET_KEY\n combination to \n/user/appuser/output\n on the host cluster (on which app is running). Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.files\n\n\ns3n://ACCESS_KEY_ID:SECRET_KEY@com.example.s3test/directory1\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nClick on the \nLaunch\n button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n    \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n    \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n    \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n    \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (these properties are all strings).\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\nHDFS path for destination directory\n\n\n/user/appuser/\n output/directory1\nhdfs://node1.company1.com\n /user/appuser/output\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.files\n\n\nAccess URL for S3 source\n\n\ns3n://ACCESS_KEY_ID:SECRET_KEY\n@BUCKET_NAME/DIRECTORY\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\ncluster-\nmemory-\n conf.xml\n\n\nDefault for\n  sandbox\n-memory -\nconf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.blocksThreshold\n\n\nRate at which block metadata is emitted per second\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/s3-to-hdfs-sync\n:\n\n\ncd examples/tutorials/s3-to-hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. This application is for copying files from source to destination. Thus, \nApplication.java\n does not involve any processing operator in between.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "S3 to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#s3-to-hdfs-sync-app", 
            "text": "", 
            "title": "S3 to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#summary", 
            "text": "Ingest and backup Amazon S3 data to hadoop HDFS for data download from Amazon to hadoop. This application copies files from the configured S3 location to the destination path in HDFS. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/s3-to-hdfs-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppHub tab from the top navigation bar.\n        Page listing the applications available on AppHub is displayed. Search for S3 to see all applications related to S3.\n     \n    Click on import button for  S3 to HDFS Sync App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n        Click on the link in the notification which navigates to the page for this application package.\n     \n    Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  S3-to-HDFS-Sync  application.    Launch S3-to-HDFS-Sync  dialogue is displayed. One can configure name of this instance of the application from this dialogue.    Select  Use saved configuration  option. This displays a list of pre-saved configurations.\nPlease select  sandbox-memory-conf.xml  or  cluster-memory-conf.xml  depending on whether\nyour environment is the DataTorrent sandbox, or other cluster.\n        Select  Specify custom properties  option. Click on  add default properties  button.\n        This expands a key-value editor pre-populated with mandatory properties for this application. Change values as needed.\n     \n     \n    For example, suppose we wish to copy from all files in  directory1  from  com.example.s3test  using  ACCESS_KEY_ID:SECRET_KEY  combination to  /user/appuser/output  on the host cluster (on which app is running). Properties should be set as follows:     name  value      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  /user/appuser/output    dt.operator.S3InputModule.prop.files  s3n://ACCESS_KEY_ID:SECRET_KEY@com.example.s3test/directory1     Details about configuration options are available in  Configuration options  section.    Click on the  Launch  button on lower right corner of the dialog to launch the application.\nA notification is displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n        Click on the  Monitor  tab from the top navigation bar.\n        A page listing all running applications is displayed. Search for the current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n        Application instance details page shows key metrics for monitoring the application status.  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n        Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties (these properties are all strings).     Property  Description  Example      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  HDFS path for destination directory  /user/appuser/  output/directory1 hdfs://node1.company1.com  /user/appuser/output    dt.operator.S3InputModule.prop.files  Access URL for S3 source  s3n://ACCESS_KEY_ID:SECRET_KEY @BUCKET_NAME/DIRECTORY", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for cluster- memory-  conf.xml  Default for   sandbox -memory - conf.xml      dt.operator.S3InputModule.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.S3InputModule.prop.blocksThreshold  Rate at which block metadata is emitted per second  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/s3-to-hdfs-sync :  cd examples/tutorials/s3-to-hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. This application is for copying files from source to destination. Thus,  Application.java  does not involve any processing operator in between.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n       Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app_data_tracker/", 
            "text": "App Data Tracker\n\n\nIntroduction\n\n\nApp Data Tracker, called ADT from now on for conciseness, is a system Apex application that utilizes App Data Framework for collecting and aggregating stats and auto-metrics of user Apex applications.\n\n\nStats\n are pre-defined measurements that tracks the health and performance of an application. There are 2 types of stats for each Apex application.\n\n Application stats - these include stats at the application level. For example - \nplannedContainers\n, \nnumOperators\n, \nfailedContainers\n, \ntotalTuplesProcessed\n, etc.\n\n Operator stats - these include stats of each operator in an application. Example of per operator stats are - \ntotalTuplesProcessed\n, \ntotalTuplesEmitted\n, \ncpuPercentageMA\n, etc.\n\n\nAutoMetrics\n are custom measurements that an operator developer can define for their operator. If an operator having auto-metrics is integrated to an application, then these auto-metrics become eligible\nto be collected by ADT for aggregation. AutoMetrics are explained in detail \nhere\n.\n\n\nInteraction between ADT and user Apex Applications\n\n\nAfter ADT has been enabled, applications will push stats and metrics to dtGateway. In this scenario, dtGateway serves as a message bus. ADT acts as a\nsubscriber to this message bus. The diagram below depicts the message flow among user Apex applications, dtGateway and ADT.\n\n\n\n\nLaunching ADT\n\n\nADT can be automatically launched by dtGateway if the dt-site.xml that is used by the dtGateway has the following properties:\n\n\nproperty\n\n  \nname\ndt.appDataTracker.enable\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.appDataTracker.transport\n/name\n\n  \nvalue\ncom.datatorrent.common.metric.AutoMetricBuiltInTransport:AppDataTrackerFeed\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.attr.METRICS_TRANSPORT\n/name\n\n  \nvalue\ncom.datatorrent.common.metric.AutoMetricBuiltInTransport:AppDataTrackerFeed\n/value\n\n\n/property\n\n\n\n\n\nNote\n: ADT will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.\n\n\nADT Topology\n\n\n\n\nThe input operator in ADT is a web-socket input operator which listens to the topic \nAppDataTrackerFeed\n. This is the topic which the\napplication masters of the user applications are publishing to.\n\n\nOnce the messages enter ADT, they are processed for aggregation and stored in HDHT.\n\n\nAggregations\n\n\nADT aggregates only \nMetrics\n by time. \nStats\n - application or operator level represent cumulative values and therefore are not time-aggregated.\n\n\nBy default, the time buckets ADT aggregates upon are one minute, one hour and one day. It can be overridden by changing the operator attribute \nMETRICS_DIMENSIONS_SCHEME\n.\n\n\nAlso by default, ADT performs all these aggregations : SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute \nMETRICS_DIMENSIONS_SCHEME\n, provided the custom aggregator is known to ADT.  (See next section)\n\n\nCustom Aggregator in ADT\n\n\nCustom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to write a Custom aggregator the implementations of the\nfollowing methods is needed:\n\n\n\n\nCombining new inputs with the current aggregation\n\n\nCombining two aggregations together into one aggregation\n\n\n\n\nLet\u2019s consider the case where we want to perform the following rolling average:\n\n\nY_n = (1/2) * X_n + (1/4) * X_n-1 + (1/8) * X_n-2 + (1/16) * X_n-3 +...\n\n\nThis aggregation could be performed by the following Custom Aggregator:\n\n\n@Name(\nIIRAVG\n)\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index \n destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}\n\n\n\n\nDiscovery of Custom Aggregators\n\n\nADT searches for custom aggregator jars under the following directories statically before launching:\n\n\n\n\n{dt_installation_dir}/plugin/aggregators\n\n\n{user_home_dir}/.dt/plugin/aggregators\n\n\n\n\nIt uses reflection to find all the classes that extend from \nIncrementalAggregator\n and \nOTFAggregator\n in these jars and registers them with the name provided by \n@Name\n annotation (or class name when \n@Name\n is absent).\n\n\nUsing \nMETRICS_DIMENSIONS_SCHEME\n\n\nHere is a sample code snippet on how you can make use of \nMETRICS_DIMENSIONS_SCHEME\n to set your own time buckets and your own set of aggregators for certain \nAutoMetric\ns performed by the ADT in your application.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] { \n1s\n, \n1m\n, \n1h\n };\n      String[] lengthAggregators = new String[] { \nIIRAVG\n, \nSUM\n };\n      String[] countAggregators = new String[] { \nSUM\n };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if (\nlength\n.equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if (\ncount\n.equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }\n\n\n\n\nDashboards\n\n\nWith ADT enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the \ndiagram\n, dtGateway relays queries and query results to and from ADT.  In this way, dtManage sends queries and receives results from ADT via dtGateway and uses the results to let the user visualize the data.\n\n\nClick on the visualize button in dtManage's application page.\n\n\n\n\nYou will see the dashboard for the AutoMetrics and the system metrics.\n\n\n\n\nThe left widget shows the AutoMetrics of \nline\n and \ncount\n for the \nLineReceiver\n operator.  The right widget shows the system metrics.\n\n\nThe Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/app_data_tracker/#app-data-tracker", 
            "text": "", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/app_data_tracker/#introduction", 
            "text": "App Data Tracker, called ADT from now on for conciseness, is a system Apex application that utilizes App Data Framework for collecting and aggregating stats and auto-metrics of user Apex applications.  Stats  are pre-defined measurements that tracks the health and performance of an application. There are 2 types of stats for each Apex application.  Application stats - these include stats at the application level. For example -  plannedContainers ,  numOperators ,  failedContainers ,  totalTuplesProcessed , etc.  Operator stats - these include stats of each operator in an application. Example of per operator stats are -  totalTuplesProcessed ,  totalTuplesEmitted ,  cpuPercentageMA , etc.  AutoMetrics  are custom measurements that an operator developer can define for their operator. If an operator having auto-metrics is integrated to an application, then these auto-metrics become eligible\nto be collected by ADT for aggregation. AutoMetrics are explained in detail  here .", 
            "title": "Introduction"
        }, 
        {
            "location": "/app_data_tracker/#interaction-between-adt-and-user-apex-applications", 
            "text": "After ADT has been enabled, applications will push stats and metrics to dtGateway. In this scenario, dtGateway serves as a message bus. ADT acts as a\nsubscriber to this message bus. The diagram below depicts the message flow among user Apex applications, dtGateway and ADT.", 
            "title": "Interaction between ADT and user Apex Applications"
        }, 
        {
            "location": "/app_data_tracker/#launching-adt", 
            "text": "ADT can be automatically launched by dtGateway if the dt-site.xml that is used by the dtGateway has the following properties:  property \n   name dt.appDataTracker.enable /name \n   value true /value  /property  property \n   name dt.appDataTracker.transport /name \n   value com.datatorrent.common.metric.AutoMetricBuiltInTransport:AppDataTrackerFeed /value  /property  property \n   name dt.attr.METRICS_TRANSPORT /name \n   value com.datatorrent.common.metric.AutoMetricBuiltInTransport:AppDataTrackerFeed /value  /property   Note : ADT will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.", 
            "title": "Launching ADT"
        }, 
        {
            "location": "/app_data_tracker/#adt-topology", 
            "text": "The input operator in ADT is a web-socket input operator which listens to the topic  AppDataTrackerFeed . This is the topic which the\napplication masters of the user applications are publishing to.  Once the messages enter ADT, they are processed for aggregation and stored in HDHT.", 
            "title": "ADT Topology"
        }, 
        {
            "location": "/app_data_tracker/#aggregations", 
            "text": "ADT aggregates only  Metrics  by time.  Stats  - application or operator level represent cumulative values and therefore are not time-aggregated.  By default, the time buckets ADT aggregates upon are one minute, one hour and one day. It can be overridden by changing the operator attribute  METRICS_DIMENSIONS_SCHEME .  Also by default, ADT performs all these aggregations : SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute  METRICS_DIMENSIONS_SCHEME , provided the custom aggregator is known to ADT.  (See next section)", 
            "title": "Aggregations"
        }, 
        {
            "location": "/app_data_tracker/#custom-aggregator-in-adt", 
            "text": "Custom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to write a Custom aggregator the implementations of the\nfollowing methods is needed:   Combining new inputs with the current aggregation  Combining two aggregations together into one aggregation   Let\u2019s consider the case where we want to perform the following rolling average:  Y_n = (1/2) * X_n + (1/4) * X_n-1 + (1/8) * X_n-2 + (1/16) * X_n-3 +...  This aggregation could be performed by the following Custom Aggregator:  @Name( IIRAVG )\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index   destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}", 
            "title": "Custom Aggregator in ADT"
        }, 
        {
            "location": "/app_data_tracker/#discovery-of-custom-aggregators", 
            "text": "ADT searches for custom aggregator jars under the following directories statically before launching:   {dt_installation_dir}/plugin/aggregators  {user_home_dir}/.dt/plugin/aggregators   It uses reflection to find all the classes that extend from  IncrementalAggregator  and  OTFAggregator  in these jars and registers them with the name provided by  @Name  annotation (or class name when  @Name  is absent).", 
            "title": "Discovery of Custom Aggregators"
        }, 
        {
            "location": "/app_data_tracker/#using-metrics_dimensions_scheme", 
            "text": "Here is a sample code snippet on how you can make use of  METRICS_DIMENSIONS_SCHEME  to set your own time buckets and your own set of aggregators for certain  AutoMetric s performed by the ADT in your application.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator( LineReceiver , new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] {  1s ,  1m ,  1h  };\n      String[] lengthAggregators = new String[] {  IIRAVG ,  SUM  };\n      String[] countAggregators = new String[] {  SUM  };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if ( length .equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if ( count .equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }", 
            "title": "Using METRICS_DIMENSIONS_SCHEME"
        }, 
        {
            "location": "/app_data_tracker/#dashboards", 
            "text": "With ADT enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the  diagram , dtGateway relays queries and query results to and from ADT.  In this way, dtManage sends queries and receives results from ADT via dtGateway and uses the results to let the user visualize the data.  Click on the visualize button in dtManage's application page.   You will see the dashboard for the AutoMetrics and the system metrics.   The left widget shows the AutoMetrics of  line  and  count  for the  LineReceiver  operator.  The right widget shows the system metrics.  The Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "Dashboards"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest Tutorial\n\n\n\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the \ndtIngest\nblog\n.\n\n\nThis tutorial refers to dtIngest version 1.0.0\n\n\nPre-requisites\n\n\n\n\n\n\nDatatorrent RTS on a Hadoop cluster. Please\n    refer to \nInstallation\n    guide\n\u00a0for\n    details.\n\n\n\n\n\n\nSource and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.\n\n\n\n\n\n\nIf source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.\n\n\n\n\n\n\nLaunching dtIngest\n\n\ndtIngest application can be configured and launched from \nDatatorrent\nManagement\nConsole\n.\n\n\n\n\n\n\nNavigate to 'Develop' tab.\n    \n\n\n\n\n\n\nThe dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0\n\n\n\n\n\n\nSelect 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n    \n\n\n\n\n\n\nConfiguration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.\n\n\n\n\n\n\nConfiguring dtIngest Instance Properties\n\n\n\n\n\n\nIn the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n    \n\n\n\n\n\n\nLeave 'Specify a queue' unchecked to use default queue.\n\n\nIf you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to \nHadoop Capacity Scheduler Docs\n\n\n\n\n\n\n\n\nUnder 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n    \n\n\nOnce it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one. \n\n\nLeave 'Use a config file' unchecked to create a new one. \n\n\n\n\n\n\nConfigure input source, refer to \nConfiguring input\n    source\n\u00a0section for details.\n\n\n\n\n\n\nConfigure output destination, refer to \nConfiguring output\n    destination\n\u00a0section for details.\n\n\n\n\n\n\nConfigure processing steps, refer to \nConfiguring processing\n    steps\n\u00a0section for details.\n\n\n\n\n\n\nUnder 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.\u00a0\n\n\n\n\n\n\nConfiguring Input Source\n\n\nConfiguring HDFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n    \nhdfs://namenode1.cluster.company.org:8020/user/john/data\n.\n\n    Where,\n\n\n\n\nhdfs://\n indicates HDFS protocol\n\n\nnamenode1.cluster.company.org\n indicates fully qualified domain\n   name for the namenode of source HDFS.\n\n\n8020\n indicates port number for HDFS namenode service\n\n\n/user/john/data\n indicates full path for destination directory  \n\n\n\n\n\n\nIf there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n    \nJava regular expression  documentation\n.\n     For example, if only \n.log\n files need to be ingested, then use \n.*\\.log\n as regular expression.  \n\n\n\n\nWhere,\n-   \n.*\n indicates any character zero or more times\n-   \n\\.\n indicates dot escaped with backslash\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only '.log' files from the source\ndirectories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.\n\n\nIf 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between subsequent scans for detecting\nnew/modified files.  \n\n\n\n\n\n\n\n\nConfiguring\u00a0NFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'File/NFS' option from the\n    drop-down. \n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL file path.\n\n\nFor example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk5/nfsmount/path/to/data/directory\n.\nWhere,\n-   \nfile://\n indicates that it is some file system mounted on the node.\n-   \n/disk5/nfsmount/\n indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-   \npath/to/data/directory\n is the directory to be ingested\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nIf there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.\n\n\n\n\n\n\nIf there are specific files, as opposed to a directory,specify complete\n    URL file path.\n\n\nFor example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt\n.\n\n\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.\n\n\n\n\nWhere,\n-   \n.\\*\n indicates any character zero or more times\n-   \n\\\\.\n indicates dot escaped with backslash '\\'\n-   \nlog\n indicates desired extension which is 'log'\n\n\nTherefore, dtingest ingests only \n.log\n files from the source directories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.\n\n\n\n\n\n\n\n\nConfiguring FTP input\n\n\nThis section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0FTP as input type\n    \n\n\n\n\n\n\nAfter selecting the FTP as input type, snapshot of UI as below:     \n\n\n\n\n\n\nThe format for FTP URL input is as follows:  \nftp://username:password@host:port/path\n\n    where,\n\n\n\n\nftp\n : \u00a0protocol name\n\n\nusername\n : \u00a0username for ftp server\n\n\npassword\n : password\n\n\nhost\n : FTP host\n\n\nport\n : port number\n\n\npath\n : path to either file / directory\n\n\n\n\n\n\nTo copy multiple files/directories, see below: \n\n\n\nTo copy multiple directories, see below:\n\n\n\n\n\n\n\nConfiguring Amazon S3 input\n\n\nFor details on Amazon Simple Storage Service (S3), please go to \nAmazon S3\nDocumentation\n. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0S3 as input type\n\n\n\n\n\n\n\nAfter selecting the S3 as source type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure S3 input url.\n\n\nInput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n\nwhere,\n- \ns3n\n: \u00a0protocol name\n- \nukey\n: access key\n- \nupass\n: secret access key\n- \nbucketName\n : bucketName\n- \npath\n : path to either file / directory\n\n\n\n\n\n\n\n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n  \n\n\nConfiguring Kafka input\n\n\nFor more details on Kafka, please refer to \nApache Kafka\nDocumentation\n.\n\n\nThis section gives details about how to ingest messages from Kafka using dtIngest.\n\n\n\n\n\n\nSelect Kafka as input type\n    \n\n\n\n\n\n\nAfter selecting Kafka as input type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n    \nhostname1:port1,hostname2:port2,hostname3:port3\n\n\nwhere,\n\n\n\n\nhostname1,hostname2,hostname3\n are hosts\n\n\nport1,port2,port3\n are ports of zookeeper server\n\n\n\n\ne.g. localhost:2181,localhost:2182\n\n\n\n\n\n\n\nSelect the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.\n\n\n\n\n\n\nIf the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:\n\n\nc1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere,\n- \nc1,c2,c3\n indicates the cluster names,\n- \nhs1,hs2,hs3,hs4,hs5,hs6\n are zookeeper host names\n- \np1,p2,p3,p4,p5,p6\n are corresponding ports.\n\n\nFor\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as \nClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181\n\n\n\n\n\n\n\n\nConfiguring JMS input\n\n\nThis section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0\n\n\n\n\n\n\nSelect JMS as input type.\n    \n\n\n\n\n\n\nAfter selecting the JMS as source type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://hostName:port\n    \n\n\n\n\n\n\nConfiguring Output Destination\n\n\nConfiguring HDFS output\n\n\n\n\n\n\nFor 'Output Location' field, select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Target directory' specify complete HDFS path URL of the destination directory. For example,   \nhdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory\n\n\n\n\n\n\n\n\nWhere,\n    - \nhdfs://\n indicates HDFS protocol\n    - \nnamenode1.cluster.company.org\n indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    - \n:8020\n indicates port number for HDFS namenode service\n    - \n/user/username/path/to/destination/directory\n indicates full path\n    for destination directory.\n\n\n\n\n\n\nUnder 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.\n\n\n\n\n\n\nUnder 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.\n\n\n\n\n\n\nCompact files\n\n\nUse 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.\n\n\n\n\n\n\n\n\nSelect 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.\n\n\n\n\n\n\nSelect delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with \n\\\n. For example, tab character\n    \n\\t\n should be specified as \n\\\\t\n.\n\n\n\n\n\n\nSpecify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.\n\n\n\n\n\n\nNote that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.\n\n\nConfiguring NFS output\n\n\n\n\n\n\nFor \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down  \n\n\n\n\n\n\n\n\nUnder \u2018Target directory\u2019 specify complete NFS path URL of the destination directory.\n\n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n    \nfile:///disk5/nfsmount/path/to/data/directory\n\n\nWhere,\n\n\n\n\nfile://\n indicates that it is some file system mounted on the node.\n\n\n/disk5/nfsmount/\n indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.\n\n\npath/to/data/directory\n is the directory to be ingested\n\n\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nConfiguring FTP output\n\n\n\n\n\n\nSelect FTP as output type.\n    \n\n\n\n\n\n\nAfter selecting FTP as output type then UI looks like as below:   \n\n\n\n\n\n\nSpecify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows: \nftp://username:password@host:port/path\n\n\nWhere,\n- \nftp\n : \u00a0protocol name\n- \nusername\n : username for ftp server\n- \npassword\n : password\n- \nhost\n : FTP host\n- \nport\n : port number\n- \npath\n : Directory path to ingested\n\n\n\n\n\n\n\n\nConfiguring Amazon S3 output\n\n\n\n\n\n\nSelect S3 as output type.\n    \n\n\n\n\n\n\nAfter selecting S3 as output then UI looks like as below:\n    \n\n\n\n\n\n\nSpecify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows: \ns3n://ukey:upass@bucketName/path\n\n    Where,\n\n\n\n\ns3n\n\u00a0: protocol name\n\n\nukey\n : access key\n\n\nupass\n :\u00a0secret access key\n\n\nbucketName\n : \u00a0bucketName\n\n\npath\n : Directory path\n\n\n\n\n\n\n\n\nConfiguring Kafka output\n\n\n\n\n\n\nSelect Kafka as output type.\n    \n\n\n\n\n\n\nAfter selecting Kafka as output then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure broker list and topic name.\n\n\n\n\n\n\nConfiguring JMS output\n\n\n\n\n\n\nSelect JMS as output type.\n\n\n\n\n\n\n\nAfter selecting JMS as output type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://host:port\n\n\n\n\n\n\n\nConfiguring Processing Steps\n\n\nConfiguring compression\n\n\nSelect compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\nSelect LZO radio button to apply LZO compression\n\n\n\n\n\n\nLzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/\n\n\n\n\nSelect GZIP radio button to apply GZIP compression\n\n\n\n\n\nConfiguring encryption\n\n\nSelect encryption type on configuration page.\n\n\n\n\n\nApply AES encryption:\n\n\n\n\nSelect AES radio button to apply AES encryption\n\n\n\n\n\n\nProvide AES symmetric encryption key in \u201cAES key\u201d text box\n\n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.\n\n\n\n\n\n\n\nApply PKI encryption:  \n\n\n\n\nSelect PKI encryption button to apply PKI encryption  \n\n\nProvide Asymmetric public key to be used for PKI encryption", 
            "title": "dtIngest"
        }, 
        {
            "location": "/dtingest/#dtingest-tutorial", 
            "text": "\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the  dtIngest\nblog .  This tutorial refers to dtIngest version 1.0.0", 
            "title": "dtIngest Tutorial"
        }, 
        {
            "location": "/dtingest/#pre-requisites", 
            "text": "Datatorrent RTS on a Hadoop cluster. Please\n    refer to  Installation\n    guide \u00a0for\n    details.    Source and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.    If source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/dtingest/#launching-dtingest", 
            "text": "dtIngest application can be configured and launched from  Datatorrent\nManagement\nConsole .    Navigate to 'Develop' tab.\n        The dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0    Select 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n        Configuration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.", 
            "title": "Launching dtIngest"
        }, 
        {
            "location": "/dtingest/#configuring-dtingest-instance-properties", 
            "text": "In the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n        Leave 'Specify a queue' unchecked to use default queue.  If you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to  Hadoop Capacity Scheduler Docs     Under 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n      Once it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one.   Leave 'Use a config file' unchecked to create a new one.     Configure input source, refer to  Configuring input\n    source \u00a0section for details.    Configure output destination, refer to  Configuring output\n    destination \u00a0section for details.    Configure processing steps, refer to  Configuring processing\n    steps \u00a0section for details.    Under 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.", 
            "title": "Configuring dtIngest Instance Properties"
        }, 
        {
            "location": "/dtingest/#configuring-input-source", 
            "text": "", 
            "title": "Configuring Input Source"
        }, 
        {
            "location": "/dtingest/#configuring-hdfs-input", 
            "text": "For 'Input data source' field; select 'HDFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested. \n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n     hdfs://namenode1.cluster.company.org:8020/user/john/data . \n    Where,   hdfs://  indicates HDFS protocol  namenode1.cluster.company.org  indicates fully qualified domain\n   name for the namenode of source HDFS.  8020  indicates port number for HDFS namenode service  /user/john/data  indicates full path for destination directory      If there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.    In the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n     Java regular expression  documentation .\n     For example, if only  .log  files need to be ingested, then use  .*\\.log  as regular expression.     Where,\n-    .*  indicates any character zero or more times\n-    \\.  indicates dot escaped with backslash\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only '.log' files from the source\ndirectories.    Under 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.  If 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between subsequent scans for detecting\nnew/modified files.", 
            "title": "Configuring HDFS input"
        }, 
        {
            "location": "/dtingest/#configuring-nfs-input", 
            "text": "For 'Input data source' field; select 'File/NFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL file path.  For example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk5/nfsmount/path/to/data/directory .\nWhere,\n-    file://  indicates that it is some file system mounted on the node.\n-    /disk5/nfsmount/  indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-    path/to/data/directory  is the directory to be ingested   Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    If there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.    If there are specific files, as opposed to a directory,specify complete\n    URL file path.  For example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt .     In the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.   Where,\n-    .\\*  indicates any character zero or more times\n-    \\\\.  indicates dot escaped with backslash '\\'\n-    log  indicates desired extension which is 'log'  Therefore, dtingest ingests only  .log  files from the source directories.    Under 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.", 
            "title": "Configuring\u00a0NFS input"
        }, 
        {
            "location": "/dtingest/#configuring-ftp-input", 
            "text": "This section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0    Select \u00a0FTP as input type\n        After selecting the FTP as input type, snapshot of UI as below:         The format for FTP URL input is as follows:   ftp://username:password@host:port/path \n    where,   ftp  : \u00a0protocol name  username  : \u00a0username for ftp server  password  : password  host  : FTP host  port  : port number  path  : path to either file / directory    To copy multiple files/directories, see below:   To copy multiple directories, see below:", 
            "title": "Configuring FTP input"
        }, 
        {
            "location": "/dtingest/#configuring-amazon-s3-input", 
            "text": "For details on Amazon Simple Storage Service (S3), please go to  Amazon S3\nDocumentation . This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0    Select \u00a0S3 as input type    After selecting the S3 as source type then UI looks like as below:    Configure S3 input url.  Input url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path  where,\n-  s3n : \u00a0protocol name\n-  ukey : access key\n-  upass : secret access key\n-  bucketName  : bucketName\n-  path  : path to either file / directory    \n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:", 
            "title": "Configuring Amazon S3 input"
        }, 
        {
            "location": "/dtingest/#configuring-kafka-input", 
            "text": "For more details on Kafka, please refer to  Apache Kafka\nDocumentation .  This section gives details about how to ingest messages from Kafka using dtIngest.    Select Kafka as input type\n        After selecting Kafka as input type then UI looks like as below:\n        Configure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n     hostname1:port1,hostname2:port2,hostname3:port3  where,   hostname1,hostname2,hostname3  are hosts  port1,port2,port3  are ports of zookeeper server   e.g. localhost:2181,localhost:2182    Select the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.    If the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:  c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where,\n-  c1,c2,c3  indicates the cluster names,\n-  hs1,hs2,hs3,hs4,hs5,hs6  are zookeeper host names\n-  p1,p2,p3,p4,p5,p6  are corresponding ports.  For\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as  ClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181", 
            "title": "Configuring Kafka input"
        }, 
        {
            "location": "/dtingest/#configuring-jms-input", 
            "text": "This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0    Select JMS as input type.\n        After selecting the JMS as source type then UI looks like as below:\n        Configure Broker URL and topic name as tcp://hostName:port", 
            "title": "Configuring JMS input"
        }, 
        {
            "location": "/dtingest/#configuring-output-destination", 
            "text": "", 
            "title": "Configuring Output Destination"
        }, 
        {
            "location": "/dtingest/#configuring-hdfs-output", 
            "text": "For 'Output Location' field, select 'HDFS' option from the\n    drop-down.     Under 'Target directory' specify complete HDFS path URL of the destination directory. For example,    hdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory     Where,\n    -  hdfs://  indicates HDFS protocol\n    -  namenode1.cluster.company.org  indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    -  :8020  indicates port number for HDFS namenode service\n    -  /user/username/path/to/destination/directory  indicates full path\n    for destination directory.    Under 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.    Under 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.", 
            "title": "Configuring HDFS output"
        }, 
        {
            "location": "/dtingest/#compact-files", 
            "text": "Use 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.     Select 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.    Select delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with  \\ . For example, tab character\n     \\t  should be specified as  \\\\t .    Specify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.    Note that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.", 
            "title": "Compact files"
        }, 
        {
            "location": "/dtingest/#configuring-nfs-output", 
            "text": "For \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down       Under \u2018Target directory\u2019 specify complete NFS path URL of the destination directory. \n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n     file:///disk5/nfsmount/path/to/data/directory  Where,   file://  indicates that it is some file system mounted on the node.  /disk5/nfsmount/  indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.  path/to/data/directory  is the directory to be ingested    Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .", 
            "title": "Configuring NFS output"
        }, 
        {
            "location": "/dtingest/#configuring-ftp-output", 
            "text": "Select FTP as output type.\n        After selecting FTP as output type then UI looks like as below:       Specify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows:  ftp://username:password@host:port/path  Where,\n-  ftp  : \u00a0protocol name\n-  username  : username for ftp server\n-  password  : password\n-  host  : FTP host\n-  port  : port number\n-  path  : Directory path to ingested", 
            "title": "Configuring FTP output"
        }, 
        {
            "location": "/dtingest/#configuring-amazon-s3-output", 
            "text": "Select S3 as output type.\n        After selecting S3 as output then UI looks like as below:\n        Specify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows:  s3n://ukey:upass@bucketName/path \n    Where,   s3n \u00a0: protocol name  ukey  : access key  upass  :\u00a0secret access key  bucketName  : \u00a0bucketName  path  : Directory path", 
            "title": "Configuring Amazon S3 output"
        }, 
        {
            "location": "/dtingest/#configuring-kafka-output", 
            "text": "Select Kafka as output type.\n        After selecting Kafka as output then UI looks like as below:\n        Configure broker list and topic name.", 
            "title": "Configuring Kafka output"
        }, 
        {
            "location": "/dtingest/#configuring-jms-output", 
            "text": "Select JMS as output type.    After selecting JMS as output type then UI looks like as below:    Configure Broker URL and topic name as tcp://host:port", 
            "title": "Configuring JMS output"
        }, 
        {
            "location": "/dtingest/#configuring-processing-steps", 
            "text": "", 
            "title": "Configuring Processing Steps"
        }, 
        {
            "location": "/dtingest/#configuring-compression", 
            "text": "Select compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Select LZO radio button to apply LZO compression    Lzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/   Select GZIP radio button to apply GZIP compression", 
            "title": "Configuring compression"
        }, 
        {
            "location": "/dtingest/#configuring-encryption", 
            "text": "Select encryption type on configuration page.   Apply AES encryption:   Select AES radio button to apply AES encryption    Provide AES symmetric encryption key in \u201cAES key\u201d text box \n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.    Apply PKI encryption:     Select PKI encryption button to apply PKI encryption    Provide Asymmetric public key to be used for PKI encryption", 
            "title": "Configuring encryption"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application administration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application administration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/application_configurations/", 
            "text": "Application Configurations\n\n\nApplication Configurations can be thought of as wrappers for Applications. You\ncreate a Configuration with properties that override and supplement the ones\nalready set in the original Application.\n\n\nYou may find Application Configurations to be particularly useful if:\n\n\n\n\nYou use the \nSpecify Launch Properties\n option when launching Applications\n  and want the properties to persist.\n\n\nUsers want to share their Configurations of the same Application.\n\n\n\n\nCreating an Application Configuration\n\n\n\n\nNavigate to either the \nApplications\n page or the \nApplication Configurations\n\n  page in the \nDevelop\n section.\n\n\nIf you're on the  \nApplications\n page, click the \nconfigure\n button \n  next to the Application you want to configure, and then click \ncreate new configuration\n.\n    \n\n  Or if you're on the \nApplication Configurations\n page, click on the \ncreate new\n button.\n    \n\n\nSelect the \nSource Application\n from the dropdown (preselected if you used\n  the \nconfigure\n button), choose a unique \nConfiguration Name\n, and click the \ncreate\n button.\n    \n\n\nOnce the Configuration is created, you will be navigated to the newly created\n  Application Configuration's page.\n\n\n\n\nModifying Properties of an Application Configuration\n\n\nProperties set in an Application Configuration will override any existing properties\nof the Application. These properties can be Operator level properties or even\nApplication Package level. To see a list of existing properties, go to the\nApplication Configuration's page.\n\n\n\n\n\n\nPackage Properties\n: Shows the existing properties of the Application. This table\n  is useful for reference when specifying \nOptional Properties\n.\n\n\nOptional Properties\n: This is where you can add/edit/remove Application property\n  overrides. The properties and values you specify here will take precedence over the\n  existing \nPackage Properties\n. The JSON tab allows you to copy-and-paste properties\n  in JSON format.\n\n\n\n\n\n\nNOTE\n: Package Properties can be copied into your clipboard by double-clicking the \nname\n\nfield of the property. You can then paste it when creating Optional Properties.\n\n\nWhen you have finished modifying your properties, remember to click the \nsave\n\nbutton in the top-right corner of the page.\n\n\nLaunching an Application Configuration\n\n\nApplication Configurations have to be launched just like Applications. \nThey can be launched from the list view (table), or from the Configuration's\npage itself:\n\n\n\n\nExporting and Importing Application Configurations\n\n\nApplication Configurations can be exported and downloaded as \n.apc\n files.\n\n\n\n\nExported \n.apc\n files can be imported by uploading them on the Application\nConfigurations page.\n\n\n\n\nJSON Application Configurations\n\n\nApplication Configurations of JSON Applications (dtAssemble) differ from\nApplication Configurations of Java Applications in several ways:\n\n\n\n\nWhen creating a Configuration of a JSON Application, the entire\n  JSON Application is copied into the Configuration.\n\n\nThe original JSON Application and its Configurations are decoupled. Any modifications\n  to either would not affect the other. They still rely on the same Package resources\n  (e.g. Operators).\n\n\n\n\nDeleting Applications with Existing Application Configurations\n\n\nIf an Application with existing Application Configurations is deleted, the\nApplication Configuration can no longer be launched.\n\n\n\n\nThe launch functionality will be restored if you re-upload the Application.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/application_configurations/#application-configurations", 
            "text": "Application Configurations can be thought of as wrappers for Applications. You\ncreate a Configuration with properties that override and supplement the ones\nalready set in the original Application.  You may find Application Configurations to be particularly useful if:   You use the  Specify Launch Properties  option when launching Applications\n  and want the properties to persist.  Users want to share their Configurations of the same Application.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/application_configurations/#creating-an-application-configuration", 
            "text": "Navigate to either the  Applications  page or the  Application Configurations \n  page in the  Develop  section.  If you're on the   Applications  page, click the  configure  button \n  next to the Application you want to configure, and then click  create new configuration .\n     \n  Or if you're on the  Application Configurations  page, click on the  create new  button.\n      Select the  Source Application  from the dropdown (preselected if you used\n  the  configure  button), choose a unique  Configuration Name , and click the  create  button.\n      Once the Configuration is created, you will be navigated to the newly created\n  Application Configuration's page.", 
            "title": "Creating an Application Configuration"
        }, 
        {
            "location": "/application_configurations/#modifying-properties-of-an-application-configuration", 
            "text": "Properties set in an Application Configuration will override any existing properties\nof the Application. These properties can be Operator level properties or even\nApplication Package level. To see a list of existing properties, go to the\nApplication Configuration's page.    Package Properties : Shows the existing properties of the Application. This table\n  is useful for reference when specifying  Optional Properties .  Optional Properties : This is where you can add/edit/remove Application property\n  overrides. The properties and values you specify here will take precedence over the\n  existing  Package Properties . The JSON tab allows you to copy-and-paste properties\n  in JSON format.    NOTE : Package Properties can be copied into your clipboard by double-clicking the  name \nfield of the property. You can then paste it when creating Optional Properties.  When you have finished modifying your properties, remember to click the  save \nbutton in the top-right corner of the page.", 
            "title": "Modifying Properties of an Application Configuration"
        }, 
        {
            "location": "/application_configurations/#launching-an-application-configuration", 
            "text": "Application Configurations have to be launched just like Applications. \nThey can be launched from the list view (table), or from the Configuration's\npage itself:", 
            "title": "Launching an Application Configuration"
        }, 
        {
            "location": "/application_configurations/#exporting-and-importing-application-configurations", 
            "text": "Application Configurations can be exported and downloaded as  .apc  files.   Exported  .apc  files can be imported by uploading them on the Application\nConfigurations page.", 
            "title": "Exporting and Importing Application Configurations"
        }, 
        {
            "location": "/application_configurations/#json-application-configurations", 
            "text": "Application Configurations of JSON Applications (dtAssemble) differ from\nApplication Configurations of Java Applications in several ways:   When creating a Configuration of a JSON Application, the entire\n  JSON Application is copied into the Configuration.  The original JSON Application and its Configurations are decoupled. Any modifications\n  to either would not affect the other. They still rely on the same Package resources\n  (e.g. Operators).", 
            "title": "JSON Application Configurations"
        }, 
        {
            "location": "/application_configurations/#deleting-applications-with-existing-application-configurations", 
            "text": "If an Application with existing Application Configurations is deleted, the\nApplication Configuration can no longer be launched.   The launch functionality will be restored if you re-upload the Application.", 
            "title": "Deleting Applications with Existing Application Configurations"
        }, 
        {
            "location": "/dtmanage/", 
            "text": "dtManage Guide\n\n\nThe DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and manage the DataTorrent RTS platform and applications running on your Hadoop cluster.\n\n\nTo download the platform or the VM sandbox, go to \nhttp://www.datatorrent.com/download\n.\n\n\n\n\nThe Console includes the following features:\n\n\n\n\nAppFactory\n\n\nLaunch\n\n\nMonitor\n\n\nVisualize\n\n\nDevelop\n\n\nConfigure\n\n\n\n\nAppFactory\n\n\nThe AppFactory hosts a collection of applications and templates grouped by various industries, that can be imported or downloaded (as .apa files). You can use the applications as they are, or use the templates as a starting point to develop custom applications.\n\n\n\n\nLaunch\n\n\nThe Launch screen lists all of the imported and uploaded packages and their applications. Applications can be launched and active instances viewed.\n\n\n\n\nLaunching Apps\n\n\nTo launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options: \n\n\n\n\nSpecify a name for the running app\n\n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Make sure it is unique among all the applications running in the Hadoop cluster of your DataTorrent installation.\n\n\nSpecify launch properties\n\n  In addition to choosing a config file, you may also directly specify properties in the launch pop-up by selecting this option. Any required properties will automatically show up in this section and require input. Note that there are several helpful functions when specifying custom properties:\n\n\nadd\n - App Packages can have custom properties applied at launch time to override existing properties.\n\n\nadd default properties\n - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults. This button can be found clicking on the \nadd\n button's submenu.\n  If any properties were added, the option to save the properties as a Configuration Package is activated.\n\n\n\n\n\n\nUse configuration file\n\n  App Package config files are xml files that contain \nproperties\n that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.\n\n\nUse configuration package\n\n  App Packages with a separate associated Configuration Package can be selected here. The Configuration Package will launch with its own custom properties.\n\n\nSpecify the \nscheduler queue\n\n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be \nroot.[USER_NAME]\n.\n\n\nEnable \nGarbage Collection logging\n\n  Checking this box enables GC logging by including the \n-Xloggc:\nLOG_DIR\n/gc.log -verbose:gc -XX:+PrintGCDateStamps\n JVM options for all the containers of the application. This GC logging in turn enables garbage collection widgets described later.\n\n\n\n\n\n\n\n\nNote:\n For more information about config files and custom properties, see the \nApplication Packages Guide\n\n\n\n\nVisualize\n\n\nSee the \ndtDashboard\n page.\n\n\nMonitor\n\n\nThe Monitor section of the Console can be used to monitor, troubleshoot, and manage running application instances.\n\n\nCluster Overview\n\n\nThe Cluster Overview page shows overall cluster statistics as well as a list of running DataTorrent applications.\n\n\n\n\nThe cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of: \nended apps\n and \nsystem apps\n. The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.\n\n\nInstance Page\n\n\nTo get to an application instance page, click on either the app name or the app id in the list of running applications.\n\n\n\n\nAll sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:\n\n\n\n\nThere are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations (\nlogical\n, \nphysical\n, \nphysical-dag-view\n, \nmetric-view\n, \nattempts\n) will suffice. The following is a list of widgets available on an app instance page:\n\n\nApplication Overview Widget\n\n\nAll the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:\n\n\n\n\nThe \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.\n\n\nYou can also use the \nset logging level\n button on this widget to specify what logging level gets written to the dt.log files. \n\n\n\n\nYou will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:\n\n\n\n\nStram Events Widget\n\n\nEach application has a stream of notable events that can be viewed with the StrAM Events widget:\n\n\n\n\nSome events have additional information attached to it, which can be viewed by clicking the \"i\" icon in the list:\n\n\n\n\nLogical DAG Widget\n\n\nThis widget visualizes the logical plan of the application being viewed:\n\n\n\n\nAdditionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown. \n\n\n\n\nPro tip:\n Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.\n\n\n\n\nPhysical DAG Widget\n\n\nThis is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.\n\n\n\n\nSame-colored physical operators in this widget indicates that these operators are in the same container.\n\n\nLogical Operators List Widget\n\n\nThis widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing. \n\n\nOne nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:\n\n\n\n\nPhysical Operators List Widget\n\n\nShows the physical operators in the application.\n\n\nContainers List Widget\n\n\nShows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.\n\n\nLogical Streams List Widget\n\n\nShows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.\n\n\nMetrics Chart\n\n\nShows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.\n\n\nGarbage Collection (GC) Chart by Heap\n\n\nThis chart shows a container's heap memory in use in KB (kilo-bytes) against time. The chart is constructed by plotting and extrapolating in-use heap memory obtained from events in the GC log file of a container which requires GC logging to be enabled as described in \nLaunching apps\n. The chart shown is for a single container that is selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nGarbage Collection (GC) Log Table\n\n\nThis table shows the garbage collection (GC) events for a group of containers. This table too requires GC logging to be enabled as described in \nLaunching apps\n. The containers included in the group depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nGarbage Collection (GC) Chart by Duration\n\n\nThis discrete bar chart shows GC event duration in seconds against time for a group of containers. Each bar is of fixed-width but the height denotes the duration of the corresponding GC event. This chart too requires GC logging to be enabled as described in \nLaunching apps\n. One or more containers are selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nRecording and Viewing Sample Tuples\n\n\nThere is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):\n\n\n\n\n\n\nPro tip:\n Select multiple tuples by holding down the shift key.\n\n\n\n\nViewing Logs\n\n\nAnother useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:\n\n\n\n\nOnce you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, \"tail\" the log to watch for real-time updates, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:\n\n\n\n\nDevelop\n\n\nApplication packages and application configurations can be viewed and managed in the Develop section. For more information about application packages visit the \nApplication Packages Guide\n.\n\n\n\n\nApplication Packages\n\n\nTo access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operations directly on application packages:\n\n\n\n\nDownload the app package\n\n\nDelete the app package\n\n\nCreate a new application in an application package via dtAssemble (requires enterprise license)\n\n\nLaunch applications in the app package\n\n\n\n\n\n\nNote:\n If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.\n\n\n\n\nApplication Package Page\n\n\nOnce you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.\n\n\n\n\nAside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package. \n\n\nViewing an Application\n\n\nAll DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications. In addition to the DAG, Package Properties and any Required Properties will be listed on this page.\n\n\n\n\nCreating apps with dtAssemble\n\n\nIf you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the \ndtAssemble guide\n.\n\n\nConfigure\n\n\nThe RTS configuration menu is accessed by the cog button on the top-right corner of the Console. Under the \nconfiguration\n section, there are links to various tools to help you configure and troubleshoot your DataTorrent installation. The available menu items may differ depending on your security settings.\n\n\n\n\nSystem Configuration\n\n\nThis page shows the system configuration, provides a way to make system changes, and displays any known issues for the DataTorrent RTS installation.\n\n\n\n\nIn addition, you can perform the following actions from this page:\n\n\n\n\nSMTP Configuration - Set up SMTP to be able to send out email alerts and notifications.\n\n\nRestart the Gateway - This button can be used to restart the gateway when the Hadoop configuration or system properties have changed.\n\n\nUsage Reporting - If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.\n\n\nInstallation Wizard - Rerun the initial installation to reconfigure HDFS installation path and Hadoop executable.\n\n\n\n\nSecurity Configuration\n\n\nBy default, your installation starts with no security enabled, which may be sufficient on a closed network with a limited set of users. However, it is recommended to use some form of authentication especially for production environments.\n\n\n\n\nDataTorrent RTS supports various authentication methods which can be enabled by following instructions in the \nAuthentication\n section.\n\n\nSystem Alerts\n\n\nSystem alerts can be configured to notify users through the Console and emails based on various system and application metrics.\n\n\n\n\nClick on the \n+ create new alert\n button to create an alert.\n\n\n\n\nAn alert consists of\n\n\n\n\na condition (a JavaScript expression)\n\n\na list of recipient email addresses\n\n\na threshold value in milliseconds\n\n\na message, and\n\n\nan enabled/disabled flag\n\n\n\n\nThe gateway periodically (every 5 seconds) processes all enabled alerts by evaluating the condition. If the condition evaluates to \ntrue\n, the alert is said to be \"in effect\".\nIf the condition evaluates to \nfalse\n, the alert is said to be \"out\" (or \"out of effect\"). If the alert stays \"in effect\" for the duration specified as the threshold value,\nthen the alert is triggered and the gateway sends an \"in effect\" email message to all the recipient email addresses.\n\n\nIf a triggered alert goes \"out of effect\" then the gateway immediately sends an \"out of effect\" email message to all the recipient email addresses.\n\n\nThe alert condition is specified as a JavaScript expression which is evaluated in the context of something called \ntopics\n which are described \nhere\n.\n\n\nThe gateway also provides pre-defined alert \"templates\" that allow a user to create alerts for certain common conditions without having to write JavaScript expressions.\n\n\n\n\nClick on the \"Predefined Conditions\" tab and select a template from the drop-down list. Depending on your selection, you will need to provide more values to be filled into the template.\nAs an example, for the \"Application Memory Usage\" template you need to provide the Application Name and Memory values as shown below:\n\n\n\n\nYou can click on the \"Javascript Code\" tab to see the generated JavaScript expression that corresponds to your alert template selection and provided values as shown below:\n\n\n\n\nYou can generate a test email to validate your alert by checking the \"Send Test Email\" check-box and clicking on the blue \"Test\" button. The test email is sent regardless of the true or false result\nof the JavaScript condition, if the evaluation has no errors provided SMTP is configured as described in the Alerts section.\n\n\nLicense Information\n\n\nUse the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.\n\n\n\n\nUser Profile\n\n\nThe User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:\n\n\n\n\nChange password \n\n\nChange the default home page\n\n\nChange the theme of the console\n\n\nRestore the default options of the console\n\n\n\n\n\n\nUser Management\n\n\nUse this page to manage users and roles of your DataTorrent cluster:\n\n\n\n\nAdd users\n\n\nChange users\u2019 roles\n\n\nChange users\u2019 password\n\n\nDelete users\n\n\nAdd roles\n\n\nEdit role permissions\n\n\nDelete roles\n\n\n\n\n\n\n\n\nNote:\n With most authentication schemes, the admin role cannot be deleted.\n\n\n\n\nInstallation Wizard\n\n\nThe first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:\n\n\n\n\nLocation of the Hadoop executable\n\n\nDFS location where all the DataTorrent files are stored\n\n\nDataTorrent license\n\n\nSummary and review of any remaining configuration items\n\n\n\n\nAt any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.\n\n\n\n\nWhen your Hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard: \n\n\n\n\nKerberos Principal\n: The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.\n\n\nKerberos Keytab\n: The location (path) of the Kerberos keytab file to use on the gateway node's local file system.\n\n\nYARN delegation token lifetime\n: If the value of the \nyarn.resourcemanager.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\nNamenode delegation token lifetime\n: If the value of the \ndfs.namenode.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\n\n\n\n\nNote:\n The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.", 
            "title": "dtManage"
        }, 
        {
            "location": "/dtmanage/#dtmanage-guide", 
            "text": "The DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and manage the DataTorrent RTS platform and applications running on your Hadoop cluster.  To download the platform or the VM sandbox, go to  http://www.datatorrent.com/download .   The Console includes the following features:   AppFactory  Launch  Monitor  Visualize  Develop  Configure", 
            "title": "dtManage Guide"
        }, 
        {
            "location": "/dtmanage/#appfactory", 
            "text": "The AppFactory hosts a collection of applications and templates grouped by various industries, that can be imported or downloaded (as .apa files). You can use the applications as they are, or use the templates as a starting point to develop custom applications.", 
            "title": "AppFactory"
        }, 
        {
            "location": "/dtmanage/#launch", 
            "text": "The Launch screen lists all of the imported and uploaded packages and their applications. Applications can be launched and active instances viewed.", 
            "title": "Launch"
        }, 
        {
            "location": "/dtmanage/#launching-apps", 
            "text": "To launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options:    Specify a name for the running app \n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Make sure it is unique among all the applications running in the Hadoop cluster of your DataTorrent installation.  Specify launch properties \n  In addition to choosing a config file, you may also directly specify properties in the launch pop-up by selecting this option. Any required properties will automatically show up in this section and require input. Note that there are several helpful functions when specifying custom properties:  add  - App Packages can have custom properties applied at launch time to override existing properties.  add default properties  - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults. This button can be found clicking on the  add  button's submenu.\n  If any properties were added, the option to save the properties as a Configuration Package is activated.    Use configuration file \n  App Package config files are xml files that contain  properties  that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.  Use configuration package \n  App Packages with a separate associated Configuration Package can be selected here. The Configuration Package will launch with its own custom properties.  Specify the  scheduler queue \n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be  root.[USER_NAME] .  Enable  Garbage Collection logging \n  Checking this box enables GC logging by including the  -Xloggc: LOG_DIR /gc.log -verbose:gc -XX:+PrintGCDateStamps  JVM options for all the containers of the application. This GC logging in turn enables garbage collection widgets described later.     Note:  For more information about config files and custom properties, see the  Application Packages Guide", 
            "title": "Launching Apps"
        }, 
        {
            "location": "/dtmanage/#visualize", 
            "text": "See the  dtDashboard  page.", 
            "title": "Visualize"
        }, 
        {
            "location": "/dtmanage/#monitor", 
            "text": "The Monitor section of the Console can be used to monitor, troubleshoot, and manage running application instances.", 
            "title": "Monitor"
        }, 
        {
            "location": "/dtmanage/#cluster-overview", 
            "text": "The Cluster Overview page shows overall cluster statistics as well as a list of running DataTorrent applications.   The cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of:  ended apps  and  system apps . The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.", 
            "title": "Cluster Overview"
        }, 
        {
            "location": "/dtmanage/#instance-page", 
            "text": "To get to an application instance page, click on either the app name or the app id in the list of running applications.   All sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:   There are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations ( logical ,  physical ,  physical-dag-view ,  metric-view ,  attempts ) will suffice. The following is a list of widgets available on an app instance page:", 
            "title": "Instance Page"
        }, 
        {
            "location": "/dtmanage/#application-overview-widget", 
            "text": "All the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:   The \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.  You can also use the  set logging level  button on this widget to specify what logging level gets written to the dt.log files.    You will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:", 
            "title": "Application Overview Widget"
        }, 
        {
            "location": "/dtmanage/#stram-events-widget", 
            "text": "Each application has a stream of notable events that can be viewed with the StrAM Events widget:   Some events have additional information attached to it, which can be viewed by clicking the \"i\" icon in the list:", 
            "title": "Stram Events Widget"
        }, 
        {
            "location": "/dtmanage/#logical-dag-widget", 
            "text": "This widget visualizes the logical plan of the application being viewed:   Additionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown.    Pro tip:  Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.", 
            "title": "Logical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#physical-dag-widget", 
            "text": "This is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.   Same-colored physical operators in this widget indicates that these operators are in the same container.", 
            "title": "Physical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#logical-operators-list-widget", 
            "text": "This widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing.   One nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:", 
            "title": "Logical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#physical-operators-list-widget", 
            "text": "Shows the physical operators in the application.", 
            "title": "Physical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#containers-list-widget", 
            "text": "Shows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.", 
            "title": "Containers List Widget"
        }, 
        {
            "location": "/dtmanage/#logical-streams-list-widget", 
            "text": "Shows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.", 
            "title": "Logical Streams List Widget"
        }, 
        {
            "location": "/dtmanage/#metrics-chart", 
            "text": "Shows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.", 
            "title": "Metrics Chart"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-chart-by-heap", 
            "text": "This chart shows a container's heap memory in use in KB (kilo-bytes) against time. The chart is constructed by plotting and extrapolating in-use heap memory obtained from events in the GC log file of a container which requires GC logging to be enabled as described in  Launching apps . The chart shown is for a single container that is selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Chart by Heap"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-log-table", 
            "text": "This table shows the garbage collection (GC) events for a group of containers. This table too requires GC logging to be enabled as described in  Launching apps . The containers included in the group depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Log Table"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-chart-by-duration", 
            "text": "This discrete bar chart shows GC event duration in seconds against time for a group of containers. Each bar is of fixed-width but the height denotes the duration of the corresponding GC event. This chart too requires GC logging to be enabled as described in  Launching apps . One or more containers are selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Chart by Duration"
        }, 
        {
            "location": "/dtmanage/#recording-and-viewing-sample-tuples", 
            "text": "There is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):    Pro tip:  Select multiple tuples by holding down the shift key.", 
            "title": "Recording and Viewing Sample Tuples"
        }, 
        {
            "location": "/dtmanage/#viewing-logs", 
            "text": "Another useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:   Once you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, \"tail\" the log to watch for real-time updates, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:", 
            "title": "Viewing Logs"
        }, 
        {
            "location": "/dtmanage/#develop", 
            "text": "Application packages and application configurations can be viewed and managed in the Develop section. For more information about application packages visit the  Application Packages Guide .", 
            "title": "Develop"
        }, 
        {
            "location": "/dtmanage/#application-packages", 
            "text": "To access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operations directly on application packages:   Download the app package  Delete the app package  Create a new application in an application package via dtAssemble (requires enterprise license)  Launch applications in the app package    Note:  If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/dtmanage/#application-package-page", 
            "text": "Once you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.   Aside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package.", 
            "title": "Application Package Page"
        }, 
        {
            "location": "/dtmanage/#viewing-an-application", 
            "text": "All DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications. In addition to the DAG, Package Properties and any Required Properties will be listed on this page.", 
            "title": "Viewing an Application"
        }, 
        {
            "location": "/dtmanage/#creating-apps-with-dtassemble", 
            "text": "If you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the  dtAssemble guide .", 
            "title": "Creating apps with dtAssemble"
        }, 
        {
            "location": "/dtmanage/#configure", 
            "text": "The RTS configuration menu is accessed by the cog button on the top-right corner of the Console. Under the  configuration  section, there are links to various tools to help you configure and troubleshoot your DataTorrent installation. The available menu items may differ depending on your security settings.", 
            "title": "Configure"
        }, 
        {
            "location": "/dtmanage/#system-configuration", 
            "text": "This page shows the system configuration, provides a way to make system changes, and displays any known issues for the DataTorrent RTS installation.   In addition, you can perform the following actions from this page:   SMTP Configuration - Set up SMTP to be able to send out email alerts and notifications.  Restart the Gateway - This button can be used to restart the gateway when the Hadoop configuration or system properties have changed.  Usage Reporting - If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.  Installation Wizard - Rerun the initial installation to reconfigure HDFS installation path and Hadoop executable.", 
            "title": "System Configuration"
        }, 
        {
            "location": "/dtmanage/#security-configuration", 
            "text": "By default, your installation starts with no security enabled, which may be sufficient on a closed network with a limited set of users. However, it is recommended to use some form of authentication especially for production environments.   DataTorrent RTS supports various authentication methods which can be enabled by following instructions in the  Authentication  section.", 
            "title": "Security Configuration"
        }, 
        {
            "location": "/dtmanage/#system-alerts", 
            "text": "System alerts can be configured to notify users through the Console and emails based on various system and application metrics.   Click on the  + create new alert  button to create an alert.   An alert consists of   a condition (a JavaScript expression)  a list of recipient email addresses  a threshold value in milliseconds  a message, and  an enabled/disabled flag   The gateway periodically (every 5 seconds) processes all enabled alerts by evaluating the condition. If the condition evaluates to  true , the alert is said to be \"in effect\".\nIf the condition evaluates to  false , the alert is said to be \"out\" (or \"out of effect\"). If the alert stays \"in effect\" for the duration specified as the threshold value,\nthen the alert is triggered and the gateway sends an \"in effect\" email message to all the recipient email addresses.  If a triggered alert goes \"out of effect\" then the gateway immediately sends an \"out of effect\" email message to all the recipient email addresses.  The alert condition is specified as a JavaScript expression which is evaluated in the context of something called  topics  which are described  here .  The gateway also provides pre-defined alert \"templates\" that allow a user to create alerts for certain common conditions without having to write JavaScript expressions.   Click on the \"Predefined Conditions\" tab and select a template from the drop-down list. Depending on your selection, you will need to provide more values to be filled into the template.\nAs an example, for the \"Application Memory Usage\" template you need to provide the Application Name and Memory values as shown below:   You can click on the \"Javascript Code\" tab to see the generated JavaScript expression that corresponds to your alert template selection and provided values as shown below:   You can generate a test email to validate your alert by checking the \"Send Test Email\" check-box and clicking on the blue \"Test\" button. The test email is sent regardless of the true or false result\nof the JavaScript condition, if the evaluation has no errors provided SMTP is configured as described in the Alerts section.", 
            "title": "System Alerts"
        }, 
        {
            "location": "/dtmanage/#license-information", 
            "text": "Use the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.", 
            "title": "License Information"
        }, 
        {
            "location": "/dtmanage/#user-profile", 
            "text": "The User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:   Change password   Change the default home page  Change the theme of the console  Restore the default options of the console", 
            "title": "User Profile"
        }, 
        {
            "location": "/dtmanage/#user-management", 
            "text": "Use this page to manage users and roles of your DataTorrent cluster:   Add users  Change users\u2019 roles  Change users\u2019 password  Delete users  Add roles  Edit role permissions  Delete roles     Note:  With most authentication schemes, the admin role cannot be deleted.", 
            "title": "User Management"
        }, 
        {
            "location": "/dtmanage/#installation-wizard", 
            "text": "The first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:   Location of the Hadoop executable  DFS location where all the DataTorrent files are stored  DataTorrent license  Summary and review of any remaining configuration items   At any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.   When your Hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard:    Kerberos Principal : The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.  Kerberos Keytab : The location (path) of the Kerberos keytab file to use on the gateway node's local file system.  YARN delegation token lifetime : If the value of the  yarn.resourcemanager.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.  Namenode delegation token lifetime : If the value of the  dfs.namenode.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.    Note:  The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the bottom panel will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the bottom panel. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\n\n\nWhen a port is selected, you will see the Port Inspector on the bottom panel. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).\n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201csave\u201d button in the top right.\n\n\nClick the \u201claunch\u201d button in the top right once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the bottom panel will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#operator-library-navigator", 
            "text": "Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.", 
            "title": "Operator Library Navigator"
        }, 
        {
            "location": "/dtassemble/#canvas", 
            "text": "The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.", 
            "title": "Canvas"
        }, 
        {
            "location": "/dtassemble/#inspector", 
            "text": "The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.", 
            "title": "Inspector"
        }, 
        {
            "location": "/dtassemble/#operator-inspector", 
            "text": "When an operator is selected on the canvas, you will see the Operator Inspector on the bottom panel. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.", 
            "title": "Operator Inspector"
        }, 
        {
            "location": "/dtassemble/#port-inspector", 
            "text": "When a port is selected, you will see the Port Inspector on the bottom panel. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).", 
            "title": "Port Inspector"
        }, 
        {
            "location": "/dtassemble/#stream-inspector", 
            "text": "The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Stream Inspector"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtassemble/#pi-demo", 
            "text": "As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201csave\u201d button in the top right.  Click the \u201claunch\u201d button in the top right once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "Pi Demo"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the \nvisualize\n button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The embedded Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators SnapshotServer (which includes embedded Query Operator) and QueryResult are the operators that serve the data being visualized in the Console.  The SnapshotServer operator takes in data from the TopCounter operator, processes incoming queries, and generates results. The Twitter Hashtag Demo application and all its operators are available in the Apache Malhar repository.\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that provide historical and real-time application statistics and custom metrics data.  You can visualize these metrics the same way as custom Data Sources in an application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from the Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, using the \nvisualize\n button will display a list of existing dashboards that are associated with the application.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\nYou can also navigate back to the application from within the dashboard's menu.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new\n button on the Dashboards screen\n\n\ncreate new dashboard\n option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the \ncreate new\n button\n\n\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with \ncreate new dashboard\n option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to create new dashboard from the visualize menu drop-down list.  The new dashboard will be automatically named based on the application name and saved.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nDashboard Controls\n\n\nDashboards controls are presented as a set of buttons to the right of the dashboard title.\n\n\nView Mode\n\n\nIn view mode, you are not allowed to modify the layout or add/edit widgets, but certain actions\ncan be taken in the dashboard menu.\n\n\n\n\nEdit Mode\n\n\nIn edit mode, you can add/edit widgets, change the dashboard layout, and change dashboard settings.\n\n\n\n\nDashboard Menu\n\n\nThe dashboard menu contains the rest of the dashboard functionality. The \nactions\n section is mostly self-explanatory, but see the \nPresentation Mode\n and \nPresentation Builder\n sections below for more information about presenting your dashboards. The \nassociated applications\n section displays the status of applications associated to the current dashboard and serves as a quick way to jump to those applications. Associated applications are selected based on the data sources of your widgets.\n\n\nNote\n: If an an associated application enters a non-running state (e.g. KILLED, INACTIVE), a warning icon will be displayed on the \ndashboard menu\n button.\n\n\n\n\nDashboard Settings\n\n\nThe dashboard settings interface allows you to change the dashboard name, description, logo image, and \nselect replacement associated applications\n.\n\n\nIt can be accessed from the \ndashboard menu\n using the \nsettings\n option.\n\n\n\n\nReplacing Associated Applications\n\n\nDashboards have widgets that rely on associated applications for data. These associated applications can be replaced in \ndashboard settings\n if the replacement is compatible with the current dashboard.\n\n\nThe replacement application must have a compatible data source. Selecting an application with an incompatible data source will simply skip the replacement process for that application. If the data source matches, but the data schema is incompatible, the widget will attempt to reset its settings to match the new data schema.\n\n\nWhen importing a dashboard, the interface tries its best to preselect the most compatible replacement application. Leaving fields blank means the replacement process for those applications will be skipped.\n\n\nPackaged Dashboards\n\n\nAuto Import When Launching An Application\n\n\nApplication packages may include packaged dashboards which can be imported. Application package developers may select some dashboards to be imported automatically when launching an application, and all packaged dashboards can be imported manually at any time from the Packaged Dashboards page.\n\n\n\n\nWhen launching an application, the \nImport Packaged Dashboards\n section will appear if there are packaged dashboards. Use the checkboxes to select which dashboards to import. The dashboard name and replacement applications can be changed (see \nreplacing associated applications\n).\n\n\nNote\n: Auto imports of packaged dashboards only happen if there isn't already an existing dashboard with the same name and owner. They can still be marked for import in the launch interface, but will have to be given a unique name.\n\n\nImport From A Running Application\n\n\nIf a running application has associated packaged dashboards, the packaged dashboards can be imported using the \nvisualize\n button in \nApplication Overview\n.\n\n\n\n\nClicking on a packaged dashboard in the dropdown will open an interface to change the dashboard name, description, logo image, and replacement applications before importing.\n\n\nNote\n: See the \nreplacing associated applications\n section for an explanation about replacement applications.\n\n\n\n\nPressing the \nimport\n button at the bottom of the interface imports the dashboard, and can then be accessed in the \nVisualize\n section.\n\n\nImport From Packaged Dashboards Page\n\n\nFor a list of all packaged dashboards across all your application packages, use the \nimport\n button in the \nVisualize\n section.\n\n\n\n\nThe \nimport\n button brings you to the following page where you can import or download individual packaged dashboards.\n\n\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking the \nadd widget\n button in edit mode, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget.\n\n\n\n\nEach data source supports one or more data schema types, such as snapshot and dimensions.  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nResults are not persisted until you press the \nsave\n button.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use \nsave\n button to persist the desired results.  If the resulting changes should not be saved, using the \ndiscard\n button will revert it to the the original state.\n\n\nControl Widget\n\n\nControl widget can be used to configure settings of one or more widgets on the dashboard.  This is a convenient way to configure multiple widgets settings in one place.  It is also the only way to change widgets settings in presentation mode.  However, settings changes in presentation mode will not be saved.\n\n\nControllable Widgets\n\n\nThe following widgets may be controlled by the Control Widget:\n\n\n\n\nDimensions Schema Data Source Widgets\n\n\nGeo Choropleth\n\n\nGeo Circles\n\n\nLine Chart\n\n\nMulti Bar Chart\n\n\nStacked Area Chart\n\n\n\n\n\n\nSnapshot Schema Data Source Widgets\n\n\nBar Chart\n\n\nHorizontal Bar Chart\n\n\nMulti Color Bar Chart\n\n\nPie Chart\n\n\n\n\n\n\n\n\nControllable Settings\n\n\nThe following settings may be controlled by the Control Widget:\n\n\n\n\nDimensions Schema Data Source Settings\n\n\nCircle Size \n(Geo Circle widget only)\n\n\nColor Intensity \n(Geo Choropleth widget only)\n\n\nGeo Coordinates \n(Geo Circle widget only)\n\n\nKeys Combinations and Aggregates\n\n\nTime Range Selection\n\n\nTooltip Values \n(Geo Choropleth and Circle widgets only)\n\n\n\n\n\n\nSnapshot Schema Data Source Settings\n\n\nField to use as label\n\n\nField to use as quantity\n\n\nSort order\n\n\n\n\n\n\n\n\nAdding Control Widget\n\n\nControl Widget can be added just like any other widget.  See the \nAdding Widgets\n section for more details.\n\n\nAdding Widgets to Control\n\n\nClick the edit button on the Control Widget to show the available widgets to control, and change the controllable widget selections.\n\n\n\n\nThe Cotnrol Widget does not allow selection of dimensional and snapshot schema widgets together. If users happen to choose widgets with both schema types, then the \"OK\" button remains disabled until only one schema type is selected.\n\n\nThe control widget allows selection of widgets with different data sources. Users are warned that some setting sections in the control widget may be disabled if the schemas are incompatible.\n\n\n\n\nWhen widgets to be controlled have incompatible schemas, the incompatible sections remain disabled until mismatched widgets are removed in the control widget settings.  This issue can also be resolved if the widgets with incompatible schemas are removed from the dashboard.\n\n\n\n\nExamples of Control Widget in Action\n\n\nChanging the time range selection for the line and stacked area charts.\n\n\n\n\nChanging dimensional keys and aggregates.\n\n\n\n\nChanging the sorting option.\n\n\n\n\nChanging the geo circle settings.\n\n\n\n\nChanging the geo choropleth settings in presentation mode.\n\n\n\n\nPresentation Mode\n\n\nDashboards can be viewed in a fullscreen mode with main navigation elements removed.\n\n\n\n\nYou can access the Presentation Mode from the dashboard menu:\n\n\n\n\nNote\n: To share your presentation, share the URL while inside Presentation Mode.\n\n\nPresentation Builder\n\n\nThe Presentation Builder can be used to create a presentation with multiple dashboards.\nThe dashboard you launch the Presentation Builder from is the \nhome dashboard\n, which means\nthat dashboard will serve as the starting point of your presentation. To start the presentation,\njust enter Presentation Mode from the home dashboard.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#twitter-example", 
            "text": "The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the  visualize  button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:", 
            "title": "Twitter Example"
        }, 
        {
            "location": "/dtdashboard/#ads-dimension-example", 
            "text": "The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Ads Dimension Example"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The embedded Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators SnapshotServer (which includes embedded Query Operator) and QueryResult are the operators that serve the data being visualized in the Console.  The SnapshotServer operator takes in data from the TopCounter operator, processes incoming queries, and generates results. The Twitter Hashtag Demo application and all its operators are available in the Apache Malhar repository.", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that provide historical and real-time application statistics and custom metrics data.  You can visualize these metrics the same way as custom Data Sources in an application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from the Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, using the  visualize  button will display a list of existing dashboards that are associated with the application.   Below is an example of accessing the data visualization dashboard from a running application.\nYou can also navigate back to the application from within the dashboard's menu.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new  button on the Dashboards screen  create new dashboard  option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the  create new  button     Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with  create new dashboard  option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to create new dashboard from the visualize menu drop-down list.  The new dashboard will be automatically named based on the application name and saved.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#dashboard-controls", 
            "text": "Dashboards controls are presented as a set of buttons to the right of the dashboard title.", 
            "title": "Dashboard Controls"
        }, 
        {
            "location": "/dtdashboard/#view-mode", 
            "text": "In view mode, you are not allowed to modify the layout or add/edit widgets, but certain actions\ncan be taken in the dashboard menu.", 
            "title": "View Mode"
        }, 
        {
            "location": "/dtdashboard/#edit-mode", 
            "text": "In edit mode, you can add/edit widgets, change the dashboard layout, and change dashboard settings.", 
            "title": "Edit Mode"
        }, 
        {
            "location": "/dtdashboard/#dashboard-menu", 
            "text": "The dashboard menu contains the rest of the dashboard functionality. The  actions  section is mostly self-explanatory, but see the  Presentation Mode  and  Presentation Builder  sections below for more information about presenting your dashboards. The  associated applications  section displays the status of applications associated to the current dashboard and serves as a quick way to jump to those applications. Associated applications are selected based on the data sources of your widgets.  Note : If an an associated application enters a non-running state (e.g. KILLED, INACTIVE), a warning icon will be displayed on the  dashboard menu  button.", 
            "title": "Dashboard Menu"
        }, 
        {
            "location": "/dtdashboard/#dashboard-settings", 
            "text": "The dashboard settings interface allows you to change the dashboard name, description, logo image, and  select replacement associated applications .  It can be accessed from the  dashboard menu  using the  settings  option.", 
            "title": "Dashboard Settings"
        }, 
        {
            "location": "/dtdashboard/#replacing-associated-applications", 
            "text": "Dashboards have widgets that rely on associated applications for data. These associated applications can be replaced in  dashboard settings  if the replacement is compatible with the current dashboard.  The replacement application must have a compatible data source. Selecting an application with an incompatible data source will simply skip the replacement process for that application. If the data source matches, but the data schema is incompatible, the widget will attempt to reset its settings to match the new data schema.  When importing a dashboard, the interface tries its best to preselect the most compatible replacement application. Leaving fields blank means the replacement process for those applications will be skipped.", 
            "title": "Replacing Associated Applications"
        }, 
        {
            "location": "/dtdashboard/#packaged-dashboards", 
            "text": "", 
            "title": "Packaged Dashboards"
        }, 
        {
            "location": "/dtdashboard/#auto-import-when-launching-an-application", 
            "text": "Application packages may include packaged dashboards which can be imported. Application package developers may select some dashboards to be imported automatically when launching an application, and all packaged dashboards can be imported manually at any time from the Packaged Dashboards page.   When launching an application, the  Import Packaged Dashboards  section will appear if there are packaged dashboards. Use the checkboxes to select which dashboards to import. The dashboard name and replacement applications can be changed (see  replacing associated applications ).  Note : Auto imports of packaged dashboards only happen if there isn't already an existing dashboard with the same name and owner. They can still be marked for import in the launch interface, but will have to be given a unique name.", 
            "title": "Auto Import When Launching An Application"
        }, 
        {
            "location": "/dtdashboard/#import-from-a-running-application", 
            "text": "If a running application has associated packaged dashboards, the packaged dashboards can be imported using the  visualize  button in  Application Overview .   Clicking on a packaged dashboard in the dropdown will open an interface to change the dashboard name, description, logo image, and replacement applications before importing.  Note : See the  replacing associated applications  section for an explanation about replacement applications.   Pressing the  import  button at the bottom of the interface imports the dashboard, and can then be accessed in the  Visualize  section.", 
            "title": "Import From A Running Application"
        }, 
        {
            "location": "/dtdashboard/#import-from-packaged-dashboards-page", 
            "text": "For a list of all packaged dashboards across all your application packages, use the  import  button in the  Visualize  section.   The  import  button brings you to the following page where you can import or download individual packaged dashboards.", 
            "title": "Import From Packaged Dashboards Page"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking the  add widget  button in edit mode, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget.   Each data source supports one or more data schema types, such as snapshot and dimensions.  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.  Results are not persisted until you press the  save  button.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use  save  button to persist the desired results.  If the resulting changes should not be saved, using the  discard  button will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/dtdashboard/#control-widget", 
            "text": "Control widget can be used to configure settings of one or more widgets on the dashboard.  This is a convenient way to configure multiple widgets settings in one place.  It is also the only way to change widgets settings in presentation mode.  However, settings changes in presentation mode will not be saved.", 
            "title": "Control Widget"
        }, 
        {
            "location": "/dtdashboard/#controllable-widgets", 
            "text": "The following widgets may be controlled by the Control Widget:   Dimensions Schema Data Source Widgets  Geo Choropleth  Geo Circles  Line Chart  Multi Bar Chart  Stacked Area Chart    Snapshot Schema Data Source Widgets  Bar Chart  Horizontal Bar Chart  Multi Color Bar Chart  Pie Chart", 
            "title": "Controllable Widgets"
        }, 
        {
            "location": "/dtdashboard/#controllable-settings", 
            "text": "The following settings may be controlled by the Control Widget:   Dimensions Schema Data Source Settings  Circle Size  (Geo Circle widget only)  Color Intensity  (Geo Choropleth widget only)  Geo Coordinates  (Geo Circle widget only)  Keys Combinations and Aggregates  Time Range Selection  Tooltip Values  (Geo Choropleth and Circle widgets only)    Snapshot Schema Data Source Settings  Field to use as label  Field to use as quantity  Sort order", 
            "title": "Controllable Settings"
        }, 
        {
            "location": "/dtdashboard/#adding-control-widget", 
            "text": "Control Widget can be added just like any other widget.  See the  Adding Widgets  section for more details.", 
            "title": "Adding Control Widget"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets-to-control", 
            "text": "Click the edit button on the Control Widget to show the available widgets to control, and change the controllable widget selections.   The Cotnrol Widget does not allow selection of dimensional and snapshot schema widgets together. If users happen to choose widgets with both schema types, then the \"OK\" button remains disabled until only one schema type is selected.  The control widget allows selection of widgets with different data sources. Users are warned that some setting sections in the control widget may be disabled if the schemas are incompatible.   When widgets to be controlled have incompatible schemas, the incompatible sections remain disabled until mismatched widgets are removed in the control widget settings.  This issue can also be resolved if the widgets with incompatible schemas are removed from the dashboard.", 
            "title": "Adding Widgets to Control"
        }, 
        {
            "location": "/dtdashboard/#examples-of-control-widget-in-action", 
            "text": "Changing the time range selection for the line and stacked area charts.   Changing dimensional keys and aggregates.   Changing the sorting option.   Changing the geo circle settings.   Changing the geo choropleth settings in presentation mode.", 
            "title": "Examples of Control Widget in Action"
        }, 
        {
            "location": "/dtdashboard/#presentation-mode", 
            "text": "Dashboards can be viewed in a fullscreen mode with main navigation elements removed.   You can access the Presentation Mode from the dashboard menu:   Note : To share your presentation, share the URL while inside Presentation Mode.", 
            "title": "Presentation Mode"
        }, 
        {
            "location": "/dtdashboard/#presentation-builder", 
            "text": "The Presentation Builder can be used to create a presentation with multiple dashboards.\nThe dashboard you launch the Presentation Builder from is the  home dashboard , which means\nthat dashboard will serve as the starting point of your presentation. To start the presentation,\njust enter Presentation Mode from the home dashboard.", 
            "title": "Presentation Builder"
        }, 
        {
            "location": "/dtgateway/", 
            "text": "dtGateway\n\n\nOne of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind \ndtManage\n. It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS \ninstaller\n.\n\n\ndtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.\n\n\n\n\nThese features are exposed through a \nREST API\n. Here are some of things you can do with the REST API:\n\n\n\n\nGet performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances\n\n\nGet performance metrics and other details of physical and logical operators of each Apex application instance\n\n\nGet performance metrics and other details of individual containers used by each Apex application instance\n\n\nRetrieve container logs\n\n\nDynamically change operator properties, and add and remove operators from the DAG of a running Apex application\n\n\nRecord and retrieve tuples on the fly\n\n\nShutdown a running container or an entire Apex application\n\n\nDynamically change logging level of a container\n\n\nCreate, manage, and view custom system alerts\n\n\nCreate, manage, and interact with dtDashboard\n\n\nCreate, manage, and launch Apex App Packages\n\n\nBasic health checks of the cluster\n\n\n\n\nSecurity\n\n\nWith all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.\n\n\nFor authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.\n\n\nFor information on configuring security see \ndtGateway security\n guide.\n\n\nSystem Alerts\n\n\nSystem Alerts provide a way for users to monitor cluster and application metrics. When an alert condition (written in JavaScript) turns true and stays that way for a configured time interval, dtGateway sends email to the configured list of email addresses. The same is true when the condition turns false. Alerts are created via the \nPUT /ws/v2/systemAlerts/alerts/{name}\n call documented in the \nREST API\n. For more details on System Alerts, please see \nthis document\n.\n\n\nRest API\n\n\nHere is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090\n\n\n$ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n  \ncheckpointStartTime\n: \n1442512091772\n,\n  \ncheckpointTime\n: \n175\n,\n  \ncheckpointTimeMA\n: \n164\n,\n  \nclassName\n: \ncom.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator\n,\n  \ncontainer\n: \ncontainer_e08_1442448722264_14891_01_000017\n,\n  \ncounters\n: null,\n  \ncpuPercentageMA\n: \n0.2039266316727741\n,\n  \ncurrentWindowId\n: \n6195527785184762469\n,\n  \nfailureCount\n: \n0\n,\n  \nhost\n: \nnode22.morado.com:8041\n,\n  \nid\n: \n40\n,\n  \nlastHeartbeat\n: \n1442512100742\n,\n  \nlatencyMA\n: \n5\n,\n  \nlogicalName\n: \nQueryResult\n,\n  \nmetrics\n: {},\n  \nname\n: \nQueryResult\n,\n  \nports\n: [\n    {\n      \nbufferServerBytesPSMA\n: \n0\n,\n      \nname\n: \ninputPort\n,\n      \nqueueSizeMA\n: \n1\n,\n      \nrecordingId\n: null,\n      \ntotalTuples\n: \n6976\n,\n      \ntuplesPSMA\n: \n0\n,\n      \ntype\n: \ninput\n\n    }\n  ],\n  \nrecordingId\n: null,\n  \nrecoveryWindowId\n: \n6195527785184762451\n,\n  \nstatus\n: \nACTIVE\n,\n  \ntotalTuplesEmitted\n: \n0\n,\n  \ntotalTuplesProcessed\n: \n6976\n,\n  \ntuplesEmittedPSMA\n: \n0\n,\n  \ntuplesProcessedPSMA\n: \n20\n,\n  \nunifierClass\n: null\n}\n\n\n\n\nFor the complete spec of the REST API, please refer to dtGateway \nREST API\n.\n\n\nFor information on configuring dtGateway in general, see \nDataTorrent RTS Configuration", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#dtgateway", 
            "text": "One of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind  dtManage . It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS  installer .  dtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.   These features are exposed through a  REST API . Here are some of things you can do with the REST API:   Get performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances  Get performance metrics and other details of physical and logical operators of each Apex application instance  Get performance metrics and other details of individual containers used by each Apex application instance  Retrieve container logs  Dynamically change operator properties, and add and remove operators from the DAG of a running Apex application  Record and retrieve tuples on the fly  Shutdown a running container or an entire Apex application  Dynamically change logging level of a container  Create, manage, and view custom system alerts  Create, manage, and interact with dtDashboard  Create, manage, and launch Apex App Packages  Basic health checks of the cluster", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#security", 
            "text": "With all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.  For authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.  For information on configuring security see  dtGateway security  guide.", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway/#system-alerts", 
            "text": "System Alerts provide a way for users to monitor cluster and application metrics. When an alert condition (written in JavaScript) turns true and stays that way for a configured time interval, dtGateway sends email to the configured list of email addresses. The same is true when the condition turns false. Alerts are created via the  PUT /ws/v2/systemAlerts/alerts/{name}  call documented in the  REST API . For more details on System Alerts, please see  this document .", 
            "title": "System Alerts"
        }, 
        {
            "location": "/dtgateway/#rest-api", 
            "text": "Here is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090  $ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n   checkpointStartTime :  1442512091772 ,\n   checkpointTime :  175 ,\n   checkpointTimeMA :  164 ,\n   className :  com.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator ,\n   container :  container_e08_1442448722264_14891_01_000017 ,\n   counters : null,\n   cpuPercentageMA :  0.2039266316727741 ,\n   currentWindowId :  6195527785184762469 ,\n   failureCount :  0 ,\n   host :  node22.morado.com:8041 ,\n   id :  40 ,\n   lastHeartbeat :  1442512100742 ,\n   latencyMA :  5 ,\n   logicalName :  QueryResult ,\n   metrics : {},\n   name :  QueryResult ,\n   ports : [\n    {\n       bufferServerBytesPSMA :  0 ,\n       name :  inputPort ,\n       queueSizeMA :  1 ,\n       recordingId : null,\n       totalTuples :  6976 ,\n       tuplesPSMA :  0 ,\n       type :  input \n    }\n  ],\n   recordingId : null,\n   recoveryWindowId :  6195527785184762451 ,\n   status :  ACTIVE ,\n   totalTuplesEmitted :  0 ,\n   totalTuplesProcessed :  6976 ,\n   tuplesEmittedPSMA :  0 ,\n   tuplesProcessedPSMA :  20 ,\n   unifierClass : null\n}  For the complete spec of the REST API, please refer to dtGateway  REST API .  For information on configuring dtGateway in general, see  DataTorrent RTS Configuration", 
            "title": "Rest API"
        }, 
        {
            "location": "/dtgateway_systemalerts/", 
            "text": "DT Gateway System Alerts\n\n\nThe DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.\n\n\nAlerts and Topics\n\n\nAs described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.\n\n\nHere is an example for simple JavaScript condition; we can make a REST call to create an\nalert named \nxyz\n which emails to \nsomeone@company.com\n when the number of running\napplications is greater than 5 for at least 60 seconds; the JSON object is the payload:\n\n\nPUT /ws/v2/systemAlerts/alerts/xyz\n\n\n{\n  \ncondition\n:\n_topic['cluster.metrics'].numAppsRunning \n 5\n,\n  \nemail\n:\nsomeone@company.com\n,\n  \ndescription\n: \nNo.of apps running is more than 5\n,\n  \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nWhen the number of running applications is greater than 5 for more than 60 seconds, a simple\nemail will be sent to \nsomeone@company.com\n, stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect.\n\n\nThe condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform \n_topic[\ntopic_name\n]\n.\nThe topic names can be looked up using the \nGET /ws/v2/systemAlerts/topicData\n REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as \nGet\n, \nPut\n, and \nDelete\n):\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\n{\n  \napplications.\napplicationId\n.logicalOperators\n: {...},\n  \napplications.\napplicationId\n.physicalOperators\n: {...},\n  \napplications.\napplicationId\n.containers\n: {...},\n  \napplications.\napplicationId\n: {...}\n  \ncluster.metrics\n: {...},\n  \napplications\n: {...}\n}\n\n\n\n\nwhere \napplicationId\n refers to the application id of a running application\n(which typically has the form \napplication_1482319446115_4314\n). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.\n\n\nConfiguring SMTP for Email\n\n\nSMTP needs to be configured for the gateway to be able to send alert emails via an \n\nSMTP server\n. Make\na note of the following steps before attempting to configure SMTP in the gateway.\n\n\n\n\nmake sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).\n\n\nif the SMTP server supports or requires \nTLS or SSL encryption\n,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish \ntrust\n.\n\n\ndetermine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).\n\n\nif you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.\n\n\ndetermine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.\n\n\ndetermine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.\n\n\nin case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described\n\nhere\n. The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.\n\n\n\n\nYou can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request. \n\n\nPUT /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\npassword\n: \nsecret_password\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nContents of the JSON Object:\n\n\n\n\n\n\n\n\nJSON Key\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nthe SMTP hostname\n\n\n\n\n\n\nport\n\n\nthe SMTP port on the SMTP server\n\n\n\n\n\n\nfromAddr\n\n\nthe \"from-address\" described above i.e. the address from which the alert email is sent\n\n\n\n\n\n\nfromName\n\n\nthe \"from-name\" described above i.e. the user friendly string for the \"from-address\"\n\n\n\n\n\n\nencryptionType\n\n\nthe \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"\n\n\n\n\n\n\nauthType\n\n\nthe \"authentication-type\" value described above i.e. \"password\" or \"none\"\n\n\n\n\n\n\nusername\n\n\nthe \"SMTP-username\" value described above\n\n\n\n\n\n\npassword\n\n\nthe \"SMTP-password\" value described above\n\n\n\n\n\n\ntestAddr\n\n\nthe \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.\n\n\n\n\n\n\n\n\nA note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.\n\n\nGET /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nThe GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.\n\n\nManaging and viewing alerts\n\n\nThe following operations are available in the Gateway REST API:\n\n\n\n\n\n\nCreate an alert\n\n\n\n\n\n\nDelete an alert\n\n\n\n\n\n\nGet alert history\n\n\n\n\n\n\nView content and status of alerts\n\n\n\n\n\n\nView all the current data in the \n_topic\n array\n\n\n\n\n\n\nCreating an alert\n\n\nTo create an alert, the user needs to specify the alert name, condition,\nemail address and duration in milliseconds. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.\n\n\nComplex JavaScript Expressions :\n\n\nThe following example shows how to issue a REST request to create an alert named\n\nWordCountAppNotRunning\n which emails to \nphil@company.com\n and \nmike@company.com\n\nwhen the \nWordCount\n app is not in the \nRUNNING\n state for at least 60 seconds.\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \n_topic['applications.application_1480063135007_0543']['state'] != 'RUNNING'\n,\n  \nemail\n:\nphil@company.com, mike@company.com\n,\n  \ndescription\n: \nWordCount Application is not running\n,\n  \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nThe above alert works for the current invocation of the \nWordCount\n application; however,\nwhen the application is restarted, a new application \nid\n is generated for which this alert\nwill no longer work. To avoid having to update the application \nid\n in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application \nid\n for the given application as shown below:\n\n\nvar alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i \n appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;\n\n\n\n\nThe JavaScript expression must however be written as a single line; tools such as\n\njavascriptcompressor\n are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert;\n,\n   \nemail\n:\nphil@company.com, mike@company.com\n,\n   \ndescription\n: \nWordCount Application is not running\n,\n   \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nDeleting an alert\n\n\nWe can delete an alert with the DELETE REST API request:\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nAlerts history\n\n\nWe can get the alerts history with the GET REST API request:\n\n\nGET /ws/v2/systemAlerts/history\n\n\nIt returns a result of the following form:\n\n\n{\n\nhistory\n: [{\n    \nname\n: \nxyz\n,\n    \ninTime\n: \n1481235199598\n,\n    \noutTime\n: \n1481235251088\n,\n    \nmessage\n: \nNo.of apps running is more than 5\n\n},{\n    \nname\n: \ncheckCsvParserNotRunning\n,\n    \ninTime\n: \n1481235251087\n,\n    \noutTime\n: \n1481235549648\n,\n    \nmessage\n: \nApplication is not running\n\n}],\n}\n\n\n\n\nThe alert history comprises the alert name, time it became active (\ninTime\n), time it\nbecame inactive (\noutTime\n) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.\n\n\nViewing the content and the status of alerts\n\n\nWe can get the content and status of alerts with the GET REST API web request:\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nIt returns a result of the following form:\n\n\n{\n \nname\n: \ncheckLatencyApp\n,\n \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i\noperators.length);++i){if(operators[i].latencyMA\n50){alert=true;break}}}alert;\n,\n \nemail\n: \nphil@company.com, mike@company.com\n,\n \ndescription\n: \nchecking latency \n 50\n,\n \ntimeThresholdMillis\n: \n10000\n,\n \nalertStatus\n: {\n   \nisInAlert\n: true,\n   \ninTime\n: \n1481264665450\n,\n   \nemailSent\n: true,\n   \nmessage\n: \nchecking latency \n 50\n\n  }\n}\n\n\n\n\nThe result includes the alert name, condition, email addresses, description and duration.\nIt also alert status info such as \nisInAlert\n which indicates whether it is still active\nor not, \ninTime\n which represents the time the alert became active, \nemailSent\n which,\nas the name suggests, indicates if email was sent, and \nmessage\n which is similar to the\ndescription.\n\n\nViewing all the current data in the \n_topic\n array\n\n\nThis section is already covered under \nAlerts and Topics\n .", 
            "title": "Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#dt-gateway-system-alerts", 
            "text": "The DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.", 
            "title": "DT Gateway System Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-and-topics", 
            "text": "As described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.  Here is an example for simple JavaScript condition; we can make a REST call to create an\nalert named  xyz  which emails to  someone@company.com  when the number of running\napplications is greater than 5 for at least 60 seconds; the JSON object is the payload:", 
            "title": "Alerts and Topics"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertsxyz", 
            "text": "{\n   condition : _topic['cluster.metrics'].numAppsRunning   5 ,\n   email : someone@company.com ,\n   description :  No.of apps running is more than 5 ,\n   timeThresholdMillis : 60000 \n}  When the number of running applications is greater than 5 for more than 60 seconds, a simple\nemail will be sent to  someone@company.com , stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect.  The condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform  _topic[ topic_name ] .\nThe topic names can be looked up using the  GET /ws/v2/systemAlerts/topicData  REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as  Get ,  Put , and  Delete ):", 
            "title": "PUT /ws/v2/systemAlerts/alerts/xyz"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2systemalertstopicdata", 
            "text": "{\n   applications. applicationId .logicalOperators : {...},\n   applications. applicationId .physicalOperators : {...},\n   applications. applicationId .containers : {...},\n   applications. applicationId : {...}\n   cluster.metrics : {...},\n   applications : {...}\n}  where  applicationId  refers to the application id of a running application\n(which typically has the form  application_1482319446115_4314 ). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_systemalerts/#configuring-smtp-for-email", 
            "text": "SMTP needs to be configured for the gateway to be able to send alert emails via an  SMTP server . Make\na note of the following steps before attempting to configure SMTP in the gateway.   make sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).  if the SMTP server supports or requires  TLS or SSL encryption ,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish  trust .  determine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).  if you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.  determine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.  determine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.  in case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described here . The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.   You can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request.", 
            "title": "Configuring SMTP for Email"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , password :  secret_password , testAddr :  testuser@yourdomain.com \n}  Contents of the JSON Object:     JSON Key  Value      host  the SMTP hostname    port  the SMTP port on the SMTP server    fromAddr  the \"from-address\" described above i.e. the address from which the alert email is sent    fromName  the \"from-name\" described above i.e. the user friendly string for the \"from-address\"    encryptionType  the \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"    authType  the \"authentication-type\" value described above i.e. \"password\" or \"none\"    username  the \"SMTP-username\" value described above    password  the \"SMTP-password\" value described above    testAddr  the \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.     A note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.", 
            "title": "PUT /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , testAddr :  testuser@yourdomain.com \n}  The GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.", 
            "title": "GET /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#managing-and-viewing-alerts", 
            "text": "The following operations are available in the Gateway REST API:    Create an alert    Delete an alert    Get alert history    View content and status of alerts    View all the current data in the  _topic  array", 
            "title": "Managing and viewing alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#creating-an-alert", 
            "text": "To create an alert, the user needs to specify the alert name, condition,\nemail address and duration in milliseconds. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.  Complex JavaScript Expressions :  The following example shows how to issue a REST request to create an alert named WordCountAppNotRunning  which emails to  phil@company.com  and  mike@company.com \nwhen the  WordCount  app is not in the  RUNNING  state for at least 60 seconds.", 
            "title": "Creating an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning", 
            "text": "{\n   condition :  _topic['applications.application_1480063135007_0543']['state'] != 'RUNNING' ,\n   email : phil@company.com, mike@company.com ,\n   description :  WordCount Application is not running ,\n   timeThresholdMillis : 60000 \n}  The above alert works for the current invocation of the  WordCount  application; however,\nwhen the application is restarted, a new application  id  is generated for which this alert\nwill no longer work. To avoid having to update the application  id  in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application  id  for the given application as shown below:  var alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i   appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;  The JavaScript expression must however be written as a single line; tools such as javascriptcompressor  are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning_1", 
            "text": "{\n   condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert; ,\n    email : phil@company.com, mike@company.com ,\n    description :  WordCount Application is not running ,\n    timeThresholdMillis : 60000 \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#deleting-an-alert", 
            "text": "We can delete an alert with the DELETE REST API request:  DELETE /ws/v2/systemAlerts/alerts/{name}", 
            "title": "Deleting an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-history", 
            "text": "We can get the alerts history with the GET REST API request:  GET /ws/v2/systemAlerts/history  It returns a result of the following form:  { history : [{\n     name :  xyz ,\n     inTime :  1481235199598 ,\n     outTime :  1481235251088 ,\n     message :  No.of apps running is more than 5 \n},{\n     name :  checkCsvParserNotRunning ,\n     inTime :  1481235251087 ,\n     outTime :  1481235549648 ,\n     message :  Application is not running \n}],\n}  The alert history comprises the alert name, time it became active ( inTime ), time it\nbecame inactive ( outTime ) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.", 
            "title": "Alerts history"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-the-content-and-the-status-of-alerts", 
            "text": "We can get the content and status of alerts with the GET REST API web request:  GET /ws/v2/systemAlerts/alerts/{name}  It returns a result of the following form:  {\n  name :  checkLatencyApp ,\n  condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i operators.length);++i){if(operators[i].latencyMA 50){alert=true;break}}}alert; ,\n  email :  phil@company.com, mike@company.com ,\n  description :  checking latency   50 ,\n  timeThresholdMillis :  10000 ,\n  alertStatus : {\n    isInAlert : true,\n    inTime :  1481264665450 ,\n    emailSent : true,\n    message :  checking latency   50 \n  }\n}  The result includes the alert name, condition, email addresses, description and duration.\nIt also alert status info such as  isInAlert  which indicates whether it is still active\nor not,  inTime  which represents the time the alert became active,  emailSent  which,\nas the name suggests, indicates if email was sent, and  message  which is similar to the\ndescription.", 
            "title": "Viewing the content and the status of alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-all-the-current-data-in-the-_topic-array", 
            "text": "This section is already covered under  Alerts and Topics  .", 
            "title": "Viewing all the current data in the _topic array"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.\n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows.\n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nMalhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.   Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.  Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.  Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "Malhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Social Media"
        }, 
        {
            "location": "/installation/", 
            "text": "DataTorrent RTS Installation Guide\n\n\nThis guide covers installation of the DataTorrent RTS platform.\n\n\nPlanning\n\n\nInstallation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:\n\n\n\n\nAccessible by users who will launch and manage applications\n\n\nAccessible by all YARN nodes running Apache Apex applications\n\n\n\n\nNote\n: With \ndtGateway security\n configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.\n\n\nRequirements\n\n\n\n\nLinux operating system (tested on CentOS 6.x and Ubuntu 12.04)\n\n\nJava 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)\n\n\nHadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nGoogle Chrome or Safari to access the DataTorrent Console UI\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\n\n\nInstallation\n\n\nComplete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]\n\n\nBinaries are available in /opt/datatorrent/current/bin and links in /usr/bin\n\n\nConfiguration files located in /opt/datatorrent/current/conf\n\n\nLog files located in /var/log/datatorrent\n\n\nDataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\na.  Installing from self-extracting archive (*.bin)\n\n\n    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin\n\n\n\nb.  Installing from RedHat Package Manager archive (*.rpm)\n\n\n  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm\n\n\n\nLimited Local Installation\n\n\nA limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]\n\n\nBinaries are available under $HOME/datatorrent/current/bin\n\n\nConfiguration files located under $HOME/datatorrent/conf\n\n\nLog files located under $HOME/.dt/logs\n\n\n\n\nDataTorrent Gateway running as current user, and managed with dtgateway command\n\n\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\ncurl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin\n\n\n\n\n\n\n\nAdd DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.\n\n\nDATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin\n\n\n\n\n\n\n\nUpgrades\n\n\nDataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.\n\n\nAutomatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.  \n\n\nFull uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.\n\n\nCustomizing Installation\n\n\nVarious options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.\n\n\n./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B \npath\n      Use \npath\n as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U \nuser\n      Use \nuser\n user account for installation.  Default: dtadmin\n-G \ngroup\n     Use \ngroup\n group for installation.  Default: dtadmin ( based on value of \nuser\n )\n-H \npath\n      Use \npath\n for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E \nexpr\n      Adds export \nexpr\n to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway and Apex CLI\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s \nfile\n      Use \nfile\n DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e \nfile\n      Use \nfile\n DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.\n\n\n\nSome Hadoop distributions may require changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):\n\n\nsudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.\n\n\n\nIf JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:\n\n\nsudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path\n\n\n\nIn some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.\n\n\nsudo ./datatorrent-rts*.bin -U myuser\n\n\n\nInstallation Wizard\n\n\nAfter the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.\n\n\nhttp://\ninstallation_host\n:9090/\n\n\n\nThe Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.\n\n\nDataTorrent installation can be verified by running included demo applications.  See \nLaunching Demo Applications\n for details.\n\n\nNote\n: The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#datatorrent-rts-installation-guide", 
            "text": "This guide covers installation of the DataTorrent RTS platform.", 
            "title": "DataTorrent RTS Installation Guide"
        }, 
        {
            "location": "/installation/#planning", 
            "text": "Installation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:   Accessible by users who will launch and manage applications  Accessible by all YARN nodes running Apache Apex applications   Note : With  dtGateway security  configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.", 
            "title": "Planning"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "Linux operating system (tested on CentOS 6.x and Ubuntu 12.04)  Java 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)  Hadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH  Minimum of 8G RAM available on the Hadoop cluster  Google Chrome or Safari to access the DataTorrent Console UI  Permissions to create HDFS directory for DataTorrent user", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Complete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following   DataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]  Binaries are available in /opt/datatorrent/current/bin and links in /usr/bin  Configuration files located in /opt/datatorrent/current/conf  Log files located in /var/log/datatorrent  DataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command   Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  a.  Installing from self-extracting archive (*.bin)      curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin  b.  Installing from RedHat Package Manager archive (*.rpm)    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#limited-local-installation", 
            "text": "A limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.   DataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]  Binaries are available under $HOME/datatorrent/current/bin  Configuration files located under $HOME/datatorrent/conf  Log files located under $HOME/.dt/logs   DataTorrent Gateway running as current user, and managed with dtgateway command    Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin    Add DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.  DATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin", 
            "title": "Limited Local Installation"
        }, 
        {
            "location": "/installation/#upgrades", 
            "text": "DataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.  Automatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.    Full uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.", 
            "title": "Upgrades"
        }, 
        {
            "location": "/installation/#customizing-installation", 
            "text": "Various options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.  ./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B  path       Use  path  as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U  user       Use  user  user account for installation.  Default: dtadmin\n-G  group      Use  group  group for installation.  Default: dtadmin ( based on value of  user  )\n-H  path       Use  path  for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E  expr       Adds export  expr  to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway and Apex CLI\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s  file       Use  file  DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e  file       Use  file  DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.  Some Hadoop distributions may require changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):  sudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.  If JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:  sudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path  In some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.  sudo ./datatorrent-rts*.bin -U myuser", 
            "title": "Customizing Installation"
        }, 
        {
            "location": "/installation/#installation-wizard", 
            "text": "After the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.  http:// installation_host :9090/  The Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.  DataTorrent installation can be verified by running included demo applications.  See  Launching Demo Applications  for details.  Note : The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/configuration/", 
            "text": "DataTorrent RTS Configuration\n\n\nThis document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit \nhadoop.apache.org\n.\n\n\nInstallation\n\n\nIf you have not installed DataTorrent RTS already, follow the installation instructions in the \ninstallation guide\n.\n\n\n\n\nConfiguration Files\n\n\nSystem configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file \ncustom-env.sh\n can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.\n\n\nDepending on the installation type, these may be located under \n/opt/datatorrent/current/conf\n or \n~/datatorrent/current/conf\n.  See \ninstallation guide\n for details.\n\n\n(install dir)/conf/custom-env.sh\n\n\nThis file can be used to configure behavior of DT Gateway service,\nas well as \napex\n command line utility. \u00a0After adding custom properties\nto this file, dtgateway and Apex CLI utilities need to be restarted for\nchanges to take effect.\n\n\nExample custom-env.sh configuration:\n\n\n# Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m\n\n\n\n\nEnvironment variables available for configuration\n\n\n\n\nDT_GATEWAY_HEAP_MEM\n \n Maximum heap size allocated to DT Gateway service.  Default is 1024m.\n\n\nDT_GATEWAY_DEBUG\n \n Set to 1 to enable additional debug information in the dtgateway.log\n\n\nDT_CLASSPATH\n \n Classpath used to load additional jars or properties for Apex CLI and dtgateway\n\n\nDT_LOG_DIR\n \n Directory for log files\n\n\nDT_RUN_DIR\n \n Directory for process id and other temporary files\ncreated at run time\n\n\n\n\n(user home)/.dt/dt-site.xml\n\n\nThis file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.\n\n\nExample of a single property configuration in dt-site.xml:\n\n\nconfiguration\n\n  \nproperty\n\n      \nname\ndt.operator.MyCustomStore.host\n/name\n\n      \nvalue\n192.168.2.35\n/value\n\n  \n/property\n\n   \u2026\n\n/configuration\n\n\n\n\n\nGateway Configuration Properties\n\n\n\n\ndt.gateway.listenAddress\n - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090\n\n\ndt.gateway.autoPublishInterval\n - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.\n\n\ndt.gateway.sslKeystorePath\n - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the \ndtGateway Security\n document)\n\n\ndt.gateway.sslKeystorePassword\n - The password of the SSL key store (See the \ndtGateway Security\n document)\n\n\ndt.gateway.allowCrossOrigin\n - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.\n\n\ndt.gateway.authentication.(OPTION)\n - Determines the scheme of Hadoop security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.http.authentication\n - Determines the scheme of DT Gateway HTTP security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.staticResourceDirectory\n - The document root directory where the DT Gateway should serve from for the /static HTTP path.\n\n\n\n\nApplication Configuration Properties\n\n\nFor a complete list of configurable application properties see the Attributes\u00a0section below.\n\n\n\n\nResources Management and Performance Tuning\n\n\nThe platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via \nREST API\n, \nApex CLI\n, and \ndtManage\n.\n\n\nThe platform is also responsible for\n\n\n\n\nHonoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.\n\n\nHonoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.\n\n\n\n\nSTRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).\n\n\nCPU\n\n\nCPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.\n\n\nNetwork\n\n\nNetwork usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.\n\n\nThe platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.\n\n\nRAM\n\n\nSTRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.\n\n\nSpike Management\n\n\nStreaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.\n\n\nThe platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.\n\n\nPartitioning\n\n\nPartitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in\n\nApplication development\n and \nOperator Development\n.\n\n\nStream Modes\n\n\nThe platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.\n\n\n\n\nTHREAD_LOCAL\n: All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.\n\n\nCONTAINER_LOCAL\n: All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in memory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, persistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.\n\n\nNODE_LOCAL\n: All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.\n\n\nRACK_LOCAL\n: All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.\n\n\nUnspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.\n\n\n\n\n\n\n\n\nMulti-Tenancy and Security\n\n\nThe platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications.\n\nThe default security for the streaming application is Kerberos based.\n\n\nKerberos Authentication\n\n\nKerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.\n\n\nCLI Configuration\n\n\nThe DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. The security configuration for the CLI program is described in the Apache Apex security document available \nhere\n.\n\n\nDT Gateway Configuration\n\n\nDT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. In a Hadoop cluster with Kerberos security enabled additional configuration is needed for this service to communicate with Hadoop. This is described in \nGateway Security\n.\n\n\nConsole Authentication\n\n\nAccess to the UI console can be secured by having users authenticate before they can access the contents of the console. Different authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to \nGateway Security\n for details of how to configure this.\n\n\nRun-Time Management\n\n\nUnlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include\n\n\n\n\nRuntime metrics and stats on various components of the\n    application including aggregated metrics\n\n\nAbility to change the logical plan, physical plan, and\n    execution plan of the application\n\n\nAbility to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)\n\n\nIntegration of STRAM state with Zookeeper (in later versions)\n\n\nDebugger, charting, and other tools triggered at run time\n\n\n\n\nDynamic Functional Modifications\n\n\nPlatform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe \napex\n tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.\n\n\nFrom an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).\n\n\nExamples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream\n\n\nAny change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude\n\n\n\n\nChange in attributes that triggers STRAM to change the number\n    of physical operators.\n\n\nRuntime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals\n\n\nDirect request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate\n\n\n\n\nAny change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include\n\n\n\n\nChanges in attributes that need a new execution plan\n\n\nChanges to stream modes\n\n\nNode recovery from an outage\n\n\n\n\nRuntime Code\n\n\nSTRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.\n\n\nLoad\n\n\nSTRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.\n\n\nUptime\n\n\nNode recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (preemption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.\n\n\n\n\nAttributes\n\n\nAttribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.\n\n\nThere are three kinds of attributes\n\n\n\n\nApplication attributes\n\n\nOperator attributes\n\n\nPort attributes\n\n\n\n\nFor implementation details look at the \njavadocs\n. Some very common attributes are\n\n\n\n\nApplication: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See \nContext.DAGContext\n\n\nOperators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See \nContext.OperatorContext\n\n\nPorts: Queue capacity, auto_record, partition parallel, etc.\n    See \nContext.PortContext\n\n\n\n\nThese attributes are available via the \nContext\n class and can be\naccessed in the \nsetup\n call of an operator.\n\n\nApplication Configuration\n\n\nStarting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the\n\nApplication Packages\n\u00a0for details.\n\n\nAdjusting Logging Levels\n\n\nApplication Logging\n\n\nLogging levels for specific classes or groups of classes can be raised or\nlowered at runtime from \ndtManage\n application view with the\n\nSet Log Levels\n button. \u00a0Explicit class paths or patterns like\n\norg.apache.hadoop.*\n\u00a0or \ncom.datatorrent.*\n\u00a0can be used to adjust logging to\nvalid \nlog4j\n levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.\n\n\nFor permanent changes to logging levels, lines similar to these can be inserted\ninto \ndt-site.xml\n. To specify multiple patterns, use a comma-separated list.\n\n\nproperty\n\n  \nname\ndt.loggers.level\n/name\n\n  \nvalue\ncom.datatorrent.*:DEBUG,org.apache.*:INFO\n/value\n\n\n/property\n\n\n\n\n\nFull DEBUG logging can be enabled by adding these lines:\n\n\nproperty\n\n  \nname\ndt.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\n\nCustom log4j Properties for Application Packages\n\n\nThere are two ways of setting custom \nlog4j\n properties in an Apex application\npackage\n\n\n\n\n\n\nAt the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:\n\n\nproperty\n\n  \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n  \nvalue\n-Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nAt an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:\n\n\nproperty\n\n  \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n  \nvalue\n -Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nMake sure that the file \ncustom_log4j.properties\n is part of your application\njar and is located under \nsrc/main/resources\n.  Some examples of custom log4j\nproperties files follow.\n\n\n\n\n\n\nWriting to a file\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n\n\n\n\n\n\nWriting to Console\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\n\n\n\n\n\n\n\ndtGateway Logging\n\n\nDT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).\n\n\nexport DT_GATEWAY_DEBUG=1\n\n\n\nHadoop Tuning\n\n\nYARN vmem-pmem ratio tuning\n\n\nAfter performing a new installation, sometimes the following\nmessage is displayed while launching an application:\n\n\nApplication application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001\n\n\n\n\nTo fix this \nyarn.nodemanager.vmem-pmem-ratio\n in \nyarn-site.xml\n should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:\n\n\nproperty\n\n   \ndescription\nRatio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n   \n/description\n\n   \nname\nyarn.nodemanager.vmem-pmem-ratio\n/name\n\n   \nvalue\n10\n/value\n\n \n/property", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#datatorrent-rts-configuration", 
            "text": "This document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit  hadoop.apache.org .", 
            "title": "DataTorrent RTS Configuration"
        }, 
        {
            "location": "/configuration/#installation", 
            "text": "If you have not installed DataTorrent RTS already, follow the installation instructions in the  installation guide .", 
            "title": "Installation"
        }, 
        {
            "location": "/configuration/#configuration-files", 
            "text": "System configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file  custom-env.sh  can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.  Depending on the installation type, these may be located under  /opt/datatorrent/current/conf  or  ~/datatorrent/current/conf .  See  installation guide  for details.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/configuration/#install-dirconfcustom-envsh", 
            "text": "This file can be used to configure behavior of DT Gateway service,\nas well as  apex  command line utility. \u00a0After adding custom properties\nto this file, dtgateway and Apex CLI utilities need to be restarted for\nchanges to take effect.  Example custom-env.sh configuration:  # Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m  Environment variables available for configuration   DT_GATEWAY_HEAP_MEM    Maximum heap size allocated to DT Gateway service.  Default is 1024m.  DT_GATEWAY_DEBUG    Set to 1 to enable additional debug information in the dtgateway.log  DT_CLASSPATH    Classpath used to load additional jars or properties for Apex CLI and dtgateway  DT_LOG_DIR    Directory for log files  DT_RUN_DIR    Directory for process id and other temporary files\ncreated at run time", 
            "title": "(install dir)/conf/custom-env.sh"
        }, 
        {
            "location": "/configuration/#user-homedtdt-sitexml", 
            "text": "This file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.  Example of a single property configuration in dt-site.xml:  configuration \n   property \n       name dt.operator.MyCustomStore.host /name \n       value 192.168.2.35 /value \n   /property \n   \u2026 /configuration", 
            "title": "(user home)/.dt/dt-site.xml"
        }, 
        {
            "location": "/configuration/#gateway-configuration-properties", 
            "text": "dt.gateway.listenAddress  - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090  dt.gateway.autoPublishInterval  - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.  dt.gateway.sslKeystorePath  - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the  dtGateway Security  document)  dt.gateway.sslKeystorePassword  - The password of the SSL key store (See the  dtGateway Security  document)  dt.gateway.allowCrossOrigin  - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.  dt.gateway.authentication.(OPTION)  - Determines the scheme of Hadoop security authentication (See the  dtGateway Security  document).  dt.gateway.http.authentication  - Determines the scheme of DT Gateway HTTP security authentication (See the  dtGateway Security  document).  dt.gateway.staticResourceDirectory  - The document root directory where the DT Gateway should serve from for the /static HTTP path.", 
            "title": "Gateway Configuration Properties"
        }, 
        {
            "location": "/configuration/#application-configuration-properties", 
            "text": "For a complete list of configurable application properties see the Attributes\u00a0section below.", 
            "title": "Application Configuration Properties"
        }, 
        {
            "location": "/configuration/#resources-management-and-performance-tuning", 
            "text": "The platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via  REST API ,  Apex CLI , and  dtManage .  The platform is also responsible for   Honoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.  Honoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.   STRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).", 
            "title": "Resources Management and Performance Tuning"
        }, 
        {
            "location": "/configuration/#cpu", 
            "text": "CPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.", 
            "title": "CPU"
        }, 
        {
            "location": "/configuration/#network", 
            "text": "Network usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.  The platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.", 
            "title": "Network"
        }, 
        {
            "location": "/configuration/#ram", 
            "text": "STRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.", 
            "title": "RAM"
        }, 
        {
            "location": "/configuration/#spike-management", 
            "text": "Streaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.  The platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.", 
            "title": "Spike Management"
        }, 
        {
            "location": "/configuration/#partitioning", 
            "text": "Partitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in Application development  and  Operator Development .", 
            "title": "Partitioning"
        }, 
        {
            "location": "/configuration/#stream-modes", 
            "text": "The platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.   THREAD_LOCAL : All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.  CONTAINER_LOCAL : All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in memory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, persistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.  NODE_LOCAL : All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.  RACK_LOCAL : All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.  Unspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.", 
            "title": "Stream Modes"
        }, 
        {
            "location": "/configuration/#multi-tenancy-and-security", 
            "text": "The platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications. \nThe default security for the streaming application is Kerberos based.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/configuration/#kerberos-authentication", 
            "text": "Kerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/configuration/#cli-configuration", 
            "text": "The DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. The security configuration for the CLI program is described in the Apache Apex security document available  here .", 
            "title": "CLI Configuration"
        }, 
        {
            "location": "/configuration/#dt-gateway-configuration", 
            "text": "DT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. In a Hadoop cluster with Kerberos security enabled additional configuration is needed for this service to communicate with Hadoop. This is described in  Gateway Security .", 
            "title": "DT Gateway Configuration"
        }, 
        {
            "location": "/configuration/#console-authentication", 
            "text": "Access to the UI console can be secured by having users authenticate before they can access the contents of the console. Different authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to  Gateway Security  for details of how to configure this.", 
            "title": "Console Authentication"
        }, 
        {
            "location": "/configuration/#run-time-management", 
            "text": "Unlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include   Runtime metrics and stats on various components of the\n    application including aggregated metrics  Ability to change the logical plan, physical plan, and\n    execution plan of the application  Ability to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)  Integration of STRAM state with Zookeeper (in later versions)  Debugger, charting, and other tools triggered at run time", 
            "title": "Run-Time Management"
        }, 
        {
            "location": "/configuration/#dynamic-functional-modifications", 
            "text": "Platform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe  apex  tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.  From an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).  Examples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream  Any change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude   Change in attributes that triggers STRAM to change the number\n    of physical operators.  Runtime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals  Direct request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate   Any change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include   Changes in attributes that need a new execution plan  Changes to stream modes  Node recovery from an outage", 
            "title": "Dynamic Functional Modifications"
        }, 
        {
            "location": "/configuration/#runtime-code", 
            "text": "STRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.", 
            "title": "Runtime Code"
        }, 
        {
            "location": "/configuration/#load", 
            "text": "STRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.", 
            "title": "Load"
        }, 
        {
            "location": "/configuration/#uptime", 
            "text": "Node recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (preemption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.", 
            "title": "Uptime"
        }, 
        {
            "location": "/configuration/#attributes", 
            "text": "Attribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.  There are three kinds of attributes   Application attributes  Operator attributes  Port attributes   For implementation details look at the  javadocs . Some very common attributes are   Application: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See  Context.DAGContext  Operators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See  Context.OperatorContext  Ports: Queue capacity, auto_record, partition parallel, etc.\n    See  Context.PortContext   These attributes are available via the  Context  class and can be\naccessed in the  setup  call of an operator.", 
            "title": "Attributes"
        }, 
        {
            "location": "/configuration/#application-configuration", 
            "text": "Starting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the Application Packages \u00a0for details.", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/configuration/#adjusting-logging-levels", 
            "text": "", 
            "title": "Adjusting Logging Levels"
        }, 
        {
            "location": "/configuration/#application-logging", 
            "text": "Logging levels for specific classes or groups of classes can be raised or\nlowered at runtime from  dtManage  application view with the Set Log Levels  button. \u00a0Explicit class paths or patterns like org.apache.hadoop.* \u00a0or  com.datatorrent.* \u00a0can be used to adjust logging to\nvalid  log4j  levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.  For permanent changes to logging levels, lines similar to these can be inserted\ninto  dt-site.xml . To specify multiple patterns, use a comma-separated list.  property \n   name dt.loggers.level /name \n   value com.datatorrent.*:DEBUG,org.apache.*:INFO /value  /property   Full DEBUG logging can be enabled by adding these lines:  property \n   name dt.attr.DEBUG /name \n   value true /value  /property", 
            "title": "Application Logging"
        }, 
        {
            "location": "/configuration/#custom-log4j-properties-for-application-packages", 
            "text": "There are two ways of setting custom  log4j  properties in an Apex application\npackage    At the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:  property \n   name dt.attr.CONTAINER_JVM_OPTIONS /name \n   value -Dlog4j.configuration=custom_log4j.properties /value  /property     At an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:  property \n   name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n   value  -Dlog4j.configuration=custom_log4j.properties /value  /property     Make sure that the file  custom_log4j.properties  is part of your application\njar and is located under  src/main/resources .  Some examples of custom log4j\nproperties files follow.    Writing to a file  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}    Writing to Console  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout", 
            "title": "Custom log4j Properties for Application Packages"
        }, 
        {
            "location": "/configuration/#dtgateway-logging", 
            "text": "DT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).  export DT_GATEWAY_DEBUG=1", 
            "title": "dtGateway Logging"
        }, 
        {
            "location": "/configuration/#hadoop-tuning", 
            "text": "", 
            "title": "Hadoop Tuning"
        }, 
        {
            "location": "/configuration/#yarn-vmem-pmem-ratio-tuning", 
            "text": "After performing a new installation, sometimes the following\nmessage is displayed while launching an application:  Application application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001  To fix this  yarn.nodemanager.vmem-pmem-ratio  in  yarn-site.xml  should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:  property \n    description Ratio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n    /description \n    name yarn.nodemanager.vmem-pmem-ratio /name \n    value 10 /value \n  /property", 
            "title": "YARN vmem-pmem ratio tuning"
        }, 
        {
            "location": "/dtgateway_security/", 
            "text": "DataTorrent Gateway Security\n\n\nDataTorrent Gateway is a service that provides the backend functionality\nfor the DataTorrent UI Console and processes the web service requests from it. The service provides real-time information about running applications, allows changes to applications, launches new applications among various other operations. Refer to \ndtGateway\n for details on the Gateway service and \ndtManage\n\u00a0for the UI Console.\n\n\nBroadly security in Gateway can be classified into two categories, frontend security and backend security. Frontend security deals with access to Gateway service which mainly involves securing the web service calls. This includes aspects such as user authentication and authorization. The backend security deals with security aspects when Gateway is communicating with a secure Hadoop infrastructure.\n\n\nAfter installation of DataTorrent RTS both these aspects can be configured manually as described in the following sections although work is being done to enable this configuration in the UI Console during installation itself and post installation.\n\n\nKerberos Secure Mode\n\n\nKerberos is the de-facto authentication mechanism supported in Hadoop. When secure mode is enabled in Hadoop, requests from clients to Hadoop are authenticated using Kerberos. In this mode Gateway service needs Kerberos credentials to communicate with Hadoop. The credentials should match the user that the DT Gateway service is running under.\n\n\nIn a multi-user installation DT Gateway is typically running as\nuser \ndtadmin\n and the Kerberos credentials specified should be for this\nuser. They are specified in the \ndt-site.xml\n configuration file located in the config folder under the installation which is typically \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install). For a single user installation where gateway is running as the user, the Kerberos credentials will be the user\u2019s credentials.\n\n\nThe snippet below shows how the credentials can be specified in the\nconfiguration file.\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.principal\n/name\n\n        \nvalue\nkerberos-principal-of-gateway-user\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.keytab\n/name\n\n        \nvalue\nabsolute-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\nLong running applications\n\n\nIn secure mode, long running applications have additional requirements. Refer to the Token Refresh section in the Apache Apex security \ndocument\n.\n\n\nAuthentication\n\n\nDataTorrent Gateway has support for authentication and when it is configured users have to authenticate before they can access the UI Console. Various authentication mechanisms are supported and this gives enterprises the flexibility to extend their existing authentication mechanism already in use within the enterprise to Gateway. It also supports roles, mapping of groups or roles from the external authentication mechanism to roles and supports role based authorization.\n\n\nThe different authentication mechanisms supported by Gateway are\n\n\n\n\nPassword Authentication\n\n\nLDAP Authentication\n\n\nKerberos Authentication\n\n\nJAAS Authentication\n for Active Directory, PAM, etc\n\n\n\n\nJAAS is a extensible authentication framework that supports different types of authentication mechanisms by plugging in an appropriate module.\n\n\nPassword Authentication\n\n\nPassword security is simple to set up and is ideal for a small to medium set of users. It comes with role-based access control, so users can be assigned roles, and roles can be assigned granular permissions (see \nUser Management\n). This is the only authentication mechanism available that does not depend on any external systems. The users will be managed locally by the Gateway. When enabled, all users will be presented with the login prompt before being able to use the DT Console.\n\n\nTo set up password security, on the \nSecurity Configuration\n page select \nPassword\n from the \nAuthentication Type\n dropdown, and save. Allow the Gateway to restart.\n\n\n\n\nWhen the Gateway has restarted, you should be prompted for username and password. Log in as the default admin user \ndtadmin\n with password \ndtadmin\n.\n\n\n\n\nOnce authenticated, active username and an option to log out is presented in the top right corner of the DT Console screen.\n\n\n\n\nAdditional users and roles can be created and managed on the \nUser Management\n page.\n\n\nNote\n: Don't forget to change your \ndtadmin\n user's password!\n\n\nPassword Authentication via dt-site.xml\n\n\nPassword authentication can alternatively be configured outside the Console by performing following two steps:\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n    \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\npassword\n/value\n\n    \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway. If running Gateway in local mode use \ndtgateway restart\n instead.\n\n\nsudo service dtgateway restart\n\n\n\n\n\n\n\nLDAP Authentication\n\n\nLDAP is a directory based authentication mechanism used in many enterprises. If your organization uses LDAP for authentication, the LDAP security option is ideal for giving your existing users access to RTS, with the role-based access control and group mapping features.\n\n\nThere are four variations for configuring LDAP authentication:\n\n\n\n\nIdentity\n\n\nProvide the parent DN of your users and \nspecify the RDN attribute of users\n.\n\n\nUsers will authenticate using their RDN attribute value as their username.\n\n\n\n\n\n\nAnonymous \n User Search Filter\n\n\nProvide the parent DN of your users and \nspecify a search filter to identify users\n.\n\n\nUsers will authenticate using an appropriate username that matches the parameters defined in the search filter.\n\n\n\n\n\n\nIdentity and Anonymous Search\n\n\nProvide the parent DN of your users, \nspecify the RDN attribute of users, and a search filter\n.\n\n\nUsers will authenticate using their RDN attribute value as their username, as well as the parameters defined in the search filter.\n\n\n\n\n\n\nNon-Anonymous Search with Group Support\n\n\nProvide basic DN info for users, DN and password of a user able to perform a non-anonymous search, and optional group support info.\n\n\nWith \ngroup support disabled\n, users need to be added in User Management before logging in. Users authenticate with their RDN attribute value as their username.\n\n\nWith \ngroup support enabled\n, users do not have to be added in User Management before logging in. Users authenticate with their RDN attribute value as their username. They will be assigned roles mapped to their LDAP group. For example, user Peter is part of GroupA (admin) and GroupB (developer, operator); Peter will be assigned roles admin, developer, and operator upon login.\n\n\n\n\n\n\n\n\nWhen group support is not configured, users must be assigned a role before they are able to log in. This means users can be restricted from logging in (blacklisted) by removing all of their roles.\n\n\nNote\n: If migrating from \nPassword\n mode, the existing users will be carried over as \"local users\" and can still login as if in \nPassword\n mode. It is recommended to keep only the \ndtadmin\n user and delete the rest. This is because local users cannot be added or deleted once \nLDAP\n mode is activated.\n\n\n\n\nAfter setting up LDAP in the Security Configuration page, the \nLDAP Users\n section will appear in the User Management page. If you have group support enabled, the \nLDAP Groups\n section will also appear. Existing users (carried over from \nPassword\n mode), will be placed in the \nLocal Users\n sections. Local users cannot be added or deleted in LDAP mode, but their roles and passwords can be modified.\n\n\n\n\nTo configure LDAP via dt-site.xml, check out the \nJAAS Authentication - LDAP\n section below.\n\n\nKerberos Authentication\n\n\nKerberos authentication can optionally be enabled for Hadoop web access.\nWhen configured, all web browser access to Hadoop management consoles are Kerberos authenticated. Web services are also Kerberos authenticated. This authentication is performed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.\n\n\nKerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by logging in to kerberos system in a terminal emulator using kinit. Then, user launches a browser to access the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.\n\n\nWhen this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the \nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nAdditional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install)\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\nkerberos\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.principal\n/name\n\n    \nvalue\n{kerberos-principal-of-web-service}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.keytab\n/name\n\n    \nvalue\n{absolute-path-to-keytab-file}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.token.validity\n/name\n\n    \nvalue\n{authentication-token-validity-in-seconds}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.cookie.domain\n/name\n\n    \nvalue\n{http-cookie-domain-for-authentication-token}\n/value\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.cookie.path\n/name\n\n    \nvalue\n{http-cookie-path}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.signature.secret\n/name\n\n    \nvalue\n{absolute-path-of-secret-file-for-signing-authentication-tokens} \n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nNote that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property\n\ndt.gateway.http.authentication.type\n should be replaced with the\nappropriate values for your setup.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nJAAS Authentication\n\n\nJAAS or Java Authentication and Authorization Service is a pluggable\nand extensible mechanism for authentication. It is an authentication framework\nwhere the actual authentication is performed by a JAAS login module plugin which can be configured using a configuration file. \n\n\nThe general configuration steps for enabling any JAAS based authentication mechanism is described below. Subsequent sections will cover the specific configuration details for LDAP, Active Directory and PAM. However JAAS is not limited to just these three mechanisms. Any other JAAS compatible mechanism can be used by specifying the appropriate module in the configuration. Also if there isn't a JAAS module available for an authentication mechanism a new one can be developed using the JAAS API and used here.\n\n\nBelow are the general steps for configuring a JAAS authentication mechanism\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n (or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n    ...\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.type\n/name\n\n      \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n      \nvalue\nname-of-jaas-module\n/value\n\n  \n/property\n\n    ...\n\n/configuration\n\n\n\n\nThe \ndt.gateway.http.authentication.jaas.name\n property specifies the login module to use with JAAS and the next step explains the process for configuring it.\n\n\n\n\n\n\nIf the login module requires a custom callback handler it needs to be specified. What a callback handler is and how it can be specified is described in the next section \nCallback Handlers\n.\n\n\n\n\n\n\nThe name of the login module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be \n/home/dtadmin/.java.login.config\n, if running as a\n    regular user it would be \n~/.java.login.config\n. The sample\n    configurations for LDAP and PAM are shown in the next sections.\n\n\n\n\n\n\nThe classes for the login module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.\n\n\nThe following step shows how to do this\n\n\na.  Edit the \ncustom-env.sh\n configuration file, typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n(when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nSimilar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the\n\nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.  \n\n\nCallback Handlers\n\n\nIn JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. These callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS provides a default callback handler that handles the common callbacks but it may\nnot be sufficient for all login modules like in the Active Directory case described below.\n\n\nRTS also supports specification of a custom callback handler to handle custom callbacks of a login module. The custom callback handler can be specified using the property \ndt.gateway.http.authentication.jaas.callback.class.name\n and when this property is not specified the default callback handler is used. The property can be specified in the \ndt-site.xml\n configuration file and as follows\n\n\nconfiguration\n\n    ...\n    \nproperty\n \n            \nname\ndt.gateway.http.authentication.jaas.callback.class.name\n/name\n\n        \nvalue\nfull-class-name-of-callback\n/value\n\n    \n/property\n\n    ...\n\n/configuration\n\n\n\n\n\nCustom callback handlers can be implemented by extending the default callback handler so they get the support for the common callbacks or they can be implemented from scratch. The Jetty callback handler described below also extends the default callback handler. The default callback handler implementation is implemented in the \ncom.datatorrent.lib.security.auth.callback.DefaultCallbackHandler\n class available in \ncom.datatorrent:dt-library\n artifact.\n\n\nLDAP\n\n\nAn alternative way to enable LDAP authentication, instead of \nLDAP Authentication\n section above, is to follow the JAAS configuration steps described above with the following specific details for the individual steps.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JAAS module for LDAP. You can choose a name like \nldap\n that is appropriate for the current scheme. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nldap\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.\n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate LDAP settings. There are different JAAS authentication modules available for LDAP. One of them is supplied by default in Java. It is the Sun LdapLoginModule class. A sample configuration when using this module is shown below\n\n\nldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};\n\n\n\nNote that the first string before the open brace, in this case\n\nldap\n must match the jaas name specified in step 1. The first property\n\ncom.sun.security.auth.module.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The fields that appear next are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.\n\n\nThe setting \nauthIdentity\n is used to derive the identity of the user that will be used for authentication with the server. Here it is specifying the pattern to derive the LDAP distinguished name (dn) for the user that will be used along with the password supplied by the user for authentication with the LDAP server. The \n{USERNAME}\n variable in the setting will be replaced by the username specified by the user. \n\n\nRefer to the \njavadoc\n for all the configuration options available with this module.\n\n\n\n\n\n\nFor step 4, no extra jars are needed as the module is available in Java\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nActive Directory\n\n\nActive Directory is used when authenting users in Microsoft Windows domains. The authentication protocol includes Microsoft's implementation of Kerberos as well as LDAP. In this section we will look into the configuration needed for LDAP authentication with Active Directory.\n\n\nFollow the JAAS configuration steps described above with the following specific details.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JAAS module. You can choose a name like \nad\n that is appropriate for the current scheme. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nad\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks. \n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured with\n   the appropriate Active Directory settings in the .java.login.config file. \n\n\nIn active directory authentication the user id is typically specified as \nusername@domain\n and sometimes just \nusername\n. Once the user is authenticated the user id needs to be mapped to the LDAP node for the user. This is done by specifying a search filter criteria which is then used to search for the user node in the directory tree starting from a specified base.\n\n\nBelow is an example showing a sample configuration for active directory authentication with the Sun LdapLoginModule\n\n\nad {\n    com.sun.security.auth.module.LdapLoginModule required\n    userProvider=\"ldap://ad.server.com/cn=users,dc=domain,dc=com\"\n    userFilter=\"samAccountName={USERNAME}\"\n    authIdentity=\"{USERNAME}@my.domain.com\";\n};\n\n\n\nThe setting \nuserFilter\n specifies the search criteria to look for the user in the Active Directory tree. The \n{USERNAME}\n variable in the setting is replaced with the username supplied by the user. The setting \nauthIdentity\n specifies the pattern to derive the user id with the domain from the specified username.\n\n\nThe search filter can have more complex logical expressions like the one shown below\n\n\nuserFilter=\"(\n(|(samAccountName={USERNAME})(userPrincipalName={USERNAME})(cn={USERNAME}))(objectClass=user))\"\n\n\n\nAn Active Directory or a LDAP browser client can be used to browse the Active Directory to determine the correct filter to use find the users.\n\n\n\n\n\n\nFor step 4, no extra jars are needed as the module is available in Java\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nJetty module \n binding\n\n\nSometimes a single \nauthIdentity\n pattern like the one used above cannot be used to identity the users because of the way they are configured in the Active Directory, for an enterprise. An example is the case where multiple domains are being used and a single domain name cannot be specified in the \nauthIdentity\n. \n\n\nIn those cases a fixed identity called the root identity or a system identity is used to connect to the server. Then the user node is searched with the search criteria and supplied username, from the node the user directory name is extracted and it is used along with the supplied password for the authentication.\n\n\nA different JAAS login module that supports the system identity is needed as the Sun LdapLoginModule does not support this functionality. Jetty implements one such login module. The steps to configure this module are as follows.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, the name \nad\n can be used.\n\n\n\n\n\n\nFor step 2, a special callback handler is needed to handle the Jetty module callbacks as they are not the common callbacks. There is an implementation of the callback handler for Jetty that is provided by DataTorrent RTS. It is the \ncom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n class in the \ncom.datatorrent:dt-contrib\n artifact.\n\n\nAs explained in the \nCallback Handlers\n section above, the callback handler can be specified in the \ndt-site.xml\n config file using a property as shown below\n\n\nconfiguration\n\n    ...\n    \nproperty\n \n        \nname\n\n            dt.gateway.http.authentication.jaas.callback.class.name\n    \n/name\n\n        \nvalue\n\n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n        \n/value\n\n    \n/property\n\n    ...  \n\n/configuration\n\n\n\n\n\n\n\n\nFor step 3, the configuration for the Jetty module is specified in the \n.java.login.config\n file. A sample configuration for this module is shown below\n\n\nad {\n    org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\"\n    hostname=\"ad.server.com\"\n    port=\"389\"\n    bindDn=\"serviceId@my.domain.com\"\n    bindPassword=\"Password1\"\n    forceBindingLogin=\"true\"\n    userBaseDn=\"DC=my,DC=domain,DC=com\"\n    userIdAttribute=\"samAccountName\"\n    userObjectClass=\"person\";\n};\n\n\n\nThe property \norg.eclipse.jetty.plus.jaas.spi.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP authentication. The properties \nbindDn\n and \nbindPassword\n specify the directory name and password for the system identity respectively. The \nuserIdAttribute\n and \nuserObjectClass\n settings are used as search criteria to search for the user directory node under the \nuserBaseDn\n path in the directory.\n\n\nRefer to the \njavadoc\n for all the configuration options available with this module.\n\n\n\n\n\n\nThe Gateway service in DataTorrent RTS 3.0 is compatible with Jetty 8. The Jetty login module in this version is in the \njetty-plus\n jar. The following version \njetty-plus-8.1.10.v20130312.jar\n is known to work but other compatible versions should work as well. The jar file can be obtained from the Jetty project or from \nmaven central\n. \n\n\nAlong with the jetty jar the dt-contrib jar containing the callback handler and it's dependencies should also be included. It is available on the DataTorrent maven server \nhere\n.  Pick the version matching the DataTorrent RTS version you are using. The dt-contrib artifact has a dependency to \ncom.datatorrent:dt-library\n artifact. It is also available on the maven server \nhere\n.\n\n\nThe jars can be specified in the gateway script file described in step 4 above as follows\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jetty-plus-8.1.10.v20130312.jar:path/to/dt-contrib-\nversion\n.jar:path/to/dt-library-\nversion\n.jar\n\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nPAM\n\n\nPAM (Pluggable Authentication Module) is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules, \nthey can be used in Gateway as well.\n\n\nPAM is implemented in C language and has a C API. JPam is a Java PAM bridge\nthat uses JNI to interface with PAM. It is available at\n\nJPAM website\n and the website also has detailed documentation on how to install and set it up. JPam has a JAAS login module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS login module is available.\n\n\nTo enable JPAM follow the JAAS configuration steps described above with the following specific details.\n\n\n\n\n\n\nJPAM has to be first installed on the system. Please follow the\n    installation instructions from the JPAM website.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JPAM authentication module. You can choose a  name like \nnet-sf-jpam\n which JPAM typically uses as the PAM configuration name. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nnet-sf-jpam\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks \n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate JPAM settings . A\n    sample configuration is shown below\n\n\nnet-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case,\n\nnet-sf-jpam\n must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM related settings. The setting \nserviceName\n specifies the PAM service which would need to be further configured in /etc/pam.d/net-sf-jpam to specify the PAM modules to use. Refer to PAM documentation on how to configure a PAM service with PAM modules. If using Linux local accounts system-auth could be specified as the PAM module in this file. The above settings are only provided as a reference example and a different name can be chosen for \nserviceName\n.\n\n\n\n\n\n\nFor step 4, add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format \nJPam-\nversion\n.jar\n where\n   \nversion\n denotes the version, version 1.1 has been tested.\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/JPam-\nversion\n.jar\n\n\n\n\n\n\n\nRestart the gateway as described in step 5 above    \n\n\n\n\n\n\nGroup Support\n\n\nFor group support such as using LDAP groups for authorization refer to\nthe \nAuthorization using external roles\n section below.\n\n\nHadoop user mapping\n\n\nWhen authentication is enabled, applications are launched on the Hadoop cluster, by default, as the user matching the user name of the authenticated user. This mapping behavior, to select the user to use on the Hadoop side, is configurable and different options are available. To configure the behavior use the following setting\n\n\nproperty\n\n  \nname\ndt.gateway.hadoop.user.strategy\n/name\n\n  \nvalue\nSTRATEGY\n/value\n\n\n/property\n\n\n\n\n\nThe \nSTRATEGY\n in the property above identifies the mapping behavior and the following options are available\n\n\nAuthenticated user\n\n\nTo specify this behavior, use \nAUTH_USER\n as the value for \nSTRATEGY\n. As explained earlier, this leads to applications being launched with the same user name as the authenticated user. This is the default behavior even when the property is not specified.\n\n\nIn Kerberos secure mode, DT Gateway would still connect to Hadoop by using its Kerberos credentials, namely credentials of the user the DT Gateway service is running under (default \ndtadmin\n) but impersonate the authenticated user, to launch the application. This impersonation requires additional configuration on Hadoop side and it is explained in the Hadoop Configuration sub-section under the Impersonation section in the Apache Apex security \ndocument\n.\n\n\nGateway user\n\n\nTo specify this behavior, use \nGATEWAY_USER\n as the value for \nSTRATEGY\n. In this mode, the Hadoop user is the same as the DT Gateway service user (default \ndtadmin\n) and no impersonation is necessary.\n\n\nSpecified user\n\n\nTo specify this behavior, use \nSPECIFIED_USER\n as the value for \nSTRATEGY\n. In this mode, a specific user name can be specified for the Hadoop user and it is used no matter who the authenticated user is. The Hadoop user name is specified by an additional property\n\n\nproperty\n\n  \nname\ndt.gateway.hadoop.user.name\n/name\n\n  \nvalue\nusername\n/value\n\n\n/property\n\n\n\n\n\nThe impersonation behavior described in the \nAuthenticated user\n section above applies here as well and so do the requirements mentioned there.\n\n\nAuthorization\n\n\nWhen any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.\n\n\nPermissions\n\n\nThe list of all possible permissions in the DT Gateway is as follow:\n\n\nACCESS_RM_PROXY\n\n\nAllow HTTP proxying requests to YARN\u2019s Resource Manager REST API\n\n\nEDIT_GLOBAL_CONFIG\n\n\nEdit global settings\n\n\nEDIT_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\nLAUNCH_APPS\n\n\nLaunch Apps\n\n\nMANAGE_LICENSES\n\n\nManage DataTorrent RTS licenses\n\n\nMANAGE_OTHER_USERS_APPS\n\n\nManage (e.g. edit, kill, etc) applications launched by other users\n\n\nMANAGE_OTHER_USERS_APP_PACKAGES\n\n\nManage App Packages uploaded by other users  \n\n\nMANAGE_ROLES\n\n\nManage roles (create/delete roles, or assign permissions to roles)\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nManage system alerts\n\n\nMANAGE_USERS\n\n\nManage users (create/delete users, change password)\n\n\nUPLOAD_APP_PACKAGES\n\n\nUpload App Packages and use the app builder\n\n\nVIEW_GLOBAL_CONFIG\n\n\nView global settings   \n\n\nVIEW_LICENSES\n\n\nView DataTorrent RTS licenses\n\n\nVIEW_OTHER_USERS_APPS\n\n\nView applications launched by others\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nView App Packages uploaded by other users\n\n\nVIEW_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\nVIEW_SYSTEM_ALERTS\n\n\nView system alerts\n\n\nDefault Roles\n\n\nDataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.\n\n\nAdmin\n\n\nAn administrator of DataTorrent RTS is intended to be able to install,\nmanage \n modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.\n\n\nOperator\n\n\nOperators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.\n\n\nHere is the list of default permissions given to operators\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_OTHER_USERS_APPS\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nNote that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.  \n\n\nDeveloper\n\n\nDevelopers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.\n\n\nHere is the list of default permissions given to developers\n\n\nLAUNCH_APPS\n\n\nUPLOAD_APP_PACKAGES\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nApp Permissions and App Package Permissions\n\n\nUsers can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.\n\n\nThe default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.\n\n\nThe Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the \nDT Gateway REST API document\n and \nhere\n for examples on how to use the REST API.\n\n\nViewing and Managing Auth in the Console\n\n\nViewing User Profile\n\n\nAfter you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.\n\n\n\nAdministering Auth\n\n\nFrom the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:\n\n\n\n\nCreate new users\n\n\nDelete users\n\n\nChange existing users\u2019 passwords\n\n\nAssign roles to users\n\n\nCreate roles\n\n\nAssign permissions to roles\n\n\n\n\n\n\nDataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.\n\n\nAuthorization using external roles\n\n\nWhen using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections \nKerberos roles\n and\n\nJAAS roles\n. Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the \nExternal Role Mapping\n section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.\n\n\nKerberos roles\n\n\nWhen Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form \nuser/group@DOMAIN\n\nthe group portion is used as the external role and no additional\nconfiguration is necessary.\n\n\nJAAS roles\n\n\nTo use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the JAAS plugin login module. If the module supports roles then some of these principals are for roles and these role principals need to be identified from the list. Additional configuration is needed to do\nthis and it is specific to the login module implementation. Specifically the Java class name identifying the role principal is needed. This can be specified using the \ndt.gateway.http.authentication.jaas.role.class.name\n property in the \ndt-site.xml\n configuration file as shown below\n\n\nconfiguration\n\n    ...\n    \nproperty\n\n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n        \nvalue\nfull-class-name-of-role\n/value\n\n    \n/property\n\n    ...\n\n/configuration\n\n\n\n\n\nLDAP Groups\n\n\nWhen using LDAP with JAAS, to utilize the LDAP roles, a LDAP login module supporting roles should be used. Any LDAP module that supports roles can be used. Jetty login module has support for roles. Refer to \nJetty module \n binding\n section above for more details about this module. The configuration steps are as follows.\n\n\n\n\n\n\nThe Jetty login module returns the roles in role principal classes. The class name identifying the role is \norg.eclipse.jetty.plus.jaas.JAASRole\n. This should be specified using the \ndt.gateway.http.authentication.jaas.role.class.name\n property in the \ndt-site.xml\n configuration file as described in the section above. \n\n\nAlso as described in the \nJetty module \n binding\n section a custom callback handler is needed for the Jetty login module.\n\n\nA sample configuration file with all the properties is shown below\n\n\nconfiguration\n\n...\n  \nproperty\n\n          \nname\ndt.gateway.http.authentication.type\n/name\n\n          \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n         \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n         \nvalue\nldap\n/value\n\n  \n/property\n\n  \nproperty\n  \n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n        \nvalue\norg.eclipse.jetty.plus.jaas.JAASRole\n/value\n\n  \n/property\n\n  \n/property\n\n        \nname\n\n                dt.gateway.http.authentication.jaas.callback.class.name\n        \n/name\n\n        \nvalue\n\n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n        \n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nIf the system identity is used then the Jetty login module can be used as is and the jar dependencies in the step below can be specified as described in the \nJetty module \n binding\n section. \n\n\nHowever, if system identity is not used something different needs to be done. An issue was discovered with the Jetty login module supplied with Jetty 8 that prevented LDAP authentication to be successful even when the user credentials were correct. DataTorrent has a fix for this and is providing the login module with the fix in a separate package called \ndt-auth\n. The class name for the module is \ncom.datatorrent.auth.jetty.JettyLdapLoginModule\n. The \ndt-auth\n project along with the source can be found here \nAuth\n. DataTorrent is working on submitting this fix back to Jetty project so that it gets back into the main source.\n\n\nThe JAAS configuration file as described in \nLDAP\n section under \nEnabling JAAS Auth\n should be configured to specify the ldap settings for roles. A sample configuration containing role settings for the \ndt-auth\n login module is shown below. \n\n\nldap {\n    com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" port=\"389\"\n    authenticationMethod=\"simple\" \n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\"cn\"\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\";\n};\n\n\n\nThe \nroleNameAttribute\n and \nroleBaseDn\n settings are used to identify the role and the \nuserRdnAttribute\n setting is used the identify the users that belong to the role. The values for these settings are dependent on attributes names are being used in your LDAP directory server.\n\n\nSimilar configuration can be used when the original Jetty login module is being used with the system id, the module class would be Jetty login module class and the binding settings would be specified as described in the \nJetty module \n binding\n section.\n\n\nRefer to the \njavadoc\n for the role and other configuration options available with this module.\n\n\n\n\n\n\nIf the Jetty login module is being used as is then the path specification instructions described in the \nJetty module \n binding\n section can be used. \n\n\nIf the login module containing the DataTorrent fix, dt-auth, is being used then it's jar along with the Jetty module dependencies containing the role class, the dt-contrib jar containing the custom callback handler along with its dependencies should all be made available for Gateway. The jars can be obtained from the \nDataTorrent Auth\n project.\n\n\nPlease follow the instructions in the above url to obtain the project jar files. After obtaining the jar files they can be specified in the gateway script file as\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway as described earlier\n\n\n\n\n\n\nExternal Role Mapping\n\n\nExternal role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps\n\n\n\n\n\n\nIn the configuration folder typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is\n\n\nadmins:admin\nstaff:developer\n\n\n\nThis maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.\n\n\n\n\n\n\nRestart the Gateway as described earlier\n\n\n\n\n\n\nAdministering Using Command Line\n\n\nYou can also utilize the \ndtGateway REST API\n (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.\n\n\nLog in as admin:\n\n\n% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login\n\n\n\nThis curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar\n\n\nChanging the admin password:\n\n\n% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin\n\n\n\nThis uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.\n\n\nAdding a second admin user:\n\n\n% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.\n\n\nAdding a user in the developer role:\n\n\n% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.\n\n\nListing all users:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users\n\n\n\nGetting info for a specific user:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command returns the information about the user \u201cjohn\u201d.\n\n\nRemoving a user:\n\n\n% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command removes the user \u201cjane\u201d.\n\n\nSetting up SSL Keystore in Gateway\n\n\nAn SSL keystore is needed for your gateway to enable HTTPS in the gateway and to configure SMTP with password authentication. SMTP configuration is needed for the gateway to \nsend email alerts as described in \nSystem Alerts\n. \n\n\nFollow the steps below to set up the keystore.\n\n\n\n\n\n\nGenerate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here: \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n.\nNote down the keystore password and full path of the keystore that you need to provide in the next step.\n\n\n\n\n\n\nAdd two properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n (or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \n!-- this is the full path to the SSL keystore you created --\n\n      \nname\ndt.gateway.sslKeystorePath\n/name\n\n      \nvalue\n{/path/to/keystore}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \n!-- this is the keystore password --\n\n      \nname\ndt.gateway.sslKeystorePassword\n/name\n\n      \nvalue\n{keystore-password}\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nPerform any additional steps required for \"Enabling HTTPS\" or \"SMTP Password Encryption\" use-case as described below.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n(when running Gateway in local mode use \ndtgateway restart\n command)\n\n\n\n\n\n\nEnabling HTTPS in Gateway\n\n\nTo enable HTTPS in the Gateway after setting up the keystore as described above, you have to add the following\nproperty to the \ndt-site.xml\n configuration file\n\n\n    \nconfiguration\n\n    ...\n      \nproperty\n\n        \nname\ndt.attr.GATEWAY_USE_SSL\n/name\n\n        \nvalue\ntrue\n/value\n\n      \n/property\n\n    ...\n    \n/configuration\n\n\n\n\nSetting up a Key for SMTP Password Encryption\n\n\nAdd another key to the keystore created above using the instructions mentioned in \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n. For example:\n\n\nkeytool -genkey -alias smtpenc-alias -keyalg RSA -keypass \nkey password\n -storepass \nstore password\n -keystore gwkeystore.jks\n\n\n\n\nNote down the alias you used for the key (smtpenc-alias in the above command). Remember to use the same key password and \nstore password as the ones you used in the previous command where you created the keystore.\n\n\nAdd the following property to \ndt-site.xml\n to indicate the alias to be used for SMTP Password Encryption:\n\n\n    \nconfiguration\n\n    ...\n     \nproperty\n\n       \nname\ndt.gateway.ssl.alias.password.encryption\n/name\n\n       \nvalue\nsmtpenc-alias\n/value\n\n     \n/property\n\n    ...\n    \n/configuration", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway_security/#datatorrent-gateway-security", 
            "text": "DataTorrent Gateway is a service that provides the backend functionality\nfor the DataTorrent UI Console and processes the web service requests from it. The service provides real-time information about running applications, allows changes to applications, launches new applications among various other operations. Refer to  dtGateway  for details on the Gateway service and  dtManage \u00a0for the UI Console.  Broadly security in Gateway can be classified into two categories, frontend security and backend security. Frontend security deals with access to Gateway service which mainly involves securing the web service calls. This includes aspects such as user authentication and authorization. The backend security deals with security aspects when Gateway is communicating with a secure Hadoop infrastructure.  After installation of DataTorrent RTS both these aspects can be configured manually as described in the following sections although work is being done to enable this configuration in the UI Console during installation itself and post installation.", 
            "title": "DataTorrent Gateway Security"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-secure-mode", 
            "text": "Kerberos is the de-facto authentication mechanism supported in Hadoop. When secure mode is enabled in Hadoop, requests from clients to Hadoop are authenticated using Kerberos. In this mode Gateway service needs Kerberos credentials to communicate with Hadoop. The credentials should match the user that the DT Gateway service is running under.  In a multi-user installation DT Gateway is typically running as\nuser  dtadmin  and the Kerberos credentials specified should be for this\nuser. They are specified in the  dt-site.xml  configuration file located in the config folder under the installation which is typically  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install). For a single user installation where gateway is running as the user, the Kerberos credentials will be the user\u2019s credentials.  The snippet below shows how the credentials can be specified in the\nconfiguration file.  property \n         name dt.gateway.authentication.principal /name \n         value kerberos-principal-of-gateway-user /value  /property  property \n         name dt.gateway.authentication.keytab /name \n         value absolute-path-to-keytab-file /value  /property", 
            "title": "Kerberos Secure Mode"
        }, 
        {
            "location": "/dtgateway_security/#long-running-applications", 
            "text": "In secure mode, long running applications have additional requirements. Refer to the Token Refresh section in the Apache Apex security  document .", 
            "title": "Long running applications"
        }, 
        {
            "location": "/dtgateway_security/#authentication", 
            "text": "DataTorrent Gateway has support for authentication and when it is configured users have to authenticate before they can access the UI Console. Various authentication mechanisms are supported and this gives enterprises the flexibility to extend their existing authentication mechanism already in use within the enterprise to Gateway. It also supports roles, mapping of groups or roles from the external authentication mechanism to roles and supports role based authorization.  The different authentication mechanisms supported by Gateway are   Password Authentication  LDAP Authentication  Kerberos Authentication  JAAS Authentication  for Active Directory, PAM, etc   JAAS is a extensible authentication framework that supports different types of authentication mechanisms by plugging in an appropriate module.", 
            "title": "Authentication"
        }, 
        {
            "location": "/dtgateway_security/#password-authentication", 
            "text": "Password security is simple to set up and is ideal for a small to medium set of users. It comes with role-based access control, so users can be assigned roles, and roles can be assigned granular permissions (see  User Management ). This is the only authentication mechanism available that does not depend on any external systems. The users will be managed locally by the Gateway. When enabled, all users will be presented with the login prompt before being able to use the DT Console.  To set up password security, on the  Security Configuration  page select  Password  from the  Authentication Type  dropdown, and save. Allow the Gateway to restart.   When the Gateway has restarted, you should be prompted for username and password. Log in as the default admin user  dtadmin  with password  dtadmin .   Once authenticated, active username and an option to log out is presented in the top right corner of the DT Console screen.   Additional users and roles can be created and managed on the  User Management  page.  Note : Don't forget to change your  dtadmin  user's password!", 
            "title": "Password Authentication"
        }, 
        {
            "location": "/dtgateway_security/#password-authentication-via-dt-sitexml", 
            "text": "Password authentication can alternatively be configured outside the Console by performing following two steps:    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n     property \n     name dt.gateway.http.authentication.type /name \n     value password /value \n     /property \n... /configuration     Restart the Gateway. If running Gateway in local mode use  dtgateway restart  instead.  sudo service dtgateway restart", 
            "title": "Password Authentication via dt-site.xml"
        }, 
        {
            "location": "/dtgateway_security/#ldap-authentication", 
            "text": "LDAP is a directory based authentication mechanism used in many enterprises. If your organization uses LDAP for authentication, the LDAP security option is ideal for giving your existing users access to RTS, with the role-based access control and group mapping features.  There are four variations for configuring LDAP authentication:   Identity  Provide the parent DN of your users and  specify the RDN attribute of users .  Users will authenticate using their RDN attribute value as their username.    Anonymous   User Search Filter  Provide the parent DN of your users and  specify a search filter to identify users .  Users will authenticate using an appropriate username that matches the parameters defined in the search filter.    Identity and Anonymous Search  Provide the parent DN of your users,  specify the RDN attribute of users, and a search filter .  Users will authenticate using their RDN attribute value as their username, as well as the parameters defined in the search filter.    Non-Anonymous Search with Group Support  Provide basic DN info for users, DN and password of a user able to perform a non-anonymous search, and optional group support info.  With  group support disabled , users need to be added in User Management before logging in. Users authenticate with their RDN attribute value as their username.  With  group support enabled , users do not have to be added in User Management before logging in. Users authenticate with their RDN attribute value as their username. They will be assigned roles mapped to their LDAP group. For example, user Peter is part of GroupA (admin) and GroupB (developer, operator); Peter will be assigned roles admin, developer, and operator upon login.     When group support is not configured, users must be assigned a role before they are able to log in. This means users can be restricted from logging in (blacklisted) by removing all of their roles.  Note : If migrating from  Password  mode, the existing users will be carried over as \"local users\" and can still login as if in  Password  mode. It is recommended to keep only the  dtadmin  user and delete the rest. This is because local users cannot be added or deleted once  LDAP  mode is activated.   After setting up LDAP in the Security Configuration page, the  LDAP Users  section will appear in the User Management page. If you have group support enabled, the  LDAP Groups  section will also appear. Existing users (carried over from  Password  mode), will be placed in the  Local Users  sections. Local users cannot be added or deleted in LDAP mode, but their roles and passwords can be modified.   To configure LDAP via dt-site.xml, check out the  JAAS Authentication - LDAP  section below.", 
            "title": "LDAP Authentication"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-authentication", 
            "text": "Kerberos authentication can optionally be enabled for Hadoop web access.\nWhen configured, all web browser access to Hadoop management consoles are Kerberos authenticated. Web services are also Kerberos authenticated. This authentication is performed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.  Kerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by logging in to kerberos system in a terminal emulator using kinit. Then, user launches a browser to access the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.  When this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the  External Role Mapping  in the  Authorization using external roles  section below for that.  Additional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install)  configuration \n...\n   property \n     name dt.gateway.http.authentication.type /name \n     value kerberos /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.principal /name \n     value {kerberos-principal-of-web-service} /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.keytab /name \n     value {absolute-path-to-keytab-file} /value \n   /property \n   property \n     name dt.gateway.http.authentication.token.validity /name \n     value {authentication-token-validity-in-seconds} /value \n   /property \n   property \n     name dt.gateway.http.authentication.cookie.domain /name \n     value {http-cookie-domain-for-authentication-token} /value \n   property \n     name dt.gateway.http.authentication.cookie.path /name \n     value {http-cookie-path} /value \n   /property \n   property \n     name dt.gateway.http.authentication.signature.secret /name \n     value {absolute-path-of-secret-file-for-signing-authentication-tokens}  /value \n   /property  /configuration   Note that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property dt.gateway.http.authentication.type  should be replaced with the\nappropriate values for your setup.    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/dtgateway_security/#jaas-authentication", 
            "text": "JAAS or Java Authentication and Authorization Service is a pluggable\nand extensible mechanism for authentication. It is an authentication framework\nwhere the actual authentication is performed by a JAAS login module plugin which can be configured using a configuration file.   The general configuration steps for enabling any JAAS based authentication mechanism is described below. Subsequent sections will cover the specific configuration details for LDAP, Active Directory and PAM. However JAAS is not limited to just these three mechanisms. Any other JAAS compatible mechanism can be used by specifying the appropriate module in the configuration. Also if there isn't a JAAS module available for an authentication mechanism a new one can be developed using the JAAS API and used here.  Below are the general steps for configuring a JAAS authentication mechanism    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  (or  ~/datatorrent/current/conf  for local install).  configuration \n    ...\n   property \n       name dt.gateway.http.authentication.type /name \n       value jaas /value \n   /property \n   property \n       name dt.gateway.http.authentication.jaas.name /name \n       value name-of-jaas-module /value \n   /property \n    ... /configuration   The  dt.gateway.http.authentication.jaas.name  property specifies the login module to use with JAAS and the next step explains the process for configuring it.    If the login module requires a custom callback handler it needs to be specified. What a callback handler is and how it can be specified is described in the next section  Callback Handlers .    The name of the login module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be  /home/dtadmin/.java.login.config , if running as a\n    regular user it would be  ~/.java.login.config . The sample\n    configurations for LDAP and PAM are shown in the next sections.    The classes for the login module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.  The following step shows how to do this  a.  Edit the  custom-env.sh  configuration file, typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart \n(when running Gateway in local mode use   dtgateway restart  command)    Similar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the External Role Mapping  in the  Authorization using external roles  section below for that.", 
            "title": "JAAS Authentication"
        }, 
        {
            "location": "/dtgateway_security/#callback-handlers", 
            "text": "In JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. These callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS provides a default callback handler that handles the common callbacks but it may\nnot be sufficient for all login modules like in the Active Directory case described below.  RTS also supports specification of a custom callback handler to handle custom callbacks of a login module. The custom callback handler can be specified using the property  dt.gateway.http.authentication.jaas.callback.class.name  and when this property is not specified the default callback handler is used. The property can be specified in the  dt-site.xml  configuration file and as follows  configuration \n    ...\n     property  \n             name dt.gateway.http.authentication.jaas.callback.class.name /name \n         value full-class-name-of-callback /value \n     /property \n    ... /configuration   Custom callback handlers can be implemented by extending the default callback handler so they get the support for the common callbacks or they can be implemented from scratch. The Jetty callback handler described below also extends the default callback handler. The default callback handler implementation is implemented in the  com.datatorrent.lib.security.auth.callback.DefaultCallbackHandler  class available in  com.datatorrent:dt-library  artifact.", 
            "title": "Callback Handlers"
        }, 
        {
            "location": "/dtgateway_security/#ldap", 
            "text": "An alternative way to enable LDAP authentication, instead of  LDAP Authentication  section above, is to follow the JAAS configuration steps described above with the following specific details for the individual steps.    For step 1 of the JAAS authentication  configuration process , pick a name for the JAAS module for LDAP. You can choose a name like  ldap  that is appropriate for the current scheme. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  ldap . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.    For step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate LDAP settings. There are different JAAS authentication modules available for LDAP. One of them is supplied by default in Java. It is the Sun LdapLoginModule class. A sample configuration when using this module is shown below  ldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};  Note that the first string before the open brace, in this case ldap  must match the jaas name specified in step 1. The first property com.sun.security.auth.module.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The fields that appear next are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.  The setting  authIdentity  is used to derive the identity of the user that will be used for authentication with the server. Here it is specifying the pattern to derive the LDAP distinguished name (dn) for the user that will be used along with the password supplied by the user for authentication with the LDAP server. The  {USERNAME}  variable in the setting will be replaced by the username specified by the user.   Refer to the  javadoc  for all the configuration options available with this module.    For step 4, no extra jars are needed as the module is available in Java    Restart the gateway as described in step 5", 
            "title": "LDAP"
        }, 
        {
            "location": "/dtgateway_security/#active-directory", 
            "text": "Active Directory is used when authenting users in Microsoft Windows domains. The authentication protocol includes Microsoft's implementation of Kerberos as well as LDAP. In this section we will look into the configuration needed for LDAP authentication with Active Directory.  Follow the JAAS configuration steps described above with the following specific details.    For step 1 of the JAAS authentication  configuration process , pick a name for the JAAS module. You can choose a name like  ad  that is appropriate for the current scheme. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  ad . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.     For step 3, the JAAS name specified above should be configured with\n   the appropriate Active Directory settings in the .java.login.config file.   In active directory authentication the user id is typically specified as  username@domain  and sometimes just  username . Once the user is authenticated the user id needs to be mapped to the LDAP node for the user. This is done by specifying a search filter criteria which is then used to search for the user node in the directory tree starting from a specified base.  Below is an example showing a sample configuration for active directory authentication with the Sun LdapLoginModule  ad {\n    com.sun.security.auth.module.LdapLoginModule required\n    userProvider=\"ldap://ad.server.com/cn=users,dc=domain,dc=com\"\n    userFilter=\"samAccountName={USERNAME}\"\n    authIdentity=\"{USERNAME}@my.domain.com\";\n};  The setting  userFilter  specifies the search criteria to look for the user in the Active Directory tree. The  {USERNAME}  variable in the setting is replaced with the username supplied by the user. The setting  authIdentity  specifies the pattern to derive the user id with the domain from the specified username.  The search filter can have more complex logical expressions like the one shown below  userFilter=\"( (|(samAccountName={USERNAME})(userPrincipalName={USERNAME})(cn={USERNAME}))(objectClass=user))\"  An Active Directory or a LDAP browser client can be used to browse the Active Directory to determine the correct filter to use find the users.    For step 4, no extra jars are needed as the module is available in Java    Restart the gateway as described in step 5", 
            "title": "Active Directory"
        }, 
        {
            "location": "/dtgateway_security/#jetty-module-binding", 
            "text": "Sometimes a single  authIdentity  pattern like the one used above cannot be used to identity the users because of the way they are configured in the Active Directory, for an enterprise. An example is the case where multiple domains are being used and a single domain name cannot be specified in the  authIdentity .   In those cases a fixed identity called the root identity or a system identity is used to connect to the server. Then the user node is searched with the search criteria and supplied username, from the node the user directory name is extracted and it is used along with the supplied password for the authentication.  A different JAAS login module that supports the system identity is needed as the Sun LdapLoginModule does not support this functionality. Jetty implements one such login module. The steps to configure this module are as follows.    For step 1 of the JAAS authentication  configuration process , the name  ad  can be used.    For step 2, a special callback handler is needed to handle the Jetty module callbacks as they are not the common callbacks. There is an implementation of the callback handler for Jetty that is provided by DataTorrent RTS. It is the  com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler  class in the  com.datatorrent:dt-contrib  artifact.  As explained in the  Callback Handlers  section above, the callback handler can be specified in the  dt-site.xml  config file using a property as shown below  configuration \n    ...\n     property  \n         name \n            dt.gateway.http.authentication.jaas.callback.class.name\n     /name \n         value \n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n         /value \n     /property \n    ...   /configuration     For step 3, the configuration for the Jetty module is specified in the  .java.login.config  file. A sample configuration for this module is shown below  ad {\n    org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\"\n    hostname=\"ad.server.com\"\n    port=\"389\"\n    bindDn=\"serviceId@my.domain.com\"\n    bindPassword=\"Password1\"\n    forceBindingLogin=\"true\"\n    userBaseDn=\"DC=my,DC=domain,DC=com\"\n    userIdAttribute=\"samAccountName\"\n    userObjectClass=\"person\";\n};  The property  org.eclipse.jetty.plus.jaas.spi.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP authentication. The properties  bindDn  and  bindPassword  specify the directory name and password for the system identity respectively. The  userIdAttribute  and  userObjectClass  settings are used as search criteria to search for the user directory node under the  userBaseDn  path in the directory.  Refer to the  javadoc  for all the configuration options available with this module.    The Gateway service in DataTorrent RTS 3.0 is compatible with Jetty 8. The Jetty login module in this version is in the  jetty-plus  jar. The following version  jetty-plus-8.1.10.v20130312.jar  is known to work but other compatible versions should work as well. The jar file can be obtained from the Jetty project or from  maven central .   Along with the jetty jar the dt-contrib jar containing the callback handler and it's dependencies should also be included. It is available on the DataTorrent maven server  here .  Pick the version matching the DataTorrent RTS version you are using. The dt-contrib artifact has a dependency to  com.datatorrent:dt-library  artifact. It is also available on the maven server  here .  The jars can be specified in the gateway script file described in step 4 above as follows  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jetty-plus-8.1.10.v20130312.jar:path/to/dt-contrib- version .jar:path/to/dt-library- version .jar    Restart the gateway as described in step 5", 
            "title": "Jetty module &amp; binding"
        }, 
        {
            "location": "/dtgateway_security/#pam", 
            "text": "PAM (Pluggable Authentication Module) is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules, \nthey can be used in Gateway as well.  PAM is implemented in C language and has a C API. JPam is a Java PAM bridge\nthat uses JNI to interface with PAM. It is available at JPAM website  and the website also has detailed documentation on how to install and set it up. JPam has a JAAS login module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS login module is available.  To enable JPAM follow the JAAS configuration steps described above with the following specific details.    JPAM has to be first installed on the system. Please follow the\n    installation instructions from the JPAM website.    For step 1 of the JAAS authentication  configuration process , pick a name for the JPAM authentication module. You can choose a  name like  net-sf-jpam  which JPAM typically uses as the PAM configuration name. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  net-sf-jpam . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks     For step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate JPAM settings . A\n    sample configuration is shown below  net-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};    Note that the first string before the open brace, in this case, net-sf-jpam  must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM related settings. The setting  serviceName  specifies the PAM service which would need to be further configured in /etc/pam.d/net-sf-jpam to specify the PAM modules to use. Refer to PAM documentation on how to configure a PAM service with PAM modules. If using Linux local accounts system-auth could be specified as the PAM module in this file. The above settings are only provided as a reference example and a different name can be chosen for  serviceName .    For step 4, add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format  JPam- version .jar  where\n    version  denotes the version, version 1.1 has been tested.  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/JPam- version .jar    Restart the gateway as described in step 5 above", 
            "title": "PAM"
        }, 
        {
            "location": "/dtgateway_security/#group-support", 
            "text": "For group support such as using LDAP groups for authorization refer to\nthe  Authorization using external roles  section below.", 
            "title": "Group Support"
        }, 
        {
            "location": "/dtgateway_security/#hadoop-user-mapping", 
            "text": "When authentication is enabled, applications are launched on the Hadoop cluster, by default, as the user matching the user name of the authenticated user. This mapping behavior, to select the user to use on the Hadoop side, is configurable and different options are available. To configure the behavior use the following setting  property \n   name dt.gateway.hadoop.user.strategy /name \n   value STRATEGY /value  /property   The  STRATEGY  in the property above identifies the mapping behavior and the following options are available", 
            "title": "Hadoop user mapping"
        }, 
        {
            "location": "/dtgateway_security/#authenticated-user", 
            "text": "To specify this behavior, use  AUTH_USER  as the value for  STRATEGY . As explained earlier, this leads to applications being launched with the same user name as the authenticated user. This is the default behavior even when the property is not specified.  In Kerberos secure mode, DT Gateway would still connect to Hadoop by using its Kerberos credentials, namely credentials of the user the DT Gateway service is running under (default  dtadmin ) but impersonate the authenticated user, to launch the application. This impersonation requires additional configuration on Hadoop side and it is explained in the Hadoop Configuration sub-section under the Impersonation section in the Apache Apex security  document .", 
            "title": "Authenticated user"
        }, 
        {
            "location": "/dtgateway_security/#gateway-user", 
            "text": "To specify this behavior, use  GATEWAY_USER  as the value for  STRATEGY . In this mode, the Hadoop user is the same as the DT Gateway service user (default  dtadmin ) and no impersonation is necessary.", 
            "title": "Gateway user"
        }, 
        {
            "location": "/dtgateway_security/#specified-user", 
            "text": "To specify this behavior, use  SPECIFIED_USER  as the value for  STRATEGY . In this mode, a specific user name can be specified for the Hadoop user and it is used no matter who the authenticated user is. The Hadoop user name is specified by an additional property  property \n   name dt.gateway.hadoop.user.name /name \n   value username /value  /property   The impersonation behavior described in the  Authenticated user  section above applies here as well and so do the requirements mentioned there.", 
            "title": "Specified user"
        }, 
        {
            "location": "/dtgateway_security/#authorization", 
            "text": "When any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.", 
            "title": "Authorization"
        }, 
        {
            "location": "/dtgateway_security/#permissions", 
            "text": "The list of all possible permissions in the DT Gateway is as follow:", 
            "title": "Permissions"
        }, 
        {
            "location": "/dtgateway_security/#access_rm_proxy", 
            "text": "Allow HTTP proxying requests to YARN\u2019s Resource Manager REST API", 
            "title": "ACCESS_RM_PROXY"
        }, 
        {
            "location": "/dtgateway_security/#edit_global_config", 
            "text": "Edit global settings", 
            "title": "EDIT_GLOBAL_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#edit_other_users_config", 
            "text": "Edit other users\u2019 settings", 
            "title": "EDIT_OTHER_USERS_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#launch_apps", 
            "text": "Launch Apps", 
            "title": "LAUNCH_APPS"
        }, 
        {
            "location": "/dtgateway_security/#manage_licenses", 
            "text": "Manage DataTorrent RTS licenses", 
            "title": "MANAGE_LICENSES"
        }, 
        {
            "location": "/dtgateway_security/#manage_other_users_apps", 
            "text": "Manage (e.g. edit, kill, etc) applications launched by other users", 
            "title": "MANAGE_OTHER_USERS_APPS"
        }, 
        {
            "location": "/dtgateway_security/#manage_other_users_app_packages", 
            "text": "Manage App Packages uploaded by other users", 
            "title": "MANAGE_OTHER_USERS_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#manage_roles", 
            "text": "Manage roles (create/delete roles, or assign permissions to roles)", 
            "title": "MANAGE_ROLES"
        }, 
        {
            "location": "/dtgateway_security/#manage_system_alerts", 
            "text": "Manage system alerts", 
            "title": "MANAGE_SYSTEM_ALERTS"
        }, 
        {
            "location": "/dtgateway_security/#manage_users", 
            "text": "Manage users (create/delete users, change password)", 
            "title": "MANAGE_USERS"
        }, 
        {
            "location": "/dtgateway_security/#upload_app_packages", 
            "text": "Upload App Packages and use the app builder", 
            "title": "UPLOAD_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#view_global_config", 
            "text": "View global settings", 
            "title": "VIEW_GLOBAL_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#view_licenses", 
            "text": "View DataTorrent RTS licenses", 
            "title": "VIEW_LICENSES"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_apps", 
            "text": "View applications launched by others", 
            "title": "VIEW_OTHER_USERS_APPS"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_app_packages", 
            "text": "View App Packages uploaded by other users", 
            "title": "VIEW_OTHER_USERS_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_config", 
            "text": "Edit other users\u2019 settings", 
            "title": "VIEW_OTHER_USERS_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#view_system_alerts", 
            "text": "View system alerts", 
            "title": "VIEW_SYSTEM_ALERTS"
        }, 
        {
            "location": "/dtgateway_security/#default-roles", 
            "text": "DataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.", 
            "title": "Default Roles"
        }, 
        {
            "location": "/dtgateway_security/#admin", 
            "text": "An administrator of DataTorrent RTS is intended to be able to install,\nmanage   modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.", 
            "title": "Admin"
        }, 
        {
            "location": "/dtgateway_security/#operator", 
            "text": "Operators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.  Here is the list of default permissions given to operators  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_OTHER_USERS_APPS  VIEW_OTHER_USERS_APP_PACKAGES  VIEW_SYSTEM_ALERTS  Note that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.", 
            "title": "Operator"
        }, 
        {
            "location": "/dtgateway_security/#developer", 
            "text": "Developers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.  Here is the list of default permissions given to developers  LAUNCH_APPS  UPLOAD_APP_PACKAGES  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_SYSTEM_ALERTS", 
            "title": "Developer"
        }, 
        {
            "location": "/dtgateway_security/#app-permissions-and-app-package-permissions", 
            "text": "Users can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.  The default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.  The Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the  DT Gateway REST API document  and  here  for examples on how to use the REST API.", 
            "title": "App Permissions and App Package Permissions"
        }, 
        {
            "location": "/dtgateway_security/#viewing-and-managing-auth-in-the-console", 
            "text": "", 
            "title": "Viewing and Managing Auth in the Console"
        }, 
        {
            "location": "/dtgateway_security/#viewing-user-profile", 
            "text": "After you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.", 
            "title": "Viewing User Profile"
        }, 
        {
            "location": "/dtgateway_security/#administering-auth", 
            "text": "From the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:   Create new users  Delete users  Change existing users\u2019 passwords  Assign roles to users  Create roles  Assign permissions to roles    DataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.", 
            "title": "Administering Auth"
        }, 
        {
            "location": "/dtgateway_security/#authorization-using-external-roles", 
            "text": "When using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections  Kerberos roles  and JAAS roles . Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the  External Role Mapping  section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.", 
            "title": "Authorization using external roles"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-roles", 
            "text": "When Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form  user/group@DOMAIN \nthe group portion is used as the external role and no additional\nconfiguration is necessary.", 
            "title": "Kerberos roles"
        }, 
        {
            "location": "/dtgateway_security/#jaas-roles", 
            "text": "To use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the JAAS plugin login module. If the module supports roles then some of these principals are for roles and these role principals need to be identified from the list. Additional configuration is needed to do\nthis and it is specific to the login module implementation. Specifically the Java class name identifying the role principal is needed. This can be specified using the  dt.gateway.http.authentication.jaas.role.class.name  property in the  dt-site.xml  configuration file as shown below  configuration \n    ...\n     property \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n         value full-class-name-of-role /value \n     /property \n    ... /configuration", 
            "title": "JAAS roles"
        }, 
        {
            "location": "/dtgateway_security/#ldap-groups", 
            "text": "When using LDAP with JAAS, to utilize the LDAP roles, a LDAP login module supporting roles should be used. Any LDAP module that supports roles can be used. Jetty login module has support for roles. Refer to  Jetty module   binding  section above for more details about this module. The configuration steps are as follows.    The Jetty login module returns the roles in role principal classes. The class name identifying the role is  org.eclipse.jetty.plus.jaas.JAASRole . This should be specified using the  dt.gateway.http.authentication.jaas.role.class.name  property in the  dt-site.xml  configuration file as described in the section above.   Also as described in the  Jetty module   binding  section a custom callback handler is needed for the Jetty login module.  A sample configuration file with all the properties is shown below  configuration \n...\n   property \n           name dt.gateway.http.authentication.type /name \n           value jaas /value \n   /property \n   property \n          name dt.gateway.http.authentication.jaas.name /name \n          value ldap /value \n   /property \n   property   \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n         value org.eclipse.jetty.plus.jaas.JAASRole /value \n   /property \n   /property \n         name \n                dt.gateway.http.authentication.jaas.callback.class.name\n         /name \n         value \n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n         /value \n   /property \n... /configuration     If the system identity is used then the Jetty login module can be used as is and the jar dependencies in the step below can be specified as described in the  Jetty module   binding  section.   However, if system identity is not used something different needs to be done. An issue was discovered with the Jetty login module supplied with Jetty 8 that prevented LDAP authentication to be successful even when the user credentials were correct. DataTorrent has a fix for this and is providing the login module with the fix in a separate package called  dt-auth . The class name for the module is  com.datatorrent.auth.jetty.JettyLdapLoginModule . The  dt-auth  project along with the source can be found here  Auth . DataTorrent is working on submitting this fix back to Jetty project so that it gets back into the main source.  The JAAS configuration file as described in  LDAP  section under  Enabling JAAS Auth  should be configured to specify the ldap settings for roles. A sample configuration containing role settings for the  dt-auth  login module is shown below.   ldap {\n    com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" port=\"389\"\n    authenticationMethod=\"simple\" \n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\"cn\"\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\";\n};  The  roleNameAttribute  and  roleBaseDn  settings are used to identify the role and the  userRdnAttribute  setting is used the identify the users that belong to the role. The values for these settings are dependent on attributes names are being used in your LDAP directory server.  Similar configuration can be used when the original Jetty login module is being used with the system id, the module class would be Jetty login module class and the binding settings would be specified as described in the  Jetty module   binding  section.  Refer to the  javadoc  for the role and other configuration options available with this module.    If the Jetty login module is being used as is then the path specification instructions described in the  Jetty module   binding  section can be used.   If the login module containing the DataTorrent fix, dt-auth, is being used then it's jar along with the Jetty module dependencies containing the role class, the dt-contrib jar containing the custom callback handler along with its dependencies should all be made available for Gateway. The jars can be obtained from the  DataTorrent Auth  project.  Please follow the instructions in the above url to obtain the project jar files. After obtaining the jar files they can be specified in the gateway script file as  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway as described earlier", 
            "title": "LDAP Groups"
        }, 
        {
            "location": "/dtgateway_security/#external-role-mapping", 
            "text": "External role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps    In the configuration folder typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is  admins:admin\nstaff:developer  This maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.    Restart the Gateway as described earlier", 
            "title": "External Role Mapping"
        }, 
        {
            "location": "/dtgateway_security/#administering-using-command-line", 
            "text": "You can also utilize the  dtGateway REST API  (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.", 
            "title": "Administering Using Command Line"
        }, 
        {
            "location": "/dtgateway_security/#log-in-as-admin", 
            "text": "% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login  This curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar", 
            "title": "Log in as admin:"
        }, 
        {
            "location": "/dtgateway_security/#changing-the-admin-password", 
            "text": "% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin  This uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.", 
            "title": "Changing the admin password:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-second-admin-user", 
            "text": "% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john  This command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.", 
            "title": "Adding a second admin user:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-user-in-the-developer-role", 
            "text": "% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane  This command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.", 
            "title": "Adding a user in the developer role:"
        }, 
        {
            "location": "/dtgateway_security/#listing-all-users", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users", 
            "title": "Listing all users:"
        }, 
        {
            "location": "/dtgateway_security/#getting-info-for-a-specific-user", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john  This command returns the information about the user \u201cjohn\u201d.", 
            "title": "Getting info for a specific user:"
        }, 
        {
            "location": "/dtgateway_security/#removing-a-user", 
            "text": "% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane  This command removes the user \u201cjane\u201d.", 
            "title": "Removing a user:"
        }, 
        {
            "location": "/dtgateway_security/#setting-up-ssl-keystore-in-gateway", 
            "text": "An SSL keystore is needed for your gateway to enable HTTPS in the gateway and to configure SMTP with password authentication. SMTP configuration is needed for the gateway to \nsend email alerts as described in  System Alerts .   Follow the steps below to set up the keystore.    Generate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here:  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html .\nNote down the keystore password and full path of the keystore that you need to provide in the next step.    Add two properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  (or  ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n     !-- this is the full path to the SSL keystore you created -- \n       name dt.gateway.sslKeystorePath /name \n       value {/path/to/keystore} /value \n   /property \n   property \n     !-- this is the keystore password -- \n       name dt.gateway.sslKeystorePassword /name \n       value {keystore-password} /value \n   /property \n... /configuration     Perform any additional steps required for \"Enabling HTTPS\" or \"SMTP Password Encryption\" use-case as described below.    Restart the Gateway by running  sudo service dtgateway restart  (when running Gateway in local mode use  dtgateway restart  command)", 
            "title": "Setting up SSL Keystore in Gateway"
        }, 
        {
            "location": "/dtgateway_security/#enabling-https-in-gateway", 
            "text": "To enable HTTPS in the Gateway after setting up the keystore as described above, you have to add the following\nproperty to the  dt-site.xml  configuration file       configuration \n    ...\n       property \n         name dt.attr.GATEWAY_USE_SSL /name \n         value true /value \n       /property \n    ...\n     /configuration", 
            "title": "Enabling HTTPS in Gateway"
        }, 
        {
            "location": "/dtgateway_security/#setting-up-a-key-for-smtp-password-encryption", 
            "text": "Add another key to the keystore created above using the instructions mentioned in  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html . For example:  keytool -genkey -alias smtpenc-alias -keyalg RSA -keypass  key password  -storepass  store password  -keystore gwkeystore.jks  Note down the alias you used for the key (smtpenc-alias in the above command). Remember to use the same key password and \nstore password as the ones you used in the previous command where you created the keystore.  Add the following property to  dt-site.xml  to indicate the alias to be used for SMTP Password Encryption:       configuration \n    ...\n      property \n        name dt.gateway.ssl.alias.password.encryption /name \n        value smtpenc-alias /value \n      /property \n    ...\n     /configuration", 
            "title": "Setting up a Key for SMTP Password Encryption"
        }, 
        {
            "location": "/dtgateway_systemalerts/", 
            "text": "DT Gateway System Alerts\n\n\nThe DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.\n\n\nAlerts and Topics\n\n\nAs described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.\n\n\nHere is an example for simple JavaScript condition; we can make a REST call to create an\nalert named \nxyz\n which emails to \nsomeone@company.com\n when the number of running\napplications is greater than 5 for at least 60 seconds; the JSON object is the payload:\n\n\nPUT /ws/v2/systemAlerts/alerts/xyz\n\n\n{\n  \ncondition\n:\n_topic['cluster.metrics'].numAppsRunning \n 5\n,\n  \nemail\n:\nsomeone@company.com\n,\n  \ndescription\n: \nNo.of apps running is more than 5\n,\n  \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nWhen the number of running applications is greater than 5 for more than 60 seconds, a simple\nemail will be sent to \nsomeone@company.com\n, stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect.\n\n\nThe condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform \n_topic[\ntopic_name\n]\n.\nThe topic names can be looked up using the \nGET /ws/v2/systemAlerts/topicData\n REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as \nGet\n, \nPut\n, and \nDelete\n):\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\n{\n  \napplications.\napplicationId\n.logicalOperators\n: {...},\n  \napplications.\napplicationId\n.physicalOperators\n: {...},\n  \napplications.\napplicationId\n.containers\n: {...},\n  \napplications.\napplicationId\n: {...}\n  \ncluster.metrics\n: {...},\n  \napplications\n: {...}\n}\n\n\n\n\nwhere \napplicationId\n refers to the application id of a running application\n(which typically has the form \napplication_1482319446115_4314\n). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.\n\n\nConfiguring SMTP for Email\n\n\nSMTP needs to be configured for the gateway to be able to send alert emails via an \n\nSMTP server\n. Make\na note of the following steps before attempting to configure SMTP in the gateway.\n\n\n\n\nmake sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).\n\n\nif the SMTP server supports or requires \nTLS or SSL encryption\n,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish \ntrust\n.\n\n\ndetermine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).\n\n\nif you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.\n\n\ndetermine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.\n\n\ndetermine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.\n\n\nin case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described\n\nhere\n. The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.\n\n\n\n\nYou can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request. \n\n\nPUT /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\npassword\n: \nsecret_password\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nContents of the JSON Object:\n\n\n\n\n\n\n\n\nJSON Key\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nthe SMTP hostname\n\n\n\n\n\n\nport\n\n\nthe SMTP port on the SMTP server\n\n\n\n\n\n\nfromAddr\n\n\nthe \"from-address\" described above i.e. the address from which the alert email is sent\n\n\n\n\n\n\nfromName\n\n\nthe \"from-name\" described above i.e. the user friendly string for the \"from-address\"\n\n\n\n\n\n\nencryptionType\n\n\nthe \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"\n\n\n\n\n\n\nauthType\n\n\nthe \"authentication-type\" value described above i.e. \"password\" or \"none\"\n\n\n\n\n\n\nusername\n\n\nthe \"SMTP-username\" value described above\n\n\n\n\n\n\npassword\n\n\nthe \"SMTP-password\" value described above\n\n\n\n\n\n\ntestAddr\n\n\nthe \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.\n\n\n\n\n\n\n\n\nA note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.\n\n\nGET /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nThe GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.\n\n\nManaging and viewing alerts\n\n\nThe following operations are available in the Gateway REST API:\n\n\n\n\n\n\nCreate an alert\n\n\n\n\n\n\nDelete an alert\n\n\n\n\n\n\nGet alert history\n\n\n\n\n\n\nView content and status of alerts\n\n\n\n\n\n\nView all the current data in the \n_topic\n array\n\n\n\n\n\n\nCreating an alert\n\n\nTo create an alert, the user needs to specify the alert name, condition,\nemail address and duration in milliseconds. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.\n\n\nComplex JavaScript Expressions :\n\n\nThe following example shows how to issue a REST request to create an alert named\n\nWordCountAppNotRunning\n which emails to \nphil@company.com\n and \nmike@company.com\n\nwhen the \nWordCount\n app is not in the \nRUNNING\n state for at least 60 seconds.\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \n_topic['applications.application_1480063135007_0543']['state'] != 'RUNNING'\n,\n  \nemail\n:\nphil@company.com, mike@company.com\n,\n  \ndescription\n: \nWordCount Application is not running\n,\n  \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nThe above alert works for the current invocation of the \nWordCount\n application; however,\nwhen the application is restarted, a new application \nid\n is generated for which this alert\nwill no longer work. To avoid having to update the application \nid\n in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application \nid\n for the given application as shown below:\n\n\nvar alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i \n appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;\n\n\n\n\nThe JavaScript expression must however be written as a single line; tools such as\n\njavascriptcompressor\n are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert;\n,\n   \nemail\n:\nphil@company.com, mike@company.com\n,\n   \ndescription\n: \nWordCount Application is not running\n,\n   \ntimeThresholdMillis\n:\n60000\n\n}\n\n\n\n\nDeleting an alert\n\n\nWe can delete an alert with the DELETE REST API request:\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nAlerts history\n\n\nWe can get the alerts history with the GET REST API request:\n\n\nGET /ws/v2/systemAlerts/history\n\n\nIt returns a result of the following form:\n\n\n{\n\nhistory\n: [{\n    \nname\n: \nxyz\n,\n    \ninTime\n: \n1481235199598\n,\n    \noutTime\n: \n1481235251088\n,\n    \nmessage\n: \nNo.of apps running is more than 5\n\n},{\n    \nname\n: \ncheckCsvParserNotRunning\n,\n    \ninTime\n: \n1481235251087\n,\n    \noutTime\n: \n1481235549648\n,\n    \nmessage\n: \nApplication is not running\n\n}],\n}\n\n\n\n\nThe alert history comprises the alert name, time it became active (\ninTime\n), time it\nbecame inactive (\noutTime\n) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.\n\n\nViewing the content and the status of alerts\n\n\nWe can get the content and status of alerts with the GET REST API web request:\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nIt returns a result of the following form:\n\n\n{\n \nname\n: \ncheckLatencyApp\n,\n \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i\noperators.length);++i){if(operators[i].latencyMA\n50){alert=true;break}}}alert;\n,\n \nemail\n: \nphil@company.com, mike@company.com\n,\n \ndescription\n: \nchecking latency \n 50\n,\n \ntimeThresholdMillis\n: \n10000\n,\n \nalertStatus\n: {\n   \nisInAlert\n: true,\n   \ninTime\n: \n1481264665450\n,\n   \nemailSent\n: true,\n   \nmessage\n: \nchecking latency \n 50\n\n  }\n}\n\n\n\n\nThe result includes the alert name, condition, email addresses, description and duration.\nIt also alert status info such as \nisInAlert\n which indicates whether it is still active\nor not, \ninTime\n which represents the time the alert became active, \nemailSent\n which,\nas the name suggests, indicates if email was sent, and \nmessage\n which is similar to the\ndescription.\n\n\nViewing all the current data in the \n_topic\n array\n\n\nThis section is already covered under \nAlerts and Topics\n .", 
            "title": "System Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#dt-gateway-system-alerts", 
            "text": "The DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.", 
            "title": "DT Gateway System Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-and-topics", 
            "text": "As described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.  Here is an example for simple JavaScript condition; we can make a REST call to create an\nalert named  xyz  which emails to  someone@company.com  when the number of running\napplications is greater than 5 for at least 60 seconds; the JSON object is the payload:", 
            "title": "Alerts and Topics"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertsxyz", 
            "text": "{\n   condition : _topic['cluster.metrics'].numAppsRunning   5 ,\n   email : someone@company.com ,\n   description :  No.of apps running is more than 5 ,\n   timeThresholdMillis : 60000 \n}  When the number of running applications is greater than 5 for more than 60 seconds, a simple\nemail will be sent to  someone@company.com , stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect.  The condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform  _topic[ topic_name ] .\nThe topic names can be looked up using the  GET /ws/v2/systemAlerts/topicData  REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as  Get ,  Put , and  Delete ):", 
            "title": "PUT /ws/v2/systemAlerts/alerts/xyz"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2systemalertstopicdata", 
            "text": "{\n   applications. applicationId .logicalOperators : {...},\n   applications. applicationId .physicalOperators : {...},\n   applications. applicationId .containers : {...},\n   applications. applicationId : {...}\n   cluster.metrics : {...},\n   applications : {...}\n}  where  applicationId  refers to the application id of a running application\n(which typically has the form  application_1482319446115_4314 ). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_systemalerts/#configuring-smtp-for-email", 
            "text": "SMTP needs to be configured for the gateway to be able to send alert emails via an  SMTP server . Make\na note of the following steps before attempting to configure SMTP in the gateway.   make sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).  if the SMTP server supports or requires  TLS or SSL encryption ,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish  trust .  determine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).  if you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.  determine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.  determine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.  in case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described here . The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.   You can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request.", 
            "title": "Configuring SMTP for Email"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , password :  secret_password , testAddr :  testuser@yourdomain.com \n}  Contents of the JSON Object:     JSON Key  Value      host  the SMTP hostname    port  the SMTP port on the SMTP server    fromAddr  the \"from-address\" described above i.e. the address from which the alert email is sent    fromName  the \"from-name\" described above i.e. the user friendly string for the \"from-address\"    encryptionType  the \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"    authType  the \"authentication-type\" value described above i.e. \"password\" or \"none\"    username  the \"SMTP-username\" value described above    password  the \"SMTP-password\" value described above    testAddr  the \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.     A note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.", 
            "title": "PUT /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , testAddr :  testuser@yourdomain.com \n}  The GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.", 
            "title": "GET /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#managing-and-viewing-alerts", 
            "text": "The following operations are available in the Gateway REST API:    Create an alert    Delete an alert    Get alert history    View content and status of alerts    View all the current data in the  _topic  array", 
            "title": "Managing and viewing alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#creating-an-alert", 
            "text": "To create an alert, the user needs to specify the alert name, condition,\nemail address and duration in milliseconds. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.  Complex JavaScript Expressions :  The following example shows how to issue a REST request to create an alert named WordCountAppNotRunning  which emails to  phil@company.com  and  mike@company.com \nwhen the  WordCount  app is not in the  RUNNING  state for at least 60 seconds.", 
            "title": "Creating an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning", 
            "text": "{\n   condition :  _topic['applications.application_1480063135007_0543']['state'] != 'RUNNING' ,\n   email : phil@company.com, mike@company.com ,\n   description :  WordCount Application is not running ,\n   timeThresholdMillis : 60000 \n}  The above alert works for the current invocation of the  WordCount  application; however,\nwhen the application is restarted, a new application  id  is generated for which this alert\nwill no longer work. To avoid having to update the application  id  in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application  id  for the given application as shown below:  var alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i   appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;  The JavaScript expression must however be written as a single line; tools such as javascriptcompressor  are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning_1", 
            "text": "{\n   condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert; ,\n    email : phil@company.com, mike@company.com ,\n    description :  WordCount Application is not running ,\n    timeThresholdMillis : 60000 \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#deleting-an-alert", 
            "text": "We can delete an alert with the DELETE REST API request:  DELETE /ws/v2/systemAlerts/alerts/{name}", 
            "title": "Deleting an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-history", 
            "text": "We can get the alerts history with the GET REST API request:  GET /ws/v2/systemAlerts/history  It returns a result of the following form:  { history : [{\n     name :  xyz ,\n     inTime :  1481235199598 ,\n     outTime :  1481235251088 ,\n     message :  No.of apps running is more than 5 \n},{\n     name :  checkCsvParserNotRunning ,\n     inTime :  1481235251087 ,\n     outTime :  1481235549648 ,\n     message :  Application is not running \n}],\n}  The alert history comprises the alert name, time it became active ( inTime ), time it\nbecame inactive ( outTime ) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.", 
            "title": "Alerts history"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-the-content-and-the-status-of-alerts", 
            "text": "We can get the content and status of alerts with the GET REST API web request:  GET /ws/v2/systemAlerts/alerts/{name}  It returns a result of the following form:  {\n  name :  checkLatencyApp ,\n  condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i operators.length);++i){if(operators[i].latencyMA 50){alert=true;break}}}alert; ,\n  email :  phil@company.com, mike@company.com ,\n  description :  checking latency   50 ,\n  timeThresholdMillis :  10000 ,\n  alertStatus : {\n    isInAlert : true,\n    inTime :  1481264665450 ,\n    emailSent : true,\n    message :  checking latency   50 \n  }\n}  The result includes the alert name, condition, email addresses, description and duration.\nIt also alert status info such as  isInAlert  which indicates whether it is still active\nor not,  inTime  which represents the time the alert became active,  emailSent  which,\nas the name suggests, indicates if email was sent, and  message  which is similar to the\ndescription.", 
            "title": "Viewing the content and the status of alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-all-the-current-data-in-the-_topic-array", 
            "text": "This section is already covered under  Alerts and Topics  .", 
            "title": "Viewing all the current data in the _topic array"
        }, 
        {
            "location": "/apexcli/", 
            "text": "Apache Apex Command Line Interface\n\n\nApex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications.\n\nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of Apex CLI is to provide scope, by connecting and executing commands in a context\nof specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.\n\n\n\n\nLaunch or kill applications\n\n\nView system metrics including load, throughput, latency, etc.\n\n\nStart or stop tuple recording\n\n\nRead operator, stream, port properties and attributes\n\n\nWrite to operator properties\n\n\nDynamically change the application logical plan\n\n\nCreate custom macros\n\n\n\n\nApex CLI Commands\n\n\nApex CLI can be launched by running following command:\n\n\napex\n\n\n\nHelp on all commands is available via \u201chelp\u201d command in the CLI\n\n\nGlobal Commands\n\n\nGLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf \napp package configuration file\n        Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives \ncomma separated list of archives\n    Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf \nconfiguration file\n                      Specify an\n                                                            application\n                                                            configuration file.\n            -D \nproperty=value\n                             Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files \ncomma separated list of files\n          Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars \ncomma separated list of libjars\n      Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId \napplication id\n                 Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue \nqueue name\n                             Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file\n\n\n\n\nCommands after connecting to an application\n\n\nCOMMANDS WHEN CONNECTED TO AN APP (via connect \nappid\n) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName \nproperty name\n    The name of the property whose\n                                             value needs to be retrieved\n            -waitTime \nwait time\n            How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\nget-container-stacktrace container-id\n    Shows the stack trace of all the threads in the container\n\n\n\n\nCommands when changing the logical plan\n\n\nCOMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change\n\n\n\n\nExamples\n\n\nAn example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.\n\n\napex\n begin-macro add-console-output\nmacro\n begin-logical-plan-change\nmacro\n create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro\n create-stream stream_$1 $2 $3 $1 in\nmacro\n submit\n\n\n\n\nThen execute the \nadd-console-output\n macro like this\n\n\napex\n add-console-output xyz opername portname\n\n\n\n\nThis macro then expands to run the following command\n\n\nbegin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit\n\n\n\n\nNote\n:  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "Apex CLI"
        }, 
        {
            "location": "/apexcli/#apache-apex-command-line-interface", 
            "text": "Apex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications. \nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of Apex CLI is to provide scope, by connecting and executing commands in a context\nof specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.   Launch or kill applications  View system metrics including load, throughput, latency, etc.  Start or stop tuple recording  Read operator, stream, port properties and attributes  Write to operator properties  Dynamically change the application logical plan  Create custom macros", 
            "title": "Apache Apex Command Line Interface"
        }, 
        {
            "location": "/apexcli/#apex-cli-commands", 
            "text": "Apex CLI can be launched by running following command:  apex  Help on all commands is available via \u201chelp\u201d command in the CLI", 
            "title": "Apex CLI Commands"
        }, 
        {
            "location": "/apexcli/#global-commands", 
            "text": "GLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf  app package configuration file         Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives  comma separated list of archives     Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf  configuration file                       Specify an\n                                                            application\n                                                            configuration file.\n            -D  property=value                              Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files  comma separated list of files           Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars  comma separated list of libjars       Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId  application id                  Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue  queue name                              Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file", 
            "title": "Global Commands"
        }, 
        {
            "location": "/apexcli/#commands-after-connecting-to-an-application", 
            "text": "COMMANDS WHEN CONNECTED TO AN APP (via connect  appid ) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName  property name     The name of the property whose\n                                             value needs to be retrieved\n            -waitTime  wait time             How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\nget-container-stacktrace container-id\n    Shows the stack trace of all the threads in the container", 
            "title": "Commands after connecting to an application"
        }, 
        {
            "location": "/apexcli/#commands-when-changing-the-logical-plan", 
            "text": "COMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change", 
            "title": "Commands when changing the logical plan"
        }, 
        {
            "location": "/apexcli/#examples", 
            "text": "An example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.  apex  begin-macro add-console-output\nmacro  begin-logical-plan-change\nmacro  create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro  create-stream stream_$1 $2 $3 $1 in\nmacro  submit  Then execute the  add-console-output  macro like this  apex  add-console-output xyz opername portname  This macro then expands to run the following command  begin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit  Note :  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "Examples"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "===============================\n\n\nDownload\n\n\nWhere can I get DataTorrent RTS software?\n\n\nDataTorrent products are available for download from \nhttps://www.datatorrent.com/download/\n\n\n\n\nCommunity Edition\n:  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.\n\n\nEnterprise Edition\n:  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.\n\n\nSandbox Edition\n:  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.\n\n\n\n\nWhat is the difference between DataTorrent RTS editions?\n\n\nPlease refer to \nDataTorrent RTS editions overview\n\n\nWhat are DataTorrent RTS package contents of Community vs Enterprise edition?\n\n\nPackage contents for Community edition:\n\n\n\n\nApache Apex\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent dtManage\n\n\n\n\nPackage contents for Enterprise edition:\n\n\n\n\nApache Apex\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\n\n\nHow do I confirm the package downloaded correctly?\n\n\nYou can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:\n\n\n# md5sum \nDT_RTS_Package\n\n\n\n\nCompare the result with MD5 sum posted on the product download page.\n\n\nHow do I download the DataTorrent RTS package using CLI?\n\n\nUse following curl command to download DataTorrent RTS package:\n\n\ncurl -LSO\u00a0\nDT_RTS_download_link\n\n\n\n\nWe recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.\n\n\nWhat are the prerequisites of DataTorrent RTS ?\n\n\nDataTorrent RTS platform has following Hadoop cluster requirements:\n\n\n\n\nOperating system supported by Hadoop distribution\n\n\nHadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable\n\n\nJava 7 or 8 as supported by Hadoop distribution\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\nGoogle Chrome or Safari to access dtManage (DataTorrent UI\n    console)\n\n\n\n\nWhere can I start from after downloading DataTorrent RTS?\n\n\n\n\nAfter successful download of DataTorrent RTS, make sure all prerequisites are satisfied.\n\n\nYou will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to \ninstallation guide\n\n\nOnce installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.\n\n\n\n\nWhat are the supported Hadoop distribution by DataTorrent RTS?\n\n\nDataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nHadoop Distribution\n\n\nSupported Version\n\n\n\n\n\n\nAmazon EMR\n\n\nHadoop 2.4 and higher\n\n\n\n\n\n\nApache Hadoop\n\n\nHadoop 2.2 and higher\n\n\n\n\n\n\nCloudera\n\n\nCDH 5.0 and higher\n\n\n\n\n\n\nHortonworks\n\n\nHDP 2.0 and higher\n\n\n\n\n\n\nMapR\n\n\n4.0 and higher\n\n\n\n\n\n\nMicrosoft\n\n\nHDInsight\n\n\n\n\n\n\nPivotal\n\n\n2.1 and higher\n\n\n\n\n\n\n\n\n\nWhat is the Datatorrent Sandbox?\n\n\nThe Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nWhere do I get DataTorrent Sandbox download link?\n\n\nSandbox can be downloaded by visiting \ndatatorrent.com/download\n\n\nWhat are the system requirements for sandbox deployment?\n\n\nThe DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:\n\n\n\n\nVirtualBox\n 4.3 or greater installed.\n\n\n6GB RAM or greater available for Sandbox VM.\n\n\n\n\nWhat are the DataTorrent RTS package content details in sandbox?\n\n\n\n\nUbuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)\n\n\nLubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)\n\n\nApache Apex, Apache Apex-Malhar\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\nDemo Applications\n\n\n\n\nWhy does the browser console on the sandbox say \nHDFS Not Ready\n ?\n\n\nThe HDFS services take a few minutes to start; the console needs all of\nthose services to be up and running and until that occurs, it displays this\nwarning. If the normal console does not appear after a few minutes, please run\nthe \njps\n command to see which services may \nnot\n be running, for example:\n\n\ndtadmin@dtbox:~/tmp$ jps\n1407 DTGateway\n4145 Jps\n2830 NodeManager\n3059 ResourceManager\n2650 NameNode\n\n\n\n\nHere we see that the \nDataNode\n is not running. In this case, stop all\nHDFS services (using, for example the script shown in the\n\nsandbox page\n. Then, remove everything\nunder these directories:\n\n\n/sfw/hadoop/shared/data/hdfs/datanode/current\n/sfw/hadoop/shared/data/hdfs/namenode/current\n\n\n\nNow reformat HDFS with \nhdfs namenode -format\n\nand finally, restart all HDFS services.\n\n\nIf all services are running and the normal console still does not appear,\nplease run following commands:\n\n\nhdfs dfsadmin -safemode leave\nhdfs fsck -delete\n\n\n\n\nIf HDFS detects that some files are corrupted (perhaps due to an earlier improper shutdown)\nit will not exit the initial safemode automatically;\nthe commands above exit safemode manually and delete corrupted files.\n\n\nHow do I get specific DT version ?\n\n\nYou can find archive list of various DataTorrent RTS versions at the bottom of each product download page.\n\n\nWhere can I request new / upgrade current license?\n\n\nPlease follow the instructions at \nLicense Upgrade\n\n\nWhere do I find product documentation?\n\n\nPlease refer to: \nDataTorrent Documentation\n\n\nWhere can I learn more about Apache Apex?\n\n\nYou can refer Apex page for more details: \nApache Apex\n\n\nDo you need help?\n\n\nYou can contact us at \nhttps://www.datatorrent.com/contact\n\n\nInstallation\n\n\nThere are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).\n\n\nMinimum hardware requirements, what happens if certain minimum configuration requirement has not been met?\n\n\nMinimum of 8G RAM is required on the Hadoop cluster.\n\n\nWhat happens if java is not installed?\n\n\nFollowing message can be seen when Java is not available on the system\n\n\nError: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.\n\n\n\nInstall Java 7 from package manager of Linux Distribution and try running installer again.\n\n\nWhat happens if Hadoop is not installed?\n\n\nInstallation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect Hadoop binary (/usr/bin/hadoop) \n DFS location.\n\n\n\n\nInstall Hadoop > 2.2.0 and update the configuration parameters above.\n\n\nHow do I check if Hadoop is installed and running correctly?\n\n\nFollowing commands can be used to confirm installed Hadoop version and if Hadoop services are running.\n\n\n$ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager\n\n\n\nWhat happens if the downloaded file is corrupted?\n\n\nMD5 checksum will result in the following error:\n\n\n\u201cVerifying archive integrity...Error in MD5 checksums: \nMD5 checksum\n is different from \nMD5 checksum\n\u201d.\n\n\n\nDownloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use \ncurl\n instead of \nwget\n.\n\n\nWhy do I see the following permissions errors?\n\n\nDuring installation following error message will be seen on screen\n\n\n\n\nThese typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:\n\n\n$ hadoop dfs -ls /user/\nUSER\n/datatorrent\n$ hadoop dfs -mkdir /user/\nUSER\n/datatorrent  \n$ hadoop dfs -chown \nUSER\n /user/\nUSER\n/datatorrent\n\n\n\nUpgrade\n\n\nLicense agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0.\n\n\nIf your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See \ninstallation guide\n for details.\n\n\nConfiguration\n\n\nConfiguring Memory\n\n\nConfiguring Operator Memory:\n\n\nOperator memory for an operator can be configured in one of the following two ways:\n\n\n1 Using the same default values for all the operators: \n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.attr.MEMORY_MB\n/name\n\n  \nvalue\n2048\n/value\n\n\n/property\n\n\n\n\nThis would set 2GB as size of all the operators in the given application.\n\n\n2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator \nOp\n.\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.attr.MEMORY_MB\n/name\n\n  \nvalue\n8192\n/value\n\n\n/property\n\n\n\n\nThe amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.\n\n\nConfiguring Buffer Server Memory:\n\n\nThere is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:\n\n\n1 Using the same default values for all the output ports of all the operators\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n128\n/value\n\n\n/property\n\n\n\n\nThis sets 128Mb as buffer memory for all the output ports of all the operators.\n\n\n2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port \np\n of an operator \nOp\n:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.port.p.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n1024\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 512 MB\n\n\nCalculating Container memory:\n\n\nFollowing formula is used to calculate the container memory.\n\n\nContainer Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.\n\n\n\nSometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between\n\n\n[yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]\n\n\n\nThese values can be found in yarn configuration\n\n\nConfiguring Application Master Memory:\n\n\nApplication Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.attr.MASTER_MEMORY_MB\n/name\n\n  \nvalue\n4096\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers\n\n\nDevelopment\n\n\nHadoop dependencies conflicts\n\n\nYou have to make sure that the Hadoop jars are not bundled with the application package o/w they may conflict with the versions available in Hadoop classpath. Here are some of the ways to exclude Hadoop dependencies from the application package\n\n\n\n\n\n\nIf your application is directly dependent on the Hadoop jars, make sure that the scope of the dependency is \nprovided\n. For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml\n\n\ndependency\n\n  \ngroupId\norg.apache.hadoop\n/groupId\n\n  \nartifactId\nhadoop-common\n/artifactId\n\n  \nversion\n2.2.0\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\n\n\n\n\n\nIf your application has transitive dependency on Hadoop jars, make sure that Hadoop jars are excluded from the transitive dependency and added back as application dependency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows\n\n\ndependency\n\n  \ngroupId\n/groupId\n\n  \nartifactId\n/artifactId\n\n  \nversion\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\norg.apache.hadoop\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\n\n\n\n\n\n\nSerialization considerations\n\n\nThe platform requires all operators and tuples in an Apex application to be serializable (and deserializable).\nAfter an application is launched, operators are serialized from the starting node and deserialized and \ninstantiated on various cluster nodes. Also checkpointing involves serializing and persisting an operator \nto a store and deserializing from the store in case of recovery. Tuples are serialized and deserialized \nwhen transmitted over a stream.\n\n\nProblems of lack of serializability (and deserializability) in the code can only be reliably uncovered at run-time.\nSo the recommended way to uncover these problems is to run the application in \n\nlocal mode\n before running on a cluster.\nUse the \nApexCLI\n to launch your application with the \n-local\n option\nto run it in local mode. When the platform runs into a situation where a field or object is not serializable or \ndeserializable, the application will fail at that point with a relevant exception logged on the console or the\nlog file as described in the \nKryo exception\n section. Check out \nthat section further for hints about troubleshooting serialization issues.\n\n\nTransient members\n\n\nCertain data members of an operator do not need to be serialized or deserialized during deployment or \ncheckpointing/recovery because they are \ntransient\n\nin nature and do not represent stateful data. Developers should judiciously use the \n\ntransient\n keyword for declaring\nsuch non-stateful members of operators (or members of objects which are indirectly members of operators) \nso that the platform skips serialization of such members and serialization/deserialization errors are \nminimized. Transient members are further described in the context of the operator life-cycle \n\nhere\n. Typical examples of\ntransient data members are database or network connection objects which need to be \ninitialized before they are used in a process, so there is no use of persisting them\nacross process invocations.\n\n\nGetting this message in STRAM logs. Is anything wrong in my code?\n\n\n2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)\n\n\n\nComing soon.\n\n\nDebugging\n\n\nHow to remote debug gateway service?\n\n\nUpdate Hadoop OPTS variable by running,\n\n\nexport HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\"\n\n\n\nHow to setup DEBUG level while running an application?\n\n\nAdd the following property to your properties file:\n\n\nproperty\n\n  \nname\ndt.application.\nAPP-NAME\n.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nMy gateway is throwing the following exception.\n\n\n  ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...\n\n\n\nCheck if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.\n\n\nWhen Apex is running in secure mode, YARN logs get flooded with several thousand messages per second.\n\n\nPlease make sure that the kerberos principal user name has an account with the\nsame user id on the cluster nodes.\n\n\nApplication throwing following Kryo exception.\n\n\n  com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):\n\n\n\nThis means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception\n\n\n\n\nAdd default constructor to the type in question.\n\n\n\n\nUsing \ncustom serializer\n for the type in question. Some existing alternative serializers can be found at \nhttps://github.com/magro/kryo-serializers\n. A custom serializer can be used as follows:\n\n\n2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.\n\n\n@FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType\n\n\n\nKryo will use this CustomSerializer to serialize and deserialize type SomeType. If\nSomeType is a Map or Collection, there are some special annotations @BindMap and\n@BindCollection; please see \nhere\n.\n\n\n2.2 Using the @DefaultSerializer annotation on the class, for example:\n\n\n@DefaultSerializer(JavaSerializer.class)\npublic class SomeClass ...\n\n\n\n2.3 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:\n\n\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec\nT\n extends com.datatorrent.lib.codec.KryoSerializableStreamCodec\nT\n\n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}\n\n\n\nLet's say there is an Operator \nCustomOperator\n with an input port \ninput\n that expects type SomeType. Following is how to use above defined custom stream codec\n\n\nCustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec\nSomeType\n codec = new CustomSerializableStreamCodec\nSomeType\n();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);\n\n\n\nThis works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways. \n\n\n\n\n\n\nLog analysis\n\n\nThere are multiple ways to adjust logging levels.  For details see \nconfiguration guide\n.\n\n\nHow to check STRAM logs\n\n\nYou can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.  \n\n\nIn dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.  \n\n\nAlternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.\n\n\nyarn logs -applicationId \napplicationId\n\n\n\n\nHow to check application logs\n\n\nOn dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.\n\n\n\n\nHow to check killed operator\u2019s state\n\n\nOn dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.\n\n\n\n\nHow to search for particular any application or container?\n\n\nIn applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.\n\n\nHow do I search within logs?\n\n\nOnce you navigate to the logs page,  \n\n\n\n\nDownload log file to search using your preferred editor  \n\n\nuse \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d\n\n\n\n\nLaunching Applications\n\n\nApplication goes from accepted state to Finished(FAILED) state\n\n\nCheck if your application name conflicts with any of the already running\napplications in your cluster. Apex does not allow two applications with\nthe same name to run simultaneously.\nYour STRAM logs will have following error:\n\n\u201cForced shutdown due to Application master failed due to application\n\\\nappId> with duplicate application name \\\nappName> by the same user\n\\\nuser name> is already started.\u201d  \n\n\nConstraintViolationException during application launch\n\n\nCheck if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException. \u00a0  \n\n\nEvents\n\n\nHow to check container failures\n\n\nIn StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.\n\n\nHow to search within events\n\n\nYou can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.\n\n\ntail vs range mode\n\n\ntail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.\n\n\nWhat is \u201cfollowing\u201d button in events pane\n\n\nWhen we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.\n\n\nHow do I get a heap dump when a container gets an OutOfMemoryError ?\n\n\nThe JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely \n-XX:+HeapDumpOnOutOfMemoryError\n and \n-XX:HeapDumpPath=/tmp/op.heapdump\n.\nTo add them to a specific operator, use this stanza in your configuration file\nwith \nOPERATOR_NAME\n replaced by the actual name of an operator:\n\n\n    \nproperty\n\n      \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nTo add them to all your containers, add this stanza to your configuration file:\n\n\n    \nproperty\n\n      \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nWith these options, when an \nOutOfMemoryError\n occurs, the JVM writes the heap dump to the\nfile \n/tmp/op.heapdump\n; you'll then need to retrieve the file from the node on which the\noperator was running.\n\n\nYou can use the tool \njmap\n (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe \n-F\n option; here is a sample invocation on the sandbox:\n\n\ndtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created\n\n\n\nThe heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as\n\njhat\n or\n\nMAT\n to analyze it.\n\n\nThe former (\njhat\n) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:\n\n\ntmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.\n\n\n\nIt is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available\n\nhere\n.\n\n\nComing Soon\n\n\n\n\nConnection Refused Exception\n\n\nClassNotFound Exception\n\n\nLaunching apa vs jar\n\n\nDAG validation failed\n\n\nMultiple gateways running simultaneously, app not launched.\n\n\nHDFS in safe mode\n\n\nApplication stays in accepted state\n\n\nSome containers do not get resources (specially in case of repartition)\n\n\nInsufficient memory set to operator causes operator kill continuously.\n\n\n\n\nWhy is the number of events same/different at input and output port of each operator?\n\n\n\n\n\n\nShutdown vs kill option\n\n\n\n\nWhy shutdown doesn\u2019t work? (if some containers are not running)\n\n\nCan I kill multiple applications at same time?\n\n\nKilling containers vs killing application\n\n\nSTRAM failures (during define partitions)\n\n\nThread local + partition parallel configuration\n\n\nWhat to do when downstream operators are slow than the input  operators.\n\n\nI am seeing high latency, what to do?\n\n\nappConf in ADT (inside apa file) vs conf option in Apex CLI\n\n\nApplication keeps restarting (has happened once due to license agent during upgrade)\n\n\nOperator getting killed after every 60 secs (Timeout issue)\n\n\nHow to change commit frequency\n\n\nDifference between exactly once, at least once and at most once\n\n\nThread local vs container local vs node local\n\n\nCluster nodes not able to access edge node where Gateway is running\n\n\n\n\nDevelopers not sure when to process incoming tuples in end window or when to do it in process function of operator\n\n\n\n\n\n\nHow partitioning works\n\n\n\n\nHow the data is partitioned between different partitions.\n\n\nHow to use stream-codec\n\n\nData on which ports is partitioned? By default default partitioner partitions data on first port.\n\n\nHow to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).\n\n\n\n\n\n\n\n\npom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes Hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc\n\n\n\n\n\n\nAll non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.\n\n\n\n\n\n\nExactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.\n\n\n\n\n\n\nHow to check checkpoint size. (large checkpoint size cause instability in the DAG).\n\n\n\n\nHow to add custom metrics and metric aggregator.\n\n\nExample of how to implement dynamic partitioning.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#download", 
            "text": "", 
            "title": "Download"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-get-datatorrent-rts-software", 
            "text": "DataTorrent products are available for download from  https://www.datatorrent.com/download/   Community Edition :  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.  Enterprise Edition :  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.  Sandbox Edition :  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.", 
            "title": "Where can I get DataTorrent RTS software?"
        }, 
        {
            "location": "/troubleshooting/#what-is-the-difference-between-datatorrent-rts-editions", 
            "text": "Please refer to  DataTorrent RTS editions overview", 
            "title": "What is the difference between DataTorrent RTS editions?"
        }, 
        {
            "location": "/troubleshooting/#what-are-datatorrent-rts-package-contents-of-community-vs-enterprise-edition", 
            "text": "Package contents for Community edition:   Apache Apex  DataTorrent Demo Applications  DataTorrent dtManage   Package contents for Enterprise edition:   Apache Apex  DataTorrent Demo Applications  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtAssemble  DataTorrent dtDashboard", 
            "title": "What are DataTorrent RTS package contents of Community vs Enterprise edition?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-confirm-the-package-downloaded-correctly", 
            "text": "You can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:  # md5sum  DT_RTS_Package   Compare the result with MD5 sum posted on the product download page.", 
            "title": "How do I confirm the package downloaded correctly?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-download-the-datatorrent-rts-package-using-cli", 
            "text": "Use following curl command to download DataTorrent RTS package:  curl -LSO\u00a0 DT_RTS_download_link   We recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.", 
            "title": "How do I download the DataTorrent RTS package using CLI?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-prerequisites-of-datatorrent-rts", 
            "text": "DataTorrent RTS platform has following Hadoop cluster requirements:   Operating system supported by Hadoop distribution  Hadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable  Java 7 or 8 as supported by Hadoop distribution  Minimum of 8G RAM available on the Hadoop cluster  Permissions to create HDFS directory for DataTorrent user  Google Chrome or Safari to access dtManage (DataTorrent UI\n    console)", 
            "title": "What are the prerequisites of DataTorrent RTS ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-start-from-after-downloading-datatorrent-rts", 
            "text": "After successful download of DataTorrent RTS, make sure all prerequisites are satisfied.  You will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to  installation guide  Once installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.", 
            "title": "Where can I start from after downloading DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-supported-hadoop-distribution-by-datatorrent-rts", 
            "text": "DataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.       Hadoop Distribution  Supported Version    Amazon EMR  Hadoop 2.4 and higher    Apache Hadoop  Hadoop 2.2 and higher    Cloudera  CDH 5.0 and higher    Hortonworks  HDP 2.0 and higher    MapR  4.0 and higher    Microsoft  HDInsight    Pivotal  2.1 and higher", 
            "title": "What are the supported Hadoop distribution by DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-is-the-datatorrent-sandbox", 
            "text": "The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.", 
            "title": "What is the Datatorrent Sandbox?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-get-datatorrent-sandbox-download-link", 
            "text": "Sandbox can be downloaded by visiting  datatorrent.com/download", 
            "title": "Where do I get DataTorrent Sandbox download link?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-system-requirements-for-sandbox-deployment", 
            "text": "The DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:   VirtualBox  4.3 or greater installed.  6GB RAM or greater available for Sandbox VM.", 
            "title": "What are the system requirements for sandbox deployment?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-datatorrent-rts-package-content-details-in-sandbox", 
            "text": "Ubuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)  Lubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)  Apache Apex, Apache Apex-Malhar  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtAssemble  DataTorrent dtDashboard  Demo Applications", 
            "title": "What are the DataTorrent RTS package content details in sandbox?"
        }, 
        {
            "location": "/troubleshooting/#why-does-the-browser-console-on-the-sandbox-say-hdfs-not-ready", 
            "text": "The HDFS services take a few minutes to start; the console needs all of\nthose services to be up and running and until that occurs, it displays this\nwarning. If the normal console does not appear after a few minutes, please run\nthe  jps  command to see which services may  not  be running, for example:  dtadmin@dtbox:~/tmp$ jps\n1407 DTGateway\n4145 Jps\n2830 NodeManager\n3059 ResourceManager\n2650 NameNode  Here we see that the  DataNode  is not running. In this case, stop all\nHDFS services (using, for example the script shown in the sandbox page . Then, remove everything\nunder these directories:  /sfw/hadoop/shared/data/hdfs/datanode/current\n/sfw/hadoop/shared/data/hdfs/namenode/current  Now reformat HDFS with  hdfs namenode -format \nand finally, restart all HDFS services.  If all services are running and the normal console still does not appear,\nplease run following commands:  hdfs dfsadmin -safemode leave\nhdfs fsck -delete  If HDFS detects that some files are corrupted (perhaps due to an earlier improper shutdown)\nit will not exit the initial safemode automatically;\nthe commands above exit safemode manually and delete corrupted files.", 
            "title": "Why does the browser console on the sandbox say HDFS Not Ready ?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-specific-dt-version", 
            "text": "You can find archive list of various DataTorrent RTS versions at the bottom of each product download page.", 
            "title": "How do I get specific DT version ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-request-new-upgrade-current-license", 
            "text": "Please follow the instructions at  License Upgrade", 
            "title": "Where can I request new / upgrade current license?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-find-product-documentation", 
            "text": "Please refer to:  DataTorrent Documentation", 
            "title": "Where do I find product documentation?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-learn-more-about-apache-apex", 
            "text": "You can refer Apex page for more details:  Apache Apex", 
            "title": "Where can I learn more about Apache Apex?"
        }, 
        {
            "location": "/troubleshooting/#do-you-need-help", 
            "text": "You can contact us at  https://www.datatorrent.com/contact", 
            "title": "Do you need help?"
        }, 
        {
            "location": "/troubleshooting/#installation", 
            "text": "There are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).", 
            "title": "Installation"
        }, 
        {
            "location": "/troubleshooting/#minimum-hardware-requirements-what-happens-if-certain-minimum-configuration-requirement-has-not-been-met", 
            "text": "Minimum of 8G RAM is required on the Hadoop cluster.", 
            "title": "Minimum hardware requirements, what happens if certain minimum configuration requirement has not been met?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-java-is-not-installed", 
            "text": "Following message can be seen when Java is not available on the system  Error: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.  Install Java 7 from package manager of Linux Distribution and try running installer again.", 
            "title": "What happens if java is not installed?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-hadoop-is-not-installed", 
            "text": "Installation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect Hadoop binary (/usr/bin/hadoop)   DFS location.   Install Hadoop > 2.2.0 and update the configuration parameters above.", 
            "title": "What happens if Hadoop is not installed?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-check-if-hadoop-is-installed-and-running-correctly", 
            "text": "Following commands can be used to confirm installed Hadoop version and if Hadoop services are running.  $ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager", 
            "title": "How do I check if Hadoop is installed and running correctly?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-the-downloaded-file-is-corrupted", 
            "text": "MD5 checksum will result in the following error:  \u201cVerifying archive integrity...Error in MD5 checksums:  MD5 checksum  is different from  MD5 checksum \u201d.  Downloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use  curl  instead of  wget .", 
            "title": "What happens if the downloaded file is corrupted?"
        }, 
        {
            "location": "/troubleshooting/#why-do-i-see-the-following-permissions-errors", 
            "text": "During installation following error message will be seen on screen   These typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:  $ hadoop dfs -ls /user/ USER /datatorrent\n$ hadoop dfs -mkdir /user/ USER /datatorrent  \n$ hadoop dfs -chown  USER  /user/ USER /datatorrent", 
            "title": "Why do I see the following permissions errors?"
        }, 
        {
            "location": "/troubleshooting/#upgrade", 
            "text": "", 
            "title": "Upgrade"
        }, 
        {
            "location": "/troubleshooting/#license-agent-errors-cause-problems-during-upgrade-from-datatorrent-rts-20-to-30", 
            "text": "If your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See  installation guide  for details.", 
            "title": "License agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0."
        }, 
        {
            "location": "/troubleshooting/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/troubleshooting/#configuring-memory", 
            "text": "", 
            "title": "Configuring Memory"
        }, 
        {
            "location": "/troubleshooting/#configuring-operator-memory", 
            "text": "Operator memory for an operator can be configured in one of the following two ways:  1 Using the same default values for all the operators:   property \n   name dt.application. APPLICATION_NAME .operator.*.attr.MEMORY_MB /name \n   value 2048 /value  /property   This would set 2GB as size of all the operators in the given application.  2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator  Op .  property \n   name dt.application. APPLICATION_NAME .operator.Op.attr.MEMORY_MB /name \n   value 8192 /value  /property   The amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.", 
            "title": "Configuring Operator Memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-buffer-server-memory", 
            "text": "There is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:  1 Using the same default values for all the output ports of all the operators  property \n   name dt.application. APPLICATION_NAME .operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n   value 128 /value  /property   This sets 128Mb as buffer memory for all the output ports of all the operators.  2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port  p  of an operator  Op :  property \n   name dt.application. APPLICATION_NAME .operator.Op.port.p.attr.BUFFER_MEMORY_MB /name \n   value 1024 /value  /property   Default value for this attribute is 512 MB", 
            "title": "Configuring Buffer Server Memory:"
        }, 
        {
            "location": "/troubleshooting/#calculating-container-memory", 
            "text": "Following formula is used to calculate the container memory.  Container Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.  Sometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between  [yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]  These values can be found in yarn configuration", 
            "title": "Calculating Container memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-application-master-memory", 
            "text": "Application Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:  property \n   name dt.application. APPLICATION_NAME .attr.MASTER_MEMORY_MB /name \n   value 4096 /value  /property   Default value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers", 
            "title": "Configuring Application Master Memory:"
        }, 
        {
            "location": "/troubleshooting/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/troubleshooting/#hadoop-dependencies-conflicts", 
            "text": "You have to make sure that the Hadoop jars are not bundled with the application package o/w they may conflict with the versions available in Hadoop classpath. Here are some of the ways to exclude Hadoop dependencies from the application package    If your application is directly dependent on the Hadoop jars, make sure that the scope of the dependency is  provided . For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml  dependency \n   groupId org.apache.hadoop /groupId \n   artifactId hadoop-common /artifactId \n   version 2.2.0 /version \n   scope provided /scope  /dependency     If your application has transitive dependency on Hadoop jars, make sure that Hadoop jars are excluded from the transitive dependency and added back as application dependency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows  dependency \n   groupId /groupId \n   artifactId /artifactId \n   version /version \n   exclusions \n     exclusion \n       groupId org.apache.hadoop /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency", 
            "title": "Hadoop dependencies conflicts"
        }, 
        {
            "location": "/troubleshooting/#serialization-considerations", 
            "text": "The platform requires all operators and tuples in an Apex application to be serializable (and deserializable).\nAfter an application is launched, operators are serialized from the starting node and deserialized and \ninstantiated on various cluster nodes. Also checkpointing involves serializing and persisting an operator \nto a store and deserializing from the store in case of recovery. Tuples are serialized and deserialized \nwhen transmitted over a stream.  Problems of lack of serializability (and deserializability) in the code can only be reliably uncovered at run-time.\nSo the recommended way to uncover these problems is to run the application in  local mode  before running on a cluster.\nUse the  ApexCLI  to launch your application with the  -local  option\nto run it in local mode. When the platform runs into a situation where a field or object is not serializable or \ndeserializable, the application will fail at that point with a relevant exception logged on the console or the\nlog file as described in the  Kryo exception  section. Check out \nthat section further for hints about troubleshooting serialization issues.", 
            "title": "Serialization considerations"
        }, 
        {
            "location": "/troubleshooting/#transient-members", 
            "text": "Certain data members of an operator do not need to be serialized or deserialized during deployment or \ncheckpointing/recovery because they are  transient \nin nature and do not represent stateful data. Developers should judiciously use the  transient  keyword for declaring\nsuch non-stateful members of operators (or members of objects which are indirectly members of operators) \nso that the platform skips serialization of such members and serialization/deserialization errors are \nminimized. Transient members are further described in the context of the operator life-cycle  here . Typical examples of\ntransient data members are database or network connection objects which need to be \ninitialized before they are used in a process, so there is no use of persisting them\nacross process invocations.", 
            "title": "Transient members"
        }, 
        {
            "location": "/troubleshooting/#getting-this-message-in-stram-logs-is-anything-wrong-in-my-code", 
            "text": "2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)  Coming soon.", 
            "title": "Getting this message in STRAM logs. Is anything wrong in my code?"
        }, 
        {
            "location": "/troubleshooting/#debugging", 
            "text": "", 
            "title": "Debugging"
        }, 
        {
            "location": "/troubleshooting/#how-to-remote-debug-gateway-service", 
            "text": "Update Hadoop OPTS variable by running,  export HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\"", 
            "title": "How to remote debug gateway service?"
        }, 
        {
            "location": "/troubleshooting/#how-to-setup-debug-level-while-running-an-application", 
            "text": "Add the following property to your properties file:  property \n   name dt.application. APP-NAME .attr.DEBUG /name \n   value true /value  /property", 
            "title": "How to setup DEBUG level while running an application?"
        }, 
        {
            "location": "/troubleshooting/#my-gateway-is-throwing-the-following-exception", 
            "text": "ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...  Check if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.", 
            "title": "My gateway is throwing the following exception."
        }, 
        {
            "location": "/troubleshooting/#when-apex-is-running-in-secure-mode-yarn-logs-get-flooded-with-several-thousand-messages-per-second", 
            "text": "Please make sure that the kerberos principal user name has an account with the\nsame user id on the cluster nodes.", 
            "title": "When Apex is running in secure mode, YARN logs get flooded with several thousand messages per second."
        }, 
        {
            "location": "/troubleshooting/#application-throwing-following-kryo-exception", 
            "text": "com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):  This means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception   Add default constructor to the type in question.   Using  custom serializer  for the type in question. Some existing alternative serializers can be found at  https://github.com/magro/kryo-serializers . A custom serializer can be used as follows:  2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.  @FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType  Kryo will use this CustomSerializer to serialize and deserialize type SomeType. If\nSomeType is a Map or Collection, there are some special annotations @BindMap and\n@BindCollection; please see  here .  2.2 Using the @DefaultSerializer annotation on the class, for example:  @DefaultSerializer(JavaSerializer.class)\npublic class SomeClass ...  2.3 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:  import java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec T  extends com.datatorrent.lib.codec.KryoSerializableStreamCodec T \n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}  Let's say there is an Operator  CustomOperator  with an input port  input  that expects type SomeType. Following is how to use above defined custom stream codec  CustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec SomeType  codec = new CustomSerializableStreamCodec SomeType ();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);  This works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways.", 
            "title": "Application throwing following Kryo exception."
        }, 
        {
            "location": "/troubleshooting/#log-analysis", 
            "text": "There are multiple ways to adjust logging levels.  For details see  configuration guide .", 
            "title": "Log analysis"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-stram-logs", 
            "text": "You can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.    In dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.    Alternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.  yarn logs -applicationId  applicationId", 
            "title": "How to check STRAM logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-application-logs", 
            "text": "On dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.", 
            "title": "How to check application logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-killed-operators-state", 
            "text": "On dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.", 
            "title": "How to check killed operator\u2019s state"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-for-particular-any-application-or-container", 
            "text": "In applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.", 
            "title": "How to search for particular any application or container?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-search-within-logs", 
            "text": "Once you navigate to the logs page,     Download log file to search using your preferred editor    use \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d", 
            "title": "How do I search within logs?"
        }, 
        {
            "location": "/troubleshooting/#launching-applications", 
            "text": "", 
            "title": "Launching Applications"
        }, 
        {
            "location": "/troubleshooting/#application-goes-from-accepted-state-to-finishedfailed-state", 
            "text": "Check if your application name conflicts with any of the already running\napplications in your cluster. Apex does not allow two applications with\nthe same name to run simultaneously.\nYour STRAM logs will have following error: \n\u201cForced shutdown due to Application master failed due to application\n\\ appId> with duplicate application name \\ appName> by the same user\n\\ user name> is already started.\u201d", 
            "title": "Application goes from accepted state to Finished(FAILED) state"
        }, 
        {
            "location": "/troubleshooting/#constraintviolationexception-during-application-launch", 
            "text": "Check if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException.", 
            "title": "ConstraintViolationException during application launch"
        }, 
        {
            "location": "/troubleshooting/#events", 
            "text": "", 
            "title": "Events"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-container-failures", 
            "text": "In StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.", 
            "title": "How to check container failures"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-within-events", 
            "text": "You can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.", 
            "title": "How to search within events"
        }, 
        {
            "location": "/troubleshooting/#tail-vs-range-mode", 
            "text": "tail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.", 
            "title": "tail vs range mode"
        }, 
        {
            "location": "/troubleshooting/#what-is-following-button-in-events-pane", 
            "text": "When we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.", 
            "title": "What is \u201cfollowing\u201d button in events pane"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-a-heap-dump-when-a-container-gets-an-outofmemoryerror", 
            "text": "The JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely  -XX:+HeapDumpOnOutOfMemoryError  and  -XX:HeapDumpPath=/tmp/op.heapdump .\nTo add them to a specific operator, use this stanza in your configuration file\nwith  OPERATOR_NAME  replaced by the actual name of an operator:       property \n       name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   To add them to all your containers, add this stanza to your configuration file:       property \n       name dt.attr.CONTAINER_JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   With these options, when an  OutOfMemoryError  occurs, the JVM writes the heap dump to the\nfile  /tmp/op.heapdump ; you'll then need to retrieve the file from the node on which the\noperator was running.  You can use the tool  jmap  (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe  -F  option; here is a sample invocation on the sandbox:  dtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created  The heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as jhat  or MAT  to analyze it.  The former ( jhat ) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:  tmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.  It is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available here .", 
            "title": "How do I get a heap dump when a container gets an OutOfMemoryError ?"
        }, 
        {
            "location": "/troubleshooting/#coming-soon", 
            "text": "Connection Refused Exception  ClassNotFound Exception  Launching apa vs jar  DAG validation failed  Multiple gateways running simultaneously, app not launched.  HDFS in safe mode  Application stays in accepted state  Some containers do not get resources (specially in case of repartition)  Insufficient memory set to operator causes operator kill continuously.   Why is the number of events same/different at input and output port of each operator?    Shutdown vs kill option   Why shutdown doesn\u2019t work? (if some containers are not running)  Can I kill multiple applications at same time?  Killing containers vs killing application  STRAM failures (during define partitions)  Thread local + partition parallel configuration  What to do when downstream operators are slow than the input  operators.  I am seeing high latency, what to do?  appConf in ADT (inside apa file) vs conf option in Apex CLI  Application keeps restarting (has happened once due to license agent during upgrade)  Operator getting killed after every 60 secs (Timeout issue)  How to change commit frequency  Difference between exactly once, at least once and at most once  Thread local vs container local vs node local  Cluster nodes not able to access edge node where Gateway is running   Developers not sure when to process incoming tuples in end window or when to do it in process function of operator    How partitioning works   How the data is partitioned between different partitions.  How to use stream-codec  Data on which ports is partitioned? By default default partitioner partitions data on first port.  How to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).     pom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes Hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc    All non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.    Exactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.    How to check checkpoint size. (large checkpoint size cause instability in the DAG).   How to add custom metrics and metric aggregator.  Example of how to implement dynamic partitioning.", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/release_notes/", 
            "text": "DataTorrent RTS Release Notes\n\n\nVersion: 3.8.1\n\n\nRelease date: Aug 14, 2017\n\n\nSummary\n\n\nThis minor release primarily addresses issues related to installation of DataTorrent RTS on a Hadoop cluster configured for secure mode.\n\n\nRTS Bug Fixes\n\n\n[SPOI-11694] : Fresh system-wide install fails with \"DFS Installation Location failed to load\" error\n[SPOI-11846] : Gateway fails when launching application in secure mode without authentication enabled\n[SPOI-11848] : Installation wizard fails on the hadoop configuration screen in secure mode\n[SPOI-11849] : The UI calls to retrieve properties in the wizard fail\n[SPOI-12047] : Gateway throws exceptions while getting restarted after security configuration\n\n\nApache Apex Bug Fixes\n\n\n[APEXCORE-737] : Buffer server may stop processing tuples when backpressure is enabled\n[APEXCORE-745] : AppMaster does not shut down because numRequestedContainers becomes negative\n\n\nVersion: 3.8.0\n\n\nRelease date: Apr 18, 2017\n\n\nSummary\n\n\nApplication Templates (AppHub)\n\n\nPre-built data ingestion templates speed time-to-production\n\n\nDeploy DataTorrent RTS application templates on a Hadoop distribution either on-premises or on the cloud. As part of this release, DataTorrent is providing AWS - EMR deployment script option for each available application template. As a result, development is simplified, enabling developers to more quickly and easily unlock value for customers. DataTorrent is focused on the goals of reducing complexity and removing dependency on Hadoop deployment, and this release represents progress in that direction.   \n\n\nApplication Configurations\n\n\nSimplifying customized application launches\n\n\nUsers can start with a single Application Package and create multiple Application Configurations to launch and run the applications on different environments (for instance, in test and development).  And, efficiencies can be realized across business units: one Application Package could have multiple configurations for multiple internal units.  Each Application Configuration introduces a safety feature, which ensures that only one instance of Application Configuration can run at a time.  The status whether Application Configuration is running or not, and controls to launch and stop the application instance are provided in the Application Configuration view.  This set of features improves management, adds safety, and increases transparency when managing and launching applications.\n\n\nDebugging\n\n\nImproving log visualizations to speed up debugging\n\n\nStrAM Event Grouping\n\n\nThe StrAM Events widget helps from development and operations perspectives to visualize notable events from the application launch and throughout its ongoing run.  With this release, StrAM events widgets now offers better readability by organizing these events into related groups.  For example, when multiple downstream operators are re-deployed due to a container failure. All events triggered by the system to restore normal function will be grouped under a single root event which caused the restarts.  With this improved readability, the user can quickly identify failure causes and then drill down into the logs for each event.\n\n\nGarbage Collection Widgets\n\n\nWith this release, developers can more easily visualize garbage collection data trends\u2014as opposed to sifting through logs. Three widgets are available for GC visualizations:\n\n\n\n\nGarbage collection log chart by heap.  Visualizes when memory is allocated and deallocated \n\n\nGarbage collection log table.  List memory allocation and deallocation details\n\n\nGarbage collection log chart by duration.  Visualizes how long it takes to deallocate memory \n\n\n\n\nLog Tailing \n Search\n\n\nUsers can follow the logs as they are generated (tailing) with the RTS UI Console.  In this release, users can now also perform searches even when tailing to focus on specific events and exclude the noise.\n\n\nAlert Templates\n\n\nSimplifying and expanding alert functionality\n\n\nIn 3.7.0, RTS introduced monitoring with alerts that a DevOps engineer could set up based on specific conditions. The 3.8.0 release continues to simplify and expand this functionality by adding:\n\n\n\n\nNine predefined system alert templates, including cluster and application memory usage, application status, and active container count, killed container alerts, etc.\n\n\nOption to disable alerts without deleting them.\n\n\nThe ability to configure custom SMTP settings for sending alert emails, instead of relying on Gateway\u2019s local node sendmail facility.\n\n\n\n\nSecurity\n\n\nSecurity enhancement in 3.8.0 applies to RTS deployment on secure Hadoop with Kerberos enabled. User's own Kerberos credentials can now be used directly by RTS to launch applications.  It is better from a security perspective.\n\n\nThe previous model involved using a single system user with Kerberos credential (Hadoop impersonation)  to launch applications. That requires access to the system Kerberos credential in order to refresh tokens before they expire.  With user\u2019s own Kerberos credential, that is no longer the case. \n\n\nLicensing\n\n\nWith the release of 3.8.0, DataTorrent is updating and simplifying its licensing policy.  What has changed?  \n\n\nStarting with 3.8, the Community Edition is no longer available. We are replacing the Community Edition with Free Edition.  Community Edition limited the features available to you, such as security.   Now with Free Edition you have access to all the features and tools of RTS up to a 128GB processing limit.\n\n\nPlease refer to the DataTorrent website for additional details.\n\n\nLicensing FAQ\n\n\nI have a Community Edition license. Is that edition still available?\n\nStarting with 3.8, the Community Edition is no longer available.   You can continue to use the Community Edition with RTS version 3.7.\n\n\nHow is license memory consumption calculated?\n\nLicense memory consumption is the sum of all running applications as can be seen in Configuration - License Information and Monitor.\n\n\nWhat will happen when my memory consumption exceed my license limit?\n\nYou\u2019ll receive a warning, which is shown for 30 minutes before most RTS features will be disabled. All existing applications will continue to run. Should you need to upgrade, you can easily contact DataTorrent for a new license.\n\n\nHow will I know when my license is going to expire and what happens if the license expires?\n\nWe provide warnings at 30 days and 7 days before expiration date.  When a license expires, most RTS features will be immediately disabled. All existing applications will continue to run. You can easily contact DataTorrent for a new license.  \n\n\nWhat can I do once RTS is locked (either because my license memory has been exceeded or its expiration date has passed)?\n\nUsers can view running applications and enter new license details to unlock RTS. The following capabilities are still accessible:\n\n Configure - System Configuration is available except for App Data Tracker\n\n Configure - License Information is available\n\n Configure - Installation Wizard is available\n\n Monitor - Application kill, inspect and shutdown are available\n\n Monitor - Application Overview (shutdown and kill only) per application is available\n\n Monitor - Containers (kill only)\n\n AppHub (download only, no import)\n\n Learn\n\n\nCan I reduce the number of applications and return to compliance?\n\nYes, you can do so if you have exceeded your memory capacity, assuming that your license has not expired\n\n\nCan I reduce the memory usage of an application and bring it back to license compliance?\n\nYes, assuming that your license has not expired.  Please note that you will have to restart the application.\n\n\nI have an enterprise license for my production cluster. Can I use the Free Edition in a non- production cluster?\n\nYes. It\u2019s worth noting that only community support is available for the Free Edition. Please visit the DataTorrent User group for RTS-related questions: https://groups.google.com/forum/#!aboutgroup/dt-users\n\n\nFor the Apache Apex mailing list and meetups information, please go to\nhttps://apex.apache.org/community.html#mailing-lists\n\n\nCan I buy DataTorrent support for Free Edition?\n\nUnfortunately, no. DataTorrent support is sold as part of our Enterprise Edition. If you\u2019re seeking support, you may consider upgrading.\n\n\nIs Application Master Container memory consumption included in the calculation for processing capacity?\n\nYes. Application Master Container consumes 1 GB by default and every application has its own Application Master. If application is not running, it does not run as well. It grows based on customer application build. Memory requirements increases along with the size of logical and physical DAG. Partitioners of an operator run in AppMaster.\n\n\nAdditional Features of 3.8.0\n\n\nMultiple gateway support\n\n\nThis allows simultaneous multiple gateway operations and increase fault tolerance due to management console failure. \n\n\nWhen there are multiple gateways (usually for High Availability), different developers may access them at the same time, with different or same user accounts. These activities will often result in simultaneous modification of the same resource stored in HDFS, and invalidate cache entries on each client. For example: When developer A tries to save a configuration package and developer B has edited and saved the same package, developer A will get an error. Developer A would then have to manually merge the differences. This release introduces a new file-based locking mechanism with HTTP ETag header to handle that scenario.\nKnown limitation: Alerts and visualization works correctly with single gateway only.\n\n\nRetain metric selections when returning to Monitor - Physical/Logical view\n\n\nBetter user experience since RTS will keep what users have selected to view (e.g. metrics) as they go from one screen to another. \n\n\nDataTorrent Apex Core fork\n\n\nRTS 3.8.0 is bundled with Apache Apex Core 3.5.0 plus forty feature and fixes that will be part of Apache Apex Core 3.6.0. Apex Core commits from Apr 3, 2017 will be included in RTS 3.9.0\n\n\nNew Feature\n\n\n\n\n[APEXCORE-579]        Custom control tuple support\n\n\n[APEXCORE-563]        Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.\n\n\n\n\nImprovements\n\n\n\n\n[APEXCORE-676]        Show description for DefaultProperties only when user requests it\n\n\n[APEXCORE-655]        Support RELEASE as archetype version when creating a project\n\n\n[APEXCORE-611]        StrAM Event Log Levels\n\n\n[APEXCORE-605]        Suppress bootstrap compiler warning\n\n\n[APEXCORE-592]        Returning description field in defaultProperties during apex cli call get-app-package-info\n\n\n[APEXCORE-572]        Remove dependency on hadoop-common test.jar\n\n\n[APEXCORE-570]        Prevent upstream operators from getting too far ahead when downstream operators are slow\n\n\n[APEXCORE-522]        Promote singleton usage pattern for String2String, Long2String and other StringCodecs\n\n\n[APEXCORE-426]        Support work preserving AM recovery\n\n\n[APEXCORE-294]        Graceful application shutdown\n\n\n[APEXCORE-143]        Graceful shutdown of test applications\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-662]        Raise StramEvent for heartbeat miss\n\n\n\n\nDependency Upgrade\n\n\n\n\n[APEXCORE-656]        Upgrade org.apache.httpcomponents.httpclient\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-674]        DTConfiguration utility class ValueEntry access level was changed\n\n\n[APEXCORE-663]        Application restart not working.\n\n\n[APEXCORE-648]        Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()\n\n\n[APEXCORE-645]        StramLocalCluster does not wait for master thread termination\n\n\n[APEXCORE-644]        get-app-package-operators with parent option does not work\n\n\n[APEXCORE-636]        Ability to refresh tokens using user's own Kerberos credentials in a managed environment where the application is launched using an admin with impersonation\n\n\n[APEXCORE-634]        Apex Platform unable to set unifier attributes for modules in DAG\n\n\n[APEXCORE-627]        Unit test AtMostOnceTest intermittently fails\n\n\n[APEXCORE-624]        Shutdown does not work because of incorrect logic in the AppMaster\n\n\n[APEXCORE-617]        InputNodeTest intermittently fails with ConcurrentModificationException\n\n\n[APEXCORE-616]        Application fails to start Kerberised cluster\n\n\n[APEXCORE-610]        Avoid multiple getBytes() calls in Tuple.writeString\n\n\n[APEXCORE-608]        Streaming Containers use stale RPC proxy after connection is closed\n\n\n[APEXCORE-598]        Embedded mode execution does not use APPLICATION_PATH for checkpointing\n\n\n[APEXCORE-597]        BufferServer needs to shut down all created execution services\n\n\n[APEXCORE-596]        Committed method on operators not called when stream locality is THREAD_LOCAL\n\n\n[APEXCORE-595]        Master incorrectly updates committedWindowId when all partitions are terminated.\n\n\n[APEXCORE-593]        apex cli get-app-package-info could not retrieve properties defined in properties.xml\n\n\n[APEXCORE-591]        SubscribeRequestTuple has wrong buffer size when mask is zero\n\n\n[APEXCORE-585]        Latency should be calculated only after the first window has been complete\n\n\n[APEXCORE-583]        Buffer Server LogicalNode should not be reused by Subscribers\n\n\n[APEXCORE-558]        Do not use yellow color to display command strings in help output\n\n\n[APEXCORE-504]        Possible race condition in StreamingContainerAgent.getStreamCodec()\n\n\n[APEXCORE-471]        Requests for container allocation are not re-submitted\n\n\n\n\nDataTorrent RTS Bug Fixes\n\n\n\n\n[SPOI-10021]  DTX Logical Operator page - BufferServerReadBytesPSMA and BufferServerWriteBytesPSMA to be removed\n\n\n[SPOI-10107]  Application service returns DAG which is null\n\n\n[SPOI-10118]  Upon launch of application, application details do not show up.\n\n\n[SPOI-10153]  Add System Properties \"change\" button should be warm in color\n\n\n[SPOI-10208]  Container state for failed attempt of app is shown as RUNNING\n\n\n[SPOI-10266]  \"host\" information is not available in appattempts API call\n\n\n[SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens\n\n\n[SPOI-10331]  Delete user modal gives error when clicked outside the frame\n\n\n[SPOI-10332]  Cancelling delete user action throws TypeError in developer console\n\n\n[SPOI-10335]  Jersey throwing exceptions \n excessive logging when WADL is enabled\n\n\n[SPOI-10355]  Update Buttons and Text\n\n\n[SPOI-10357]  Redirect User to Login Page\n\n\n[SPOI-10363]  CheckPermission should not throw exception when auth is not enabled.\n\n\n[SPOI-10416]  dtAssemble does not show connection between operators correctly\n\n\n[SPOI-10421]  Fix oauth login\n\n\n[SPOI-10438]  Hide Top Nav Menu Dropdown When Item is Clicked\n\n\n[SPOI-10463]  Enhance Date/Time Picker for StrAM Events Date Range\n\n\n[SPOI-10587]  Implement Date/Time Picker for Dashboard Widget\n\n\n[SPOI-10588]  Change Button Label to Close During Launching\n\n\n[SPOI-10862]  Multiple containers are labelled as AppMaster after dynamic partitioning\n\n\n[SPOI-10866]  Time Range Selection Saved Settings Not Loaded Correctly\n\n\n[SPOI-10894]  dtAssemble - Inspector contents not showing up consistently\n\n\n[SPOI-10911]  determine AppMaster container by id that contains _000001 instead of by including 0 operators\n\n\n[SPOI-10963]  appInstance page - fail to show \"packagedDashboard\" which is included in appPackage that appInstance is launched from\n\n\n[SPOI-10970]  AppHub on gateway does not load in HTTPS\n\n\n[SPOI-10996]  Subscribers/DataListeners may not be scheduled to execute even when they have data to process\n\n\n[SPOI-10997]  BufferServer needs to shut down all created execution services\n\n\n[SPOI-11000]  Upgrade org.apache.httpcomponents.httpclient\n\n\n[SPOI-11024]  Alerts Icon Issue\n\n\n[SPOI-11057]  Restart of the apps are failing\n\n\n[SPOI-11108]  DAG View Javascript Error\n\n\n[SPOI-11127]  Enhance \"lastNbytes\" to behave like \"tail\" command\n\n\n[SPOI-11142]  Unable to launch app if another app with default APPLICATION_NAME is already running\n\n\n[SPOI-11152]  Avoid usage of Apache Apex engine core class com.datatorrent.stram.client.DTConfiguration.ValueEntry\n\n\n[SPOI-11163]  Cannot launch application from application details page\n\n\n[SPOI-11164]  \"Add default properties\" option under \"Specify launch properties\" is missing\n\n\n[SPOI-11168]  License API Does Not Return Latest State Information\n\n\n[SPOI-11179]  Update Root Cause Failure to Use Newer Object Structure\n\n\n[SPOI-11185]  Invalid license expiration message sent by Gateway\n\n\n[SPOI-11186]  AppHub should be visible if license is invalid\n\n\n[SPOI-11188]  App Packages search does not work for \"format\" column\n\n\n[SPOI-11190]  Toggling \"system apps\" option does not show ended system apps\n\n\n[SPOI-11192]  Search for \"lifetime\" column on Monitor screen does not work\n\n\n[SPOI-11193]  Search for \"memory\" column on Monitor screen does not work\n\n\n[SPOI-11194]  Search on \"state\" column on Monitor page is not alphabetical\n\n\n[SPOI-11197]  Search on locality/source/sinks columns under Streams widget on logical tab does not work\n\n\n[SPOI-11198]  Search for \"allocated mem\" and \"free memory\" under Containers widget does not work\n\n\n[SPOI-11199]  \"download file\" option for empty container log files should be disabled\n\n\n[SPOI-11200]  Search for \"allocated mem\" and \"started time\" under Containers widget  for app attempt does not work\n\n\n[SPOI-11208]  DTgateway install screen messed up\n\n\n[SPOI-11210]  On entering corrupt license file error message should be a proper one\n\n\n[SPOI-11212]  Trailing and non trailing search should have same string\n\n\n[SPOI-11213]  Unable to save SMTP configuration using gmail\n\n\n[SPOI-11214]  Launching application with \u00ef\u00bf\u00bc\"Enable Garbage Collection\" throws 404\n\n\n[SPOI-11215]  ADMIN_NOT_CONFIGURED warning is only shown to Dev user instead of admin\n\n\n[SPOI-11220]  Fresh RTS installation fails because of blank response from \"/ws/v2/config\" api call\n\n\n[SPOI-11222]  StackTrace feature not available from physical tab containers widget\n\n\n[SPOI-11223]  Show \"Password change\" warning for dtadmin only\n\n\n[SPOI-11231]  AppHub fails to load previous package versions\n\n\n[SPOI-11234]  Show all AppHub package versions option is missing in list view\n\n\n[SPOI-11236]  Extend application-level gc.log API to take in new parameter \"descendingOrder\" (false/true)\n\n\n[SPOI-11237]  Angular not resolving certain dtText-wrapped expressions in Console modals\n\n\n[SPOI-11238]  AppHub Check for Updates Does Not Show In All Cases\n\n\n[SPOI-11242]  \"Upload package\" option on Application Packages should not be available with invalid/no license\n\n\n[SPOI-11265]  Invalid message displayed when no license is uploaded\n\n\n[SPOI-11266]  Alert notification is delayed\n\n\n[SPOI-11270]  System Alert history is empty after relogin\n\n\n[SPOI-11271]  Developer user cannot create system alert\n\n\n[SPOI-11281]  .class file generated by tuple schema manager is invalid\n\n\n[SPOI-11291]  Creating clone of JSON application gives 500 Server Error\n\n\n[SPOI-11303]  Creating clone of JSON application gives 500 Server Error (UI)\n\n\n[SPOI-11304]  App package load errors after migrating from 3.7 to 3.8\n\n\n[SPOI-11307]  Search for \"lifetime\" column on Monitor screen does not work\n\n\n[SPOI-11318]  Copy To Clipboard option from Failure Message modal does not work\n\n\n[SPOI-11323]  Upgrade from 3.7.0 evaluation edition to 3.8.0 retains old license\n\n\n[SPOI-11325]  Selecting KILLED applications and then disabling ended apps, shows shutdown/kill options\n\n\n[SPOI-11326]  Clean install of 3.8.0 comes with no license\n\n\n[SPOI-11327]  StramEvents are grouped incorrectly\n\n\n[SPOI-11366]  Copy to clipboard not working when viewing stram event stack trace\n\n\n[SPOI-11367]  Wrong 'uptime' value in Application Overview\n\n\n[SPOI-11368]  Log and info icons should be right aligned in stram events\n\n\n[SPOI-11369]  Socket is unsubscribed by Console when critical path is not checked\n\n\n[SPOI-11371]  \"source package\" column in Application Configurations table is not sortable\n\n\n[SPOI-11372]  Unable to view gc.log on UI\n\n\n[SPOI-11375]  Wrong stream locality is shown in Physical DAG widget\n\n\n[SPOI-11377]  Incorrect GC stats for logical operators if they are connected by CONTAINER_LOCAL stream\n\n\n[SPOI-11379]  Heap reduction percentage is negative for some GC events\n\n\n[SPOI-11382]  UI hangs when trying to change settings for GC Log Chart widgets\n\n\n[SPOI-11383]  Selecting a container in GC Log Chart widgets throws error in Developer console\n\n\n[SPOI-11417]  Memory usage of App Data Tracker is not counted against license\n\n\n[SPOI-11418]  Event grouping: UI should ignore groupId 0 \n null\n\n\n[SPOI-11421]  Unable to use Security Configuration feature with free license\n\n\n[SPOI-11422]  Admin user should not be able to delete its own user account\n\n\n[SPOI-11424]  Show tooltip for version string in application overview widget\n\n\n[SPOI-7887]   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}] should return error instead of success where there is error \"Failed to load\"\n\n\n[SPOI-8248]   Packaged dashboards do not reconnect with new app instances\n\n\n[SPOI-8477]   Upgrade License Opens in dtManage window, it should be in opened up in new window\n\n\n[SPOI-8610]   Disable editing operator properties which are of Object type\n\n\n[SPOI-9375]   Uptime values shown on UI are out of whack immediately after app is launched\n\n\n[SPOI-9474]   Failed to restart DT application in MapR secure cluster\n\n\n[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors\n\n\n[SPOI-9945]   Top navigation menu is wrapping into two lines\n\n\n\n\nVersion: 3.7.1\n\n\nRelease date: Feb 28, 2017\n\n\nSummary\n\n\nThis is primarily for users who install RTS in a Kerberized cluster as application fails to launch in 3.7.0. This release fixes the issue. \n\n\nOther issues that are also fixed:\n\n\n\n\nIn 3.6.0, users have the option to override certain properties from the config file. This ability was missing in 3.7.0.\n\n\nGateway fails to recognize AppDataTracker application and it continuously relaunched on a Kerberized cluster.\n\n\nOn a Kerberized cluster, when the configuration parameters are specified in the \ndt-site.xml\n file in the user's home directory, the installation wizard does not allow the user to continue with the installation.\n\n\n\n\nAppendix\n\n\nBug Fixes\n\n\n\n\n[SPOI-10698] - Allow Custom Properties with Config XML file while launching an application\n\n\n[SPOI-10737] - AppDataTracker application relaunches continuously and fails in a Kerberized cluster.\n\n\n[SPOI-10932] - Installation Wizard does not allow to complete gateway configuration\n\n\n[SPOI-10773] - Application fails to run in fully enabled Kerberized mode (APEXCORE-616 - https://issues.apache.org/jira/browse/APEXCORE-616)\n\n\n\n\nVersion: 3.7.0\n\n\nRelease date: Dec 30, 2016\n\n\nSummary\n\n\nThe new features on this release are functionalities that will ease debugging an application and administering application alerts in production. \n\n\nOperation related features for a Dev Ops role:  \n\n\n\n\nManage and see a history of previous alerts so that users can be aware of potential issues before they become critical. \n\n\nView operator ID(s) and name(s) in the dtManage-Physical-Container list table to quickly identify what operators are in each container. \n\n\nFilter matching tuple recordings by searching across tuple recording data.\n\n\n\n\nDebugging features: When trying to troubleshoot or debug a distributed system, these capabilities allow users to quickly identify problem areas and easily drill down into relevant details (i.e. logs). \n\n\n\n\nNotify users when log files have been removed\n\n\nNew Application Attempts section under Monitor. It is located along other views such as logical and physical\n\n\nNew Application Master logs in Application Overview section\n\n\nNew log button shortcut in Stram Events and Physical Operator section\n\n\nShow dead container logs in Running applications. That is to show history of physical operator containers and logs in the physical operator view. For each previous incarnation, there are start time, end time, link to corresponding logs, root cause with error code, and recovery window id.\n\n\nShow the same details for killed or finished application like running application view.\n\n\nShow container history as default in Physical Operator view\n\n\nNew historic count field in Physical Operator view\n\n\nGet thread dump from a container. It is useful to analyze issues such as \"stuck operator\", and obtain statistics from the running JVM. In production environments users often don't have direct access to the machines, thus making it available through the REST API will help. \n\n\nOption to auto tail container logs. When viewing a container log via the UI, there is an option to periodically poll for more data (i.e. \"tail -f\" effect).\n\n\n\n\ndtAssemble\n\n\n\n\nNew validate button so that user can validate DAG without having to save. \n\n\nRemove auto save function. Save will be initiated by user only.\n\n\nSupport custom JSON input for tuple schema creation. This is particularly useful when user needs to add a large number of fields. \n\n\n\n\ndtDashboard\n\n\n\n\nNew gauge widget\n\n\n\n\nAppHub\n\n\n\n\nContinued to refine application templates in AppHub (introduced in RTS 3.6.0).  \n\n\n\n\nRTS 3.7.0 is based on Apache Apex Core 3.5.0 (released Dec 19, 2016) and Apache Apex Malhar 3.6.0 (released Dec 8, 2016).\n\n\nApache Apex Core 3.5.0\n\n\nThis release upgrades the Apache Hadoop YARN dependency from 2.2 to 2.6. The community determined that current users run on versions equal or higher than 2.6 and Apex can now take advantage of more recent capabilities of YARN. The release contains a number of important bug fixes and operability improvements. \nChange log: https://github.com/apache/apex-core/blob/v3.5.0/CHANGELOG.md\n\n\nApache Apex Malhar 3.6.0\n\n\nThe release adds first iteration of SQL support via Apache Calcite. Features include SELECT, INSERT, INNER JOIN with non-empty equi join condition, WHERE clause, SCALAR functions that are implemented in Calcite, custom scalar functions. Endpoint can be file, Kafka or internal streaming port for both input and output. CSV format is implemented for both input and output. See examples for usage of the new API.\n\n\nThe windowed state management has been improved (WindowedOperator). There is now an option to use spillable data structures for the state storage. This enables the operator to store large states and perform efficient checkpointing.\n\n\nThere was also benchmarking on WindowedOperator with the spillable data structures. From the result, the community significantly improved how objects are serialized and reduced garbage collection considerably in the Managed State layer. Work is still in progress for purging state that is not needed any more and further improving the performance of Managed State that the spillable data structures depend on. More information about the windowing support can be found at http://apex.apache.org/docs/malhar/operators/windowedOperator/.\n\n\nThis release also adds a new, alternative Cassandra output operator (non-transactional, upsert based) and support for fixed length file format to the enrichment operator. \nChange log: https://github.com/apache/apex-malhar/blob/v3.6.0/CHANGELOG.md\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors\n\n\n[SPOI-9923]   Custom panes' on operator monitoring page should have character limits for titles\n\n\n[SPOI-9924]   Can not escape the \"custom\" pane name\n\n\n[SPOI-9925]   Limit number of custom panes\n\n\n[SPOI-9947]   JDBC Poll Input operator processes extra records when its container is killed\n\n\n[SPOI-9948]   JDBC Poll Input operator does not process new records when they are inserted while the app is processing the existing records\n\n\n[SPOI-9965]   Restarting the KILLED application with JDBC Poll Input operator plays the duplicate data\n\n\n[SPOI-10046]  Deleting a property directly creates tuple schema with remaining properties\n\n\n[SPOI-10049]  Limit the number of characters in role name \n\n\n[SPOI-10107]  Application service returns dag which is null\n\n\n[SPOI-10151]  Add System Properties modal should validate the properties\n\n\n[SPOI-10153]  Add System Properties \"change\" button should be warm in color\n\n\n[SPOI-10154]  Rerun Install wizard allows extension of trial\n\n\n[SPOI-10158]  Logical/Physical plan view does not retain metric selections in drop-down\n\n\n[SPOI-10165]  Container logs, dt.log files produced by chklogs.py have HTML escapes\n\n\n[SPOI-10202]  About API call gives out information without authentication.\n\n\n[SPOI-10203]  User can set any non existent package name \n\n\n[SPOI-10208]  Container state for failed attempt of app is shown as RUNNING\n\n\n[SPOI-10267]  If number of alerts are huge, gateway starts slowing down\n\n\n[SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens\n\n\n[SPOI-10275]  Alert contains an exception trace\n\n\n[SPOI-10292]  Delay in cluster metrics when the authentication is enabled\n\n\n[SPOI-10315]  \"requires Apex version\" information is missing for latest app packages on AppHub\n\n\n[SPOI-10332]  Canceling delete user action throws TypeError in developer console\n\n\n[SPOI-10333]  Configuration issue modal content goes out of modal and is not scrollable\n\n\n[SPOI-10334]  App restart doesn't take to new page\n\n\n[SPOI-10373]  FinishedTime/EndTime information is available only after application if killed/shutdown for CDH\n\n\n[SPOI-10379]  Enable Reporting button should have cool colors\n\n\n[SPOI-10385]  The app name field should not be editable at launch time\n\n\n[SPOI-10389]  UI Console should not allow creation of config pkg with special characters\n\n\n[SPOI-10396]  Application configuration should require save before launch\n\n\n[SPOI-10398]  UI says \"An error occurred while fetching data\" immediately after launching apps (intermittent)\n\n\n[SPOI-10399]  New permission do not reflect unless user logs out\n\n\n[SPOI-10401]  Deleted user can do any operations\n\n\n[SPOI-10406]  Application Configurations upload modal title should say \"Application Configuration Upload\"\n\n\n[SPOI-10409]  Updating app packages using \"check for updates\" option from AppHub gives wrong notification\n\n\n[SPOI-10410]  Deleting a property while creating tuple schema gives error in developer console\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-7720]   DT Hub shows just one version of an application\n\n\n[SPOI-10182]  dtAssemble - if there is unsaved change, keep launch button enabled which will pop up a dialog box asking save-and-launch when being clicked.\n\n\n[SPOI-8879]   Support custom JSON input for tuple schema creation\n\n\n[SPOI-9877]   Alerts History\n\n\n[SPOI-9876]   Alerts Notification\n\n\n[SPOI-9875]   Alerts Management\n\n\n[SPOI-8570]   Ability to filter tuple recording\n\n\n[SPOI-9525]   UI for dtDebug - Logs\n\n\n[SPOI-8499]   Create diagnostic tool for analyzing RM and container logs\n\n\n[SPOI-10364]  Update launch and configuration package views for simplified properties\n\n\n[SPOI-9772]   App Launch properties\n\n\n[SPOI-8764]   UI Console Configuration Packages support\n\n\n[SPOI-8611]   Support gateway configuration changes in UI\n\n\n[SPOI-10323]  Always show User profile even when license type is not enterprise\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-9785]   send GA events instead of page views for apphub page events\n\n\n[SPOI-10290]  Certification tool - RTS Installer Support for tool\n\n\n[SPOI-10179]  dtAssemble should warn about the unsaved changes when navigating away\n\n\n[SPOI-8989]   Show operator names under Physical -\n Containers\n\n\n[SPOI-10163]  dtDebug utilities README should have list of requirements  \n\n\n[SPOI-9881]   appAttempt page - Containers table - \"containerLogsUrl\" column - change it from showing a hyperlink to \"logs\" button.\n\n\n[SPOI-7343]   Ability to obtain a thread dump from a container\n\n\n[SPOI-3553]   Option to auto-tail container logs\n\n\n[SPOI-9133]   Gateway restart modal and button has soothing colors\n\n\n[SPOI-9132]   Gateway restart UI button has soothing colors\n\n\n[SPOI-9105]   Refactor security validation in console (consolidate resolve:{} from many places into one place)\n\n\n[SPOI-9047]   Make Package Upload Message clickable\n\n\n[SPOI-8766]   Make Physical DAG has the same metric selection (Top dropdown, Bottom dropdown) like Logical DAG does.\n\n\n[SPOI-8645]   Logical DAG, Physical DAG - show spinner in panel instead of blank panel before graph has been rendered and displayed.\n\n\n[SPOI-8644]   Do not show graph options(Show/Hide Stream Locality, Reset Position, Top dropdown, Bottom dropdown) until graph has been rendered and displayed.\n\n\n[SPOI-7810]   dtManage: Number of failures for an operator should have 'number search' option instead of 'string search'\n\n\n[SPOI-7277]   Ability to upload configuration file during app launch ( like -conf in dtcli )\n\n\n[SPOI-9418]   ConfigPackages backend support\n\n\n[SPOI-9400]   Change licensing for RTS to be managed/displayed in GB instead of MB\n\n\n[SPOI-9086]   Add support for DIGEST enabled Hadoop web services environment\n\n\n[SPOI-7963]   Show container stack trace in dtManage\n\n\n[SPOI-10230]  containerLogsUrl shown in appattempt table can be pretty-printed\n\n\n\n\nTask\n\n\n\n\n[SPOI-9291]   Calcite\n\n\n[SPOI-8027]   TBD: Apex Java high level API - Aggregation Part 1\n\n\n[SPOI-9769]   Tiles for apps on AppHub\n\n\n[SPOI-7965]   Productize certification tool to size RTS\n\n\n[SPOI-6350]   Gauge widget\n\n\n[SPOI-7433]   Update all relevant docs with AppHub\n\n\n[SPOI-9693]   Add Validate button in dtAssemble\n\n\n[SPOI-9688]   Remove autosave from dtAssemble\n\n\n[SPOI-9971]   Verify that alerts can only be sent by mail\n\n\n[SPOI-9364]   dtDebug Logs backend feature\n\n\n[SPOI-9695]   Launch application not using config package name\n\n\n[SPOI-9413]   Permission changes for tenancy \n\n\n[SPOI-7966]   Provide user ability to configure security through dtManage UI (only password option)\n\n\n[SPOI-8736]   dtManage should alert user when there's a potential Hadoop config issue\n\n\n[SPOI-9575]   Create demo app\n\n\n[SPOI-9021]   State management benchmark\n\n\n[SPOI-8933]   Change Megh repository to ASL  \n\n\n[SPOI-8865]   Operator Maturity Framework - Cassandra Output\n\n\n[SPOI-8788]   Operator Maturity Framework - Enhancement of FS Output Operator\n\n\n[SPOI-9966]   App-templates misc \n\n\n[SPOI-9774]   Update Database to HDFS app template \n\n\n[SPOI-9234]   AppHub - App Pipeline creation with continuous iteration\n\n\n[SPOI-10063]  Create new apex core build based on master\n\n\n[SPOI-9859]   Log retrieval tool\n\n\n[SPOI-9043]   Requirements discussion on Batch Support - Definition of Batch, Scheduling of Batch DAG, State of Batch, Replay of Batch and Monitoring of Batch  \n\n\n[SPOI-8990]   DAG Editor - unable to drag a connection stream from a port if having restriction DISABLE_APP_EDIT_STRUCTURE\n\n\n[SPOI-9972]   Document DT Gateway System Alerts\n\n\n[SPOI-9970]   Modify access to allow Admin (and ONLY admin) to set alerts\n\n\n[SPOI-9653]   Plan and implement (1 item in v1)for Gateway Alerts\n\n\n[SPOI-10261]  Show friendly message when user logs files have been removed.\n\n\n[SPOI-9163]   Refactor configPackages to configurations\n\n\n[SPOI-8223]   Troubleshooting improvements in dtManage\n\n\n[SPOI-9382]   QA - Operator Maturity Framework - AbstractFileInputOperator\n\n\n[SPOI-8885]   Operator Testing (Operator Maturity Framework)\n\n\n[SPOI-9483]   Superuser role and oAuth cleanups\n\n\n[SPOI-8996]   Add authentication configuration web services spc to gateway REST api doc\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-5852]   App Package page has a single word \"ago\" for modification time after importing pi demo\n\n\n[SPOI-6651]   Launching App from UI ignores APPLICATION_NAME attribute defined in properties.xml file\n\n\n[SPOI-7062]   dtHub UI - tags column - filter - searching from the beginning of a tag.\n\n\n[SPOI-7652]   AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings\n\n\n[SPOI-8039]   UI says \"An error occurred fetching data.\" after launching the application\n\n\n[SPOI-8349]   User should not be able to delete the default apps in ingestion-solution package\n\n\n[SPOI-8379]   Property editor for array of enum is not rendered correctly\n\n\n[SPOI-8489]   Application_Name attribute from the config file is not honored.\n\n\n[SPOI-8507]   Unable to launch an AppDataTracker application imported from dtHub\n\n\n[SPOI-8516]   DataTorrent rpm version inconsistency\n\n\n[SPOI-8522]   Unable to set roles while creating user in secure environment\n\n\n[SPOI-8523]   Users can kill the app even if privileges get revoked in secure environment \n\n\n[SPOI-8531]   Multiple MachineData demos are available at dtHub\n\n\n[SPOI-8534]   README.html for sandbox contains references to 'malhar-users'\n\n\n[SPOI-8536]   DT RTS gateway log floods with WARN message\n\n\n[SPOI-8542]   Installation: User home directory is not created by default\n\n\n[SPOI-8566]   Tuple Recording Modal Fixes\n\n\n[SPOI-8622]   Exception in retrieving app state in certification when application has not yet reached running state\n\n\n[SPOI-8630]   \"merge\" configurations option for appPackages also creates new applications\n\n\n[SPOI-8717]   Updating sandbox generates errors\n\n\n[SPOI-8781]   Buffer server metrics not available for physical operator in Metrics Chart\n\n\n[SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available\n\n\n[SPOI-8888]   Unable to see imported/uploaded/running applications on DT UI in SSL enable envornment \n\n\n[SPOI-8995]   Running through the unit tests in dtx creates a residual file\n\n\n[SPOI-9007]   Kryo Exception while re-deploying the DimensionsComputationFlexibleSingleSchemaPOJO operator\n\n\n[SPOI-9127]   Wrong notification provided by dtConsole when package upload is failed\n\n\n[SPOI-9134]   Gateway restart modal has incorrect focus\n\n\n[SPOI-9135]   Gateway restart modal should be horizontally and vertically aligned\n\n\n[SPOI-9140]   dtConsole shows \"Failed to parse\" error when 'Monitor' tab is refreshed\n\n\n[SPOI-9152]   Application package link is not working\n\n\n[SPOI-9153]   Hyperlink not required on AppPackage tab\n\n\n[SPOI-9161]   Recordings rest API gives wrong number of totalTuples\n\n\n[SPOI-9199]   Error running application due to YARN API exception\n\n\n[SPOI-9200]   dt-site.xml has a misguiding warning\n\n\n[SPOI-9245]   \nSet logging level\n cannot delete the set logs\n\n\n[SPOI-9431]   Disable tenant option in TenancyFilter\n\n\n[SPOI-9496]   Use the AppPackageOwner field instead of logged in user, while working with configPackages.\n\n\n[SPOI-9503]   AbstractFileInputOperator does not honor filePatternRegexp parameter\n\n\n[SPOI-9580]   APEXMALHAR-2314 Improper functioning in partitioning of sequentialFileRead property of FSRecordReader\n\n\n[SPOI-9658]   Apps are not filtered correctly using tags column on AppHub \n\n\n[SPOI-9660]   AppHub navigation tab is still seen as dtHub\n\n\n[SPOI-9696]   Prevent Navigation in DAG Diagram When Dragging Image Around\n\n\n[SPOI-9738]   DAG Diagram Doesn't Display Sometimes\n\n\n[SPOI-9783]   Operators stay in PENDING_DEPLOY\n\n\n[SPOI-9787]   Configuration package spelling error\n\n\n[SPOI-9790]   Recording Tuple Dialog Display Bug\n\n\n[SPOI-9791]   Fix log line format\n\n\n[SPOI-9793]   Can not start gateway after installation\n\n\n[SPOI-9794]   Can not start dtcli after installation \n\n\n[SPOI-9908]   Container StackTrace is not functioning\n\n\n[SPOI-9914]   Container buttons should be contextual based on state\n\n\n[SPOI-9916]   Kafka Input Operator (0.9) validation app is missing from QA/test-apps repository\n\n\n[SPOI-9933]   Unable to run JDBC Poll app on latest SNAPSHOT build (3.7.0)\n\n\n[SPOI-9934]   Notification History links, when clicked modal doesn't get closed\n\n\n[SPOI-9939]   Links to log files should not be restricted to enterprise edition\n\n\n[SPOI-9941]   Gateway password security errors\n\n\n[SPOI-9974]   Unable to upload packages to the gateway\n\n\n[SPOI-9977]   Unable to launch applications using apex CLI, gives ClassNotFoundException\n\n\n[SPOI-10002]  Config Package Page Not Showing Saved Properties\n\n\n[SPOI-10016]  Config package upload fails\n\n\n[SPOI-10018]  Unnecessary check boxes present for applications under Develop tab\n\n\n[SPOI-10020]  Tuple recording feature is not working\n\n\n[SPOI-10036]  Last modified time for imported packages is always shown with additional 2 minutes.\n\n\n[SPOI-10043]  User should not be allowed to save tupleSchema with blank values\n\n\n[SPOI-10044]  Editing existing tuple schema gives TypeError\n\n\n[SPOI-10046]  Deleting a property while creating tuple schema directly creates final schema with remaining properties\n\n\n[SPOI-10047]  Tuple schema created using JSON input does not take latest JSON as input\n\n\n[SPOI-10050]  Can not record samples\n\n\n[SPOI-10051]  Delete roles modal should have warm colors\n\n\n[SPOI-10052]  Restore roles modal should have warm colors\n\n\n[SPOI-10054]  Launch application for configuration package not using local settings for application naming\n\n\n[SPOI-10056]  Malhar-angular-table temporary fix for application packages and app properties lists\n\n\n[SPOI-10065]  User is allowed to \"Add System Property\" with blank value\n\n\n[SPOI-10068]  dtGateway script doesn't return correct status when gateway is down\n\n\n[SPOI-10080]  Issues with containers table from UI console\n\n\n[SPOI-10110]  AppPackage get info should not be used to show the configPackage apps\n\n\n[SPOI-10143]  StramEvents API gives 500 error\n\n\n[SPOI-10147]  \"cluster/metrics\" API gives 500 error\n\n\n[SPOI-10148]  Add system properties has weird titles\n\n\n[SPOI-10150]  Change system properties modal button should not be in cool colors\n\n\n[SPOI-10152]  Disable appdatatracker modal buttons should be in warm color\n\n\n[SPOI-10155]  AppDataTracker can not be enabled\n\n\n[SPOI-10156]  Clicking on \"ended apps\" and \"system apps\" multiple times shows multiple shadows of \"system apps\"\n\n\n[SPOI-10157]  Inspect port UI hangs\n\n\n[SPOI-10159]  Can not upload application packages\n\n\n[SPOI-10181]  killed applications - (1) \"AM Logs\" dropdown is empty. (2) AppMaster container does not have purple label in id column. \n\n\n[SPOI-10195]  Selected schema doesn't show the fields in the schema\n\n\n[SPOI-10200]  Button for \"Delete logging level\" is misaligned\n\n\n[SPOI-10209]  Link is missing for \"originalTrackingUrl\" field on currently running app attempt\n\n\n[SPOI-10228]  Sorting by \nhost\n in the Physical Plan -\n Containers tab is not working\n\n\n[SPOI-10229]  \"attempts\" tab for dtDebug is available in community edition\n\n\n[SPOI-10233]  Application attempts API does not return startedTime and finishedTime for FAILED attempts\n\n\n[SPOI-10235]  Kill Application Master container modal should have warm colors\n\n\n[SPOI-10236]  Kill selected container title has unwanted text\n\n\n[SPOI-10242]  Configuration Packages missing\n\n\n[SPOI-10257]  An error is shown for a while while launching Application Configurations\n\n\n[SPOI-10258]  Application package launch modal shows wrong \"Use configuration file\" option instead of \"Use configuration package\" \n\n\n[SPOI-10259]  Security configuration page on console has illegible content\n\n\n[SPOI-10260]  Links in Alert configuration modal are broken\n\n\n[SPOI-10277]  Stacktrace is not fully shown in alerts detail\n\n\n[SPOI-10279]  Update app hub description.\n\n\n[SPOI-10300]  ValidateApplication \n PutApplication should have CheckViewPermission instead of checkModifyPermission\n\n\n[SPOI-10343]  UI errors while displaying containers in Physical tab\n\n\n[SPOI-10350]  Jackson jars are missing from the RTS after Hadoop upgrade to 2.6 causing API failures\n\n\n[SPOI-10356]  Update Buttons Labels\n\n\n[SPOI-10358]  Schemas in ConfigPackages\n\n\n[SPOI-10359]  Schema in ConfigPackage Permission on 2 APIs should be reduced to view\n\n\n[SPOI-10361]  Upload of configPackage failed\n\n\n[SPOI-10381]  Unable to create configPackage from java application\n\n\n[SPOI-10387]  Unable to launch application configurations from \"Application Configuration details\" page\n\n\n[SPOI-10390]  Use Configuration Package Should be Enabled\n\n\n\n\nVersion: 3.6.0\n\n\nRelease date: Nov 9, 2016\n\n\nSummary\n\n\nDataTorrent RTS releases AppHub, a repository of application templates for various Big Data use cases. The key of this release is that RTS now have an infrastructure to distribute application templates easily. Developers can reduce the time to develop Big Data applications using templates. There are five templates in this release with many more to come. \n\n\n\n\nHDFS Sync \n\n\nAmazon S3 to HDFS Sync\n\n\nKafka to HDFS Sync\n\n\nHDFS to HDFS Line Copy\n\n\nHDFS to Kafka Sync\n\n\n\n\nAppendix\n\n\nImprovement\n\n\n\n\n[SPOI-9136] - Enforce DefaultOutputPort.emit() or Sink.put() thread affinity\n\n\n\n\nTask\n\n\n\n\n[SPOI-9118] - Publish App Templates for Ingestion on AppHub\n\n\n[SPOI-9419] - Update AppHub API to include the markdown content\n\n\n[SPOI-9432] - Update AppHub back end to extract markdown from apa\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-3277] - Show app master logs on UI for applications that fail at launch when we upgrade to Hadoop 2.4 or above\n\n\n[SPOI-9079] - Creation of example application for transform operator\n\n\n[SPOI-9235] - Allow users to create new configurations from Application Configurations view\n\n\n[SPOI-9240] - Create individual package view for AppHub artifacts\n\n\n[SPOI-9464] - Rename all references of AppHub on console UI to AppHub\n\n\n[SPOI-9574] - Rename title on AppHub list page\n\n\n[SPOI-9612] - Upgrade AppHub server deployment\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-9140] - dtManage shows \"Failed to parse\" error when 'Monitor' tab is refreshed\n\n\n[SPOI-9522] - DELETE call to /ws/v2/config/properties/{name} returns 500\n\n\n[SPOI-9727] - DTINSTALL_SOURCE incorrectly assumes file name\n\n\n\n\nVersion 3.5.0\n\n\nRelease date: Sep 26, 2016\n\n\nSummary\n\n\nDataTorrent RTS continues to deliver features that sets it apart in bringing operability in running an enterprise grade big data-in-motion platform. This particular release brought new features such as\n\n\n\n\nAllowing users to analyze \"stuck operator\" by obtaining stats from the running JVM (i.e. GC stats and thread dump)\n\n\nAbility to show/hide critical path in both logical and physical DAG\n\n\n\n\nApache Apex Malhar\n\n\nThe other important part of going to production is a library of operators that is more than just functional. They need to be fault tolerant, partitionable, support idempotency, and dynamically scalable. The recent release of Apache Apex Malhar 3.5.0 provides new and updated operators and APIs to bring those enterprise features. \n\n\n\n\nWindowed Operator that supports the windowing semantics outlined by Apache Beam and Google Cloud DataFlow, including the concepts of event time windows, session windows, watermarks, allowed lateness, and triggering.\n\n\nHigh level Java stream API now uses the aforementioned Windowed Operator to support stateful transformation with Apache Beam style windowing semantics.\n\n\nIntroduction of Spillable Data Structures that make use of Managed State.\n\n\nDeduper Operator to process  whether a given record is a duplicate or not\n\n\nEnricher Operator to join a stream with a lookup source and operate on any POJO object\n\n\nHBase input operator. Improve HBasePOJOInputOperator with support for threaded read\n\n\nFile Record reader module. It is useful for reading from files \"line by line\" in parallel and emit each line as seperate * tuple.\n\n\nJDBC Poll Input Operator\n\n\n\n\nFor the full release note, please go to\nhttps://blogs.apache.org/apex/entry/apache_apex_malhar_3_5\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-9232] Dedup with manage state operator marking all impression as duplicate\n\n\n[SPOI-9203]   \"check for updates\" option says 'no updated versions' and then displays updated packages\n\n\n[SPOI-9183]   Nested operator properties should follow order specified in the ORB on dtAssemble UI\n\n\n[SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-7343] - Ability to obtain a thread dump from a container\n\n\n[SPOI-8191] - Operator properties should follow order specified in the ORB on dtAssemble UI\n\n\n[SPOI-8352] - Warning message while restarting app should be changed\n\n\n[SPOI-8450] - Dedup ports connected to console should not write to log\n\n\n[SPOI-8722] - Logical DAG, Physical DAG - change \"Show/Hide String Locality\", \"Show/Hide Critical Path\" to checkbox.\n\n\n\n\nStory\n\n\n\n\n[SPOI-6794] - HBase Input Operator\n\n\n[SPOI-6795] - Creation of Concrete Cassandra Output\n\n\n[SPOI-6932] - Module to read from HDFS Input record by record and emit it downstream\n\n\n[SPOI-7948] - JDBC Input Operator\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-7836] - Trend widget fails after dashboard save/reload with PiDemo app\n\n\n[SPOI-7886] - Duplicate ports show up in the UI for POJO operators\n\n\n[SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same\n\n\n[SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500\n\n\n[SPOI-8721] - Column labeled buffer service size is showing bytes per second\n\n\n[SPOI-9084] - Refresh tokens failing in some scenarios with a login failure message\n\n\n[SPOI-9130] - dtConsole shows no applications on monitor tab\n\n\n[SPOI-9131] - gateway REST and WebSocket APIs for cluster metrics fail to report stats\n\n\n\n\nTask\n\n\n\n\n[SPOI-8411] - Deduper operator using Managed State\n\n\n[SPOI-9004] - Deduper Documentation\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-8389] - ORB defaults for FileSystem related operators\n\n\n[SPOI-8390] - Added operator default values in Application.json for user visibility\n\n\n[SPOI-8410] - Kafka Input Operator Unit test failed\n\n\n[SPOI-8461] - AbstractKafkaInputOperator problems\n\n\n\n\nVersion 3.4.0\n\n\nSummary\n\n\nAffinity rules provides a way to specify hints on how operators should be deployed in a Hadoop cluster. There are two types of rules: affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together. Anti-affinity rule, the new feature in Apache Apex 3.4.0, indicates that the group of operators should be allocated separately.\n\n\nThis release also includes a lot of bug fixes. Please see appendix for full list. \n\n\ndtManage\n\n\n\n\nUser can restart a killed application from dtManage\n\n\n\"Retrieve Ended Apps\" button changes to \"Hide Ended Apps\" after dtManage retrieves killed apps.\n\n\nUser can use mouse scroll to zoom in/out of physical DAG view\n\n\n\n\nApache Apex 3.4.0\n\n\n\n\nBlacklist problem nodes from future container requests\n\n\nSupport adding module to application using property file API.\n\n\nAbility to obtain a thread dump from a container\n\n\nRPC timeout is now configurable\n\n\nWhen an operator is blocked, it nows print out a warning instead of debug\n\n\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-8518] - Support links from Dashboard to Application instance pages\n\n\n[SPOI-8516] - Datatorrent rpm version inconsistency\n\n\n[SPOI-8470] - Check for already existing app package should be done before uploading the whole package\n\n\n[SPOI-8436] - Documentation is needed on \"How to use Transform operators in ingestion solution app package\"\n\n\n[SPOI-8434] - Documentation is needed on \"How to use Generic JDBC/PostgreSQL operators in ingestion solution app package\"\n\n\n[SPOI-8433] - Documentation is needed on \"How to use Enrichment operator in ingestion solution app package\"\n\n\n[SPOI-8414] - Drop down is not shown for \"Fields to copy\" property of \"POJO Enricher\" unless output schema is not specified\n\n\n[SPOI-8352] - Warning message while restarting app should be changed\n\n\n[SPOI-8296] - \"File Path\" and \"Output File Name\" properties for HDFS File Output Operator should be clubbed together\n\n\n[SPOI-8295] - \"Include Fields\" parameter for \"POJO Enricher\" has misleading description\n\n\n[SPOI-8294] - Operator Class Name for \"Delimited Parser\" operator should not be \"CSV Parser\"\n\n\n[SPOI-8293] - File permission property is not working properly for HDFS File Output Operator\n\n\n[SPOI-8291] - Parameters on dtAssemble should be logically ordered\n\n\n[SPOI-8224] - Providing Field Infos value for JDBC POJO Input Operator is too complex\n\n\n[SPOI-8192] - Clicking on edit option for apps throws validation errors in activity panel\n\n\n[SPOI-8158] - Options to modify property values should be closer to the property name on dtAssemble canvas\n\n\n[SPOI-8144] - \"Tuple Schemas\" link at top right corner of dtAssemble canvas should open in new tab\n\n\n[SPOI-8143] - Tuple Schema page not available directly from Develop tab\n\n\n[SPOI-8133] - When stream is added in dtAssemble, user should be notified if schema is required\n\n\n[SPOI-8131] - Couple of parameters for Kafka Input Operator should have dropdown selection in dtAssemble\n\n\n[SPOI-8130] - Documentation for ingestion beta operators need to be improved\n\n\n[SPOI-8128] - Port names for operators are not intuitive when displayed on dtAssemble canvas\n\n\n[SPOI-8087] - JDBC input operator query should not require explicit ordering of column names\n\n\n[SPOI-8038] - Output file names for HDFS output should not contain timestamp and '.tmp' extension\n\n\n[SPOI-8522] - Unable to set role while creating user in a secure environment.  There is a workaround by create user with no role and then assign the role\n\n\n[SPOI-8523] - User can kill app even though privileges got revoked.  This applies to secure environment only.  \n\n\n[SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500\n\n\n[SPOI-8536] - DT RTS gateway log floods with WARN message\n\n\n[SPOI-8535] - Need to restart dtgateway for enabling password authentication in sandbox\n\n\n[SPOI-8534] - README.html for sandbox contains references to 'malhar-users'\n\n\n[SPOI-8533] - Importing 'Apache Apex Malhar Iteration Demo' throws error for 'property' tag in properties.xml\n\n\n[SPOI-8532] - Importing packages from dtHub sometimes gives ZipException\n\n\n[SPOI-8531] - Multiple MachineData demos are available at dtHub\n\n\n[SPOI-8524] - Clicking on \"generate new dashboard\" first navigates to 'Learn' tab on the dtConsole\n\n\n[SPOI-8523] - Users can kill the app even if privileges get revoked in secure environment\n\n\n[SPOI-8522] - Unable to set roles while creating user in secure environment\n\n\n[SPOI-8519] - Ingestion application on dtHub still shows 'requires Apex version' with \"-incubating\"\n\n\n[SPOI-8511] - Gateway Websocket API leaks information while unauthorized\n\n\n[SPOI-8509] - dtAssemble operator documentation shows '@link' markers\n\n\n[SPOI-8507] - Unable to launch an AppDataTracker application imported from dtHub\n\n\n[SPOI-8491] - UI mixes the order and ids of tuple recording ports\n\n\n[SPOI-8490] - gateway issues in SSL enabled cluster\n\n\n[SPOI-8479] - Uninstall does not work even though the install was successful previously\n\n\n[SPOI-8477] - Upgrade License Opens in dtManage window, it should be in opened up in new window\n\n\n[SPOI-8476] - Hadoop-common-tests library shouldn't be part of RTS build\n\n\n[SPOI-8474] - On addition of huge role name, non-specific errors are shown\n\n\n[SPOI-8473] - Gateway, console allows impractially longer user roles additions\n\n\n[SPOI-8472] - Visually ugly error presentation\n\n\n[SPOI-8469] - If Kerberos tickets are changed, you have to refresh whole UI\n\n\n[SPOI-8467] - installation script provides incorrect information\n\n\n[SPOI-8432] - JDBC input operator is failing with exception \"fetching metadata\"\n\n\n[SPOI-8424] - Dedup does not honor the expiryPeriod when error tuple is introduced in between two valid tuples\n\n\n[SPOI-8422] - Time properties for operators should have units mentioned for them\n\n\n[SPOI-8416] - dtAssemble Can't change application name\n\n\n[SPOI-8365] - HDFS sync app : Unable to sync 500 GB file\n\n\n[SPOI-8358] - Uptime and latency values are very high exactly after app is launched\n\n\n[SPOI-8317] - dtIngest 1.1.0 (Compiled against 3.2.0) can not be launched\n\n\n[SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same\n\n\n[SPOI-8213] - Application DAG is not displayed when clicked on app link\n\n\n[SPOI-8202] - Unable to add custom properties while launching apps\n\n\n[SPOI-8197] - Default values for \"Field Infos\" and \"Bucket Manager\" properties should be set appropriately\n\n\n[SPOI-7986] - Gateway proxy feature is not working\n\n\n[SPOI-7934] - Kafka-dedup-HDFS-solution: Ahead in time messages are lost from Dedup operator\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-7939] - Dynamic repartition causes application to hang\n\n\n[SPOI-8468] - Can't assign roles for users in secure environment\n\n\n[SPOI-8464] - \"Disable Reporting\" option on System Configuration page gives NullPointerException\n\n\n[SPOI-8024] - Gateway is leaving behind dtcheck temp files in HDFS\n\n\n[SPOI-7640] - Changing dashboard name in dashboard settings modal and then canceling does not revert dashboard name\n\n\n[SPOI-8077] - Gateway logs NPE if an app master is misbehaving\n\n\n[SPOI-8046] - Kerberos Cluster: Installation Wizard can not get past beyond Hadoop Configuration screen\n\n\n[SPOI-8055] - Console references to dt-text-tooltip no longer produce a tool tip\n\n\n[SPOI-7898] - Default JAAS support classes duplicated in dt-library\n\n\n[SPOI-7929] - Update log4j.properties in the DTX/dist/install to set debug level for org.apache.apex\n\n\n[SPOI-7943] - The demo applications fails due to numberOfBuckets is less than 1\n\n\n[SPOI-8092] - Problems in launching jobs with authentication enabled on secure cluster\n\n\n[SPOI-7794] - Monitor page not refreshing properly\n\n\n[SPOI-7889] - Cannot read property 'hideBreadcrumbs' of undefined\n\n\n[SPOI-7856] - Modify application packages dtHub import/update paths\n\n\n[SPOI-7907] - Downloads of AppPackages result in corrupted files during local testing\n\n\n[SPOI-7971] - After dynamic repartition application appears blocked\n\n\n\n\nApache Apex 3.4.0\n\n\nNew Feature\n\n\n\n\n[APEXCORE-10] - Enable non-affinity of operators per node (not containers)\n\n\n[APEXCORE-359] - Add clean-app-directories command to CLI to clean up data of terminated apps\n\n\n[APEXCORE-411] - Restart app without specifying app package\n\n\n\n\nImprovement\n\n\n\n\n[APEXCORE-92] - Blacklist problem nodes from future container requests\n\n\n[APEXCORE-107] - Support adding module to application using property file API.\n\n\n[APEXCORE-304] - Ability to add jars to classpath in populateDAG\n\n\n[APEXCORE-328] - CLI tests should not rely on default maven repository or mvn being on the PATH\n\n\n[APEXCORE-330] - Ability to obtain a thread dump from a container\n\n\n[APEXCORE-358] - Make RPC timeout configurable\n\n\n[APEXCORE-380] - Idle time sleep time should increase from 0 to a configurable max value\n\n\n[APEXCORE-383] - Time to sleep while reservoirs are full should increase from 0 to a configurable max value\n\n\n[APEXCORE-384] - For smaller InlineStream port queue size use ArrayBlockingQueueReservoir as default\n\n\n[APEXCORE-399] - Need better debug information in stram web service filter initializer\n\n\n[APEXCORE-400] - Create documentation for security\n\n\n[APEXCORE-401] - Create a separate artifact for checkstyle and other common configurations\n\n\n[APEXCORE-407] - Adaptive SPIN_MILLIS for input operators\n\n\n[APEXCORE-409] - Document json application format\n\n\n[APEXCORE-419] - When operator is blocked, print out a warning instead of debug\n\n\n[APEXCORE-447] - Document: update AutoMetrics with AppDataTracker link\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block\n\n\n[APEXCORE-201] - Reported latency is wrong when a downstream operator is behind more than 1000 windows\n\n\n[APEXCORE-326] - Iteration causes problems when there are multiple streams between two operators\n\n\n[APEXCORE-335] - StramLocalCluster should teardown StreaminContainerManager after run is complete\n\n\n[APEXCORE-349] - Application/operator/port attributes should be returned using string codec in REST service\n\n\n[APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers\n\n\n[APEXCORE-352] - Temp directories/files not created in temp directory specified by system property java.io.tmpdir\n\n\n[APEXCORE-353] - Buffer server may stop processing data\n\n\n[APEXCORE-355] - CLI list-*-attributes command name change\n\n\n[APEXCORE-362] - NPE in StreamingContainerManager\n\n\n[APEXCORE-363] - NPE in StreamingContainerManager\n\n\n[APEXCORE-374] - Block with positive reference count is found during buffer server purge\n\n\n[APEXCORE-375] - Container killed because of Out of Sequence tuple error.\n\n\n[APEXCORE-376] - CLI command 'dump-properties-file' does not work when connected to an app\n\n\n[APEXCORE-385] - Temp directories/files not always cleaned up when launching applications\n\n\n[APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily\n\n\n[APEXCORE-393] - Reset failure count when consecutive failed node is removed from blacklist\n\n\n[APEXCORE-397] - Allow configurability of stram web services authentication\n\n\n[APEXCORE-398] - Ack may not be delivered from buffer server to it's client\n\n\n[APEXCORE-403] - DelayOperator unit test fails intermittently\n\n\n[APEXCORE-413] - Collision between Sink.getCount() and SweepableReservoir.getCount()\n\n\n[APEXCORE-415] - Input Operator double checkpoint\n\n\n[APEXCORE-421] - Double Checkpointing May Happen In Input Node On Shutdown\n\n\n[APEXCORE-422] - Checkstyle rule related to allowSamelineParameterizedAnnotation suppression\n\n\n[APEXCORE-434] - ClassCastException when making webservice calls to STRAM in secure mode\n\n\n[APEXCORE-436] - Update log4j.properties in archetype test resources to set debug level for org.apache.apex\n\n\n[APEXCORE-439] - After dynamic repartition application appears blocked\n\n\n[APEXCORE-444] - 401 authentication errors when making webservice calls to STRAM in secure mode\n\n\n[APEXCORE-445] - Race condition in AsynFSStorageAgent.save()\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-293] - Add core and malhar documentation to project web site\n\n\n[APEXCORE-319] - Document backward compatibility guidelines\n\n\n[APEXCORE-340] - Rename dtcli script to apex\n\n\n[APEXCORE-345] - Upgrade to 0.7.0 japicmp\n\n\n[APEXCORE-381] - Upgrade async-http-client dependency version because of security issue\n\n\n[APEXCORE-410] - Upgrade to netlet 1.2.1\n\n\n[APEXCORE-423] - Fix style violations in Apex Core\n\n\n[APEXCORE-446] - Add source jar in the shaded-ning build\n\n\n\n\nSub-task\n\n\n\n\n[APEXCORE-254] - Introduce Abstract and Forwarding Reservoir classes\n\n\n[APEXCORE-269] - Provide concrete implementation of AbstractReservoir based on SpscArrayQueue\n\n\n[APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size\n\n\n[APEXCORE-369] - Fix timeout in AbstractReservoirTest.performanceTest\n\n\n[APEXCORE-402] - SpscArrayQueue to notify publishing thread on not full condition\n\n\n\n\nVersion 3.3.0\n\n\nSummary\n\n\ndtHub\n\n\nDataTorrent RTS allows companies to quickly build low latency real time Big Data application that can scale and fault tolerant. With the introduction of dtHub, DataTorrent now host and maintain an application distributor. You can access it through dtManage and easily install or update application without having to upgrade the whole RTS platform. Prior to dtHub, RTS installer packaged both platform and applications. In 3.3, they are decoupled with significantly reduced installer size. You can independently choose when to upgrade the platform and when to install/upgrade applications.\n\n\ndtManage\n\n\n\n\nTroubleshooting is made easier now that user can view the containers where physical operators have lived\n\n\nRTS Community Edition user can now view container logs and set log levels via dtManage. API for runtime DAG and property change are also available\n\n\nRTS Enterprise Edition is changed from 30 days to 60 days\n\n\nWhen an application is killed, RTS will delete the app directory according to policy  in dt-site.xml\n\n\n\n\ndtDasbhoard\n\n\n\n\nNew widgets for the dashboard: Geo coordinates with circles, Geo regions with gradient fill and single Value \n\n\n\n\ndtAssemble (beta)\n\n\n\n\nTop level \u201cDevelop\u201d takes users directly to Application Page\n\n\nNavigation changes on how user get to Tuple Schema. User goes to It is now \"Edit Application\" then \"Tuple Schema\"\n\n\nDevelopment (breadcrumb link)\n\n\n\n\nDocumentation\n\n\n\n\nFileSplitter: http://docs.datatorrent.com/operators/file_splitter/\n\n\nBlock Reader: http://docs.datatorrent.com/operators/block_reader/\n\n\n\n\nApache Apex 3.3\n\n\n\n\nSupport for iterative processing. It is a building block to support machine learning\n\n\nAbility to populate DAG at application launch time\n\n\nPre checkpoint operator callback so that it can execute a logic before the operator gets checkpointed (e.g. flush file to HDFS)\n\n\nProvide the option for operator to do checkpointing in a distribute in-memory store. It is faster than HDFS due to disk i/o latency\n\n\nAdd group ID information in an applicatin package.  It is visible for application grouping in dtHub.\n\n\n\n\nKnown Issues in 3.3\n\n\n\n\n[SPOI-7696] - Community edition (which gets activated after expired license) does not have newly added community features\n\n\n[SPOI-7471] - Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7697] - \"Set Logging Levels\" option on application details page does not show initial target/loglevel fields\n\n\n[SPOI-7668] - If AppDataTracker is disabled, changing the YARN queue does not restart it\n\n\n[SPOI-7652] - AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings\n\n\n[SPOI-7678] - On configuration complete, summary page should populate RTS version\n\n\n[SPOI-7479] - dtHub \"check for updates\" option says 'no updated versions' and then displays updated packages\n\n\n[SPOI-7626] - Stacked Area Chart widget shows NaN values on Firefox\n\n\n\n\nAppendix\n\n\ndtHub\n\n\n\n\n[SPOI-6787] - dtHub UI - explore, import, download\n\n\n[SPOI-7023] - dtHub UI - check for updates\n\n\n[SPOI-3643] - Remove import default packages from Application Packages page\n\n\n[SPOI-7451] - Add options summary to Application Packages screen\n\n\n[SPOI-6968] - Please make API return a new property that indicates whether gateway has internet access or not (for dtHub feature)\n\n\n[SPOI-7059] - add \"tags\" to import list\n\n\n\n\ndtManage\n\n\n\n\n[SPOI-3549] - dtManage - Ability to view container history where physical operator has lived\n\n\n[SPOI-7382] - UI evaluation license expiration message colour update\n\n\n[SPOI-7418] - Visualize AppDataTracker data\n\n\n[SPOI-7129] - Add countdown and links to enterprise evaluation in dtManage\n\n\n[SPOI-7226] - Remove choice of community edition and enterprise evaluation in install wizard\n\n\n\n\ndtDashboard\n\n\n\n\n[SPOI-6522] - Geo coordinates with weighted circles widget\n\n\n[SPOI-6523] - Geo regions with gradient fill widget\n\n\n[SPOI-7247] - Create dimensional single value widget\n\n\n[SPOI-7258] - Persist label changes in trend and single value widgets\n\n\n[SPOI-6023] - dtDashboard - Widgets that support dimension schema can support multi-value keys \n\n\n[SPOI-6021] - Support tag for  snapshot server and dimension store\n\n\n[SPOI-6331] - Notify user when widget is unable to automatically load data\n\n\n[SPOI-6658] - UI says \"no rows testing\" when no data available to display in tables\n\n\n[SPOI-6887] - Changing choropleth map class does not remove previously selected map\n\n\n[SPOI-7246] - Change default widget colors to be websafe\n\n\n[SPOI-7257] - add tags-based dimension query settings in trend widget \n\n\n\n\ndtAssemble (beta)\n\n\n\n\n[SPOI-7133] - Tuple Schemas change and Develop on top navigation bar change\n\n\n\n\nMegh\n\n\n\n\n[SPOI-7665] - Megh enhancements for TelecomDemo\n\n\n[SPOI-7230] - Add group id information to all megh app packages\n\n\n[SPOI-7251] - Add directory structure for modules in megh\n\n\n[SPOI-7693] - Add Telecom Demo To Megh\n\n\n\n\nDocs\n\n\n\n\n[SPOI-6471] - Documentation: FileSplitter\n\n\n[SPOI-6472] - Documentation: BlockReader\n\n\n\n\nRTS Community Edition\n\n\n\n\n[SPOI-7670] - Removing dtManage restrictions from community edition\n\n\n[SPOI-7682] - Lock down all auth/security features in community edition\n\n\n[SPOI-7130] - Community edition to have an upgrade button to Enterprise eval\n\n\n[SPOI-7243] - make all locked-feature places(e.g. Visualize link, logs button, new application button) to have the same effect of \"upgrade to enterprise                \" link in community edition, and to have a key icon instead of being crossing out\n\n\n\n\nRTS Enterprise Edition\n\n\n\n\n[SPOI-7127] - Change enterprise evaluation days from 30 days to 60 days\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-5622] - dtcli: Command 'show-physical-plan' fails with \"Failed web service request\" error\n\n\n[SPOI-6352] - Gateway's RM Proxy REST calls don't work with HA-enabled\n\n\n[SPOI-6424] - Gateway cannot determine its own local address to the cluster when RM HA is enabled\n\n\n[SPOI-6555] - Add Missing datatorrent.apppackage.classpath property to dt-demos\n\n\n[SPOI-6624] - ADT issues impacting dtingest Dashboard\n\n\n[SPOI-6674] - AbstractFileOuptutOperator refactoring and fixes\n\n\n[SPOI-6834] - fixing scope of jars in Megh\n\n\n[SPOI-6837] - Gateway has a lot of blocked threads under heavy load\n\n\n[SPOI-6886] - Selected map feature / object should be persisted\n\n\n[SPOI-6949] - Collation in BucketManager incurs a performance hit while writing to HDFS and is not an optimization\n\n\n[SPOI-6999] - Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7026] - only lists the ones that have different versions in update section\n\n\n[SPOI-7027] - fix a bug that loading is forever when there is no updates in check for updates\n\n\n[SPOI-7028] - compare version number and only list packages whose dtHub version is higher than installed version in update section\n\n\n[SPOI-7037] - support sorting, filter in string-type columns in import pkgs, update pkgs\n\n\n[SPOI-7040] - optimize visual layout of import pkgs page\n\n\n[SPOI-7042] - only shows packages that are compatible with the APEX version in check for update list\n\n\n[SPOI-7080] - remove bar chart widget that is not in use\n\n\n[SPOI-7106] - Update Megh japi version and fix the broken build because of semantic version\n\n\n[SPOI-7138] - Dimension unifier return empty results \n\n\n[SPOI-7236] - Console build fails due to jsHint issues after updating version\n\n\n[SPOI-7293] - Post installation links no longer available on datatorrent.com\n\n\n[SPOI-7296] - gateway spills out error trying to write to /var/log/datatorrent/ in local install\n\n\n[SPOI-7297] - Local install fails in secure mode\n\n\n[SPOI-7309] - HDHT Broken and Hangs After Wall And Purge Changes\n\n\n[SPOI-7338] - After changing configuration \"dt.appDataTracker.queue\" (e.g. to \"root.ashwin\") and kill system application AppDataTracker, AppDataTracker                 incorrectly starts with the default queue (\"root.dtadmin\").\n\n\n[SPOI-7350] - Installation wizard final step refers to invalid developers URL\n\n\n[SPOI-7361] - get-operator-attributes fails on CDH\n\n\n[SPOI-7383] - Update invalid links in UI console info section\n\n\n[SPOI-7447] - dtHub UI - check for updates - when there is no newer version for any installed package, loading image will be hanging forever\n\n\n[SPOI-7460] - On Visualize tab, link for documentation does not work\n\n\n[SPOI-7469] - Remove \"in HDHT\" from System Configuration page\n\n\n[SPOI-7471] - Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7474] - \"Disable App Data Tracker\" option does not work\n\n\n[SPOI-7481] - Launch macros for demo apps should be removed in dtcli\n\n\n[SPOI-7482] - Cloning/deleting application under Application package does not refresh the list\n\n\n[SPOI-7498] - Verify dtingest download link change on datatorrent website\n\n\n[SPOI-7502] - Lock mishandling in App Package local cache code\n\n\n[SPOI-7503] - Changing \"dt.appDataTracker.enable=true/false\" using REST API calls doesn't take effect unless gateway restarts\n\n\n[SPOI-7510] - Application launch notifications no longer show up\n\n\n[SPOI-7523] - Kill only AppDataTracker whose user is the current login user when App Data Tracker queue is changed.\n\n\n[SPOI-7542] - Unable to import app package from dtHub\n\n\n[SPOI-7574] - dtcli command 'dump-properties-file' does not work when connected to an app\n\n\n[SPOI-7577] - For 3.3.0 release, RTS version is shown as 3.3.1 in System Configuration\n\n\n[SPOI-7604] - HDHT bucket meta class is obfuscated incorrectly\n\n\n[SPOI-7615] - dtHub intermittently throws error as \"Failed to import\" while importing multiple pkgs at the same time \n\n\n[SPOI-7619] - specify bower link malhar-angular-dashboard to version 1.0.1\n\n\n[SPOI-7644] - Need to update installer script\n\n\n[SPOI-7679] - Unable to access newly added features for Community edition.\n\n\n\n\nApache Apex 3.3 Change Logs\n\n\nhttps://github.com/apache/incubator-apex-core/blob/v3.3.0-incubating/CHANGELOG.md\n\n\n\n\n[SPOI-7061] - Implement retention policy for terminated apps\n\n\n[SPOI-7492] - DT_GATEWAY_CLIENT_OPTS overrides all JVM options and there is no way to supply additional options to the default options\n\n\n[SPOI-5735] - Create local file cache for app package\n\n\n[SPOI-6981] - Move orderedOutput feature to AbstractDeduper. Rename AbstractDeduperOptimized to AbstractBloomFilterDeduper\n\n\n[SPOI-7448] - Work around the attribute bug detailed in APEXCORE-349 so that ADT still works\n\n\n[SPOI-7470] - Work around namenode NPE bug in hadoop 2.7.x to avoid throwing NPE to the user. https://issues.apache.org/jira/browse/APEXCORE-45 and https://issues.apache.org/jira/browse/HDFS-9851\n\n\n[SPOI-6381] - Support Dynamically Updating Enum Values For Keys In The Dimensions Store\n\n\n[SPOI-6545] - Allow Gateway to directly contact dtHub to install app packages\n\n\n\n\nNew Feature\n\n\n\n\n[APEXCORE-3] - Ability for an operator to populate DAG at launch time\n\n\n[APEXCORE-60] - Iterative processing support\n\n\n[APEXCORE-78] - Pre-Checkpoint Operator Callback\n\n\n[APEXCORE-276] - Make App Data Push transport pluggable and configurable\n\n\n[APEXCORE-283] - Operator checkpointing in distributed in-memory store\n\n\n[APEXCORE-288] - Add group id information to apex app package\n\n\n\n\nImprovement\n\n\n\n\n[APEXCORE-40] - Semver dependencies should be in Maven Central\n\n\n[APEXCORE-162] - Enhance StramTestSupport.TestMeta API\n\n\n[APEXCORE-181] - Expose methods in StramWSFilterInitializer to get the RM webapp address\n\n\n[APEXCORE-188] - Make type graph lazy load\n\n\n[APEXCORE-199] - CLI should check for version compatibility when launching app package\n\n\n[APEXCORE-228] - Add maven 3.0.5 as prerequisites to the Apex parent pom\n\n\n[APEXCORE-229] - Upgrade checkstyle maven plugin (2.17) and checkstyle dependency (6.11.2)\n\n\n[APEXCORE-291] - Provide a way for an operator to specify its metric aggregator instance\n\n\n[APEXCORE-305] - Enable checkstyle violations logging to console during maven build\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-58] - endWindow is being called even when the operator is being undeployed\n\n\n[APEXCORE-83] - beginWindow not called on recovery\n\n\n[APEXCORE-193] - apex-app-archetype has extraneous entry that generates a warning when running it\n\n\n[APEXCORE-204] - Update checkstyle and codestyle to be the same\n\n\n[APEXCORE-211] - Brace placement after static blocks in checkstyle configuration\n\n\n[APEXCORE-263] - Checkpoint can be performed twice for same window\n\n\n[APEXCORE-274] - removeTerminatedPartition fails for Unifier operator\n\n\n[APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection\n\n\n[APEXCORE-278] - GenericNodeTest clutters test logs with unnecessary statement\n\n\n[APEXCORE-296] - Memory leak in operator stats processing\n\n\n[APEXCORE-300] - Fix checkstyle regular expression\n\n\n[APEXCORE-303] - Launch properties not evaluated\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-24] - Takes out usage of Rhino as it is GPL 2.0\n\n\n[APEXCORE-186] - Enable license check in Travis CI\n\n\n[APEXCORE-253] - Apex archetype includes dependencies which do not belong to org.apache.apex\n\n\n[APEXCORE-298] - Reduce the severity of  line length check\n\n\n[APEXCORE-301] - Add \"io\" as a separate import to checkstyle rules\n\n\n[APEXCORE-302] - Update NOTICE copyright year\n\n\n[APEXCORE-308] - Implement findbugs plugin reporting\n\n\n[APEXCORE-317] - Run performance benchmark for the Apex Core 3.3.0 release\n\n\n\n\nSub-task\n\n\n\n\n[APEXCORE-104] - Expand Module DAG\n\n\n[APEXCORE-105] - Support injecting properties through xml file on modules.\n\n\n[APEXCORE-144] - Provide REST api for listing information about module.\n\n\n[APEXCORE-151] - Provide code style templates for major IDEs (Eclipse, IntelliJ and NetBeans)\n\n\n[APEXCORE-182] - Add Apache copyright to IntelliJ\n\n\n[APEXCORE-194] - Add support for ProxyPorts in Modules\n\n\n[APEXCORE-226] - Strictly enforce wrapping indentation in checkstyle\n\n\n[APEXCORE-227] - Enforce left brace placement for anonymous class on the next line\n\n\n[APEXCORE-230] - Limit line lengths to be 120\n\n\n[APEXCORE-239] - Upgrade checkstyle to 6.12 from 6.11.2\n\n\n[APEXCORE-248] - Increase wrapping indentation from 2 to 4.\n\n\n[APEXCORE-249] - Enforce class, method, constructor annotations on a separate line\n\n\n[APEXCORE-250] - Exclude DtCli from System.out checks\n\n\n[APEXCORE-267] - Fix existing checkstyle violations in api\n\n\n[APEXCORE-270] - Enforce checkstyle validations on test classes\n\n\n[APEXCORE-272] - Attributes added to operator inside Module is not preserved.\n\n\n[APEXCORE-273] - Fix existing checkstyle violations in bufferserver module\n\n\n[APEXCORE-306] - Recovery checkpoint handing in iteration loops\n\n\n\n\nVersion 3.2.1\n\n\nSummary\n\n\nThis release bundles a few fixes and features for customers who are on 3.2.0 and not ready to upgrade to 3.3.0  \n\n\nImprovement\n\n\n\n\n[SPOI-7900] add default role to first time login user\n\n\n[SPOI-7061] Implement retention policy for terminated apps\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7471] Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7971] After dynamic repartition application appears blocked\n\n\n\n\nKnown Issues\n\n\n\n\n[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7061] Implement retention policy for terminated apps\n\n\n[SPOI-7471] Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7971]After dynamic repartition application appears blocked\n\n\n\n\nApache Apex\n\n\n\n\n[APEXCORE-327] - Implement proper semantic version checks in patch release branches\n\n\n[APEXCORE-358] - Make RPC timeout configurable\n\n\n[APEXCORE-410] - Upgrade to Netlet 1.2.1\n\n\n[APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size\n\n\n\n\nApache Apex Bug Fixes\n\n\n\n\n[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block\n\n\n[APEXCORE-274] - removeTerminatedPartition fails for unifier operator\n\n\n[APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection\n\n\n[APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers\n\n\n[APEXCORE-353] - Buffer server may stop processing data\n\n\n[APEXCORE-362] - NPE in StreamingContainerManager\n\n\n[APEXCORE-363] - NPE in StreamingContainerManager\n\n\n[APEXCORE-374] - Block with positive reference count is found during buffer server purge\n\n\n[APEXCORE-375] - Container killed because of Out of Sequence tuple error.\n\n\n[APEXCORE-385] - Temp directories/files not always cleaned up when launching applications\n\n\n[APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily\n\n\n[APEXCORE-398] - Ack may not be delivered from buffer server to it's client\n\n\n\n\nVersion 3.2.0\n\n\nNew Feature\n\n\n\n\n[SPOI-6351] - Add feature to REST API to get queue information from cluster\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n[SPOI-6641] - Implement \"forever\" bucket in DimensionComputation\n\n\n[DTIN-40] - Observed unused variables in SplunkBytesInputOperator\n\n\n[DTIN-69] - Move Query Operators implementation to newer one\n\n\n[DTIN-50] - [dtIngest] add parameter \"parallel readers\"\n\n\n\n\nBug\n\n\n\n\n[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory\n\n\n[SPOI-5216] - FileSplitter fails with ConcurrentModificationException\n\n\n[SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'\n\n\n[SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up\n\n\n[SPOI-5823] - Downstream container falling behind when buffer spooling is enabled\n\n\n[SPOI-5899] - Ability to retrieve schemas from an app package\n\n\n[SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation\n\n\n[SPOI-6098] - 'single run' doesnt work\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6236] - Add 1s aggregations to App Data Tracker\n\n\n[SPOI-6298] - Dimensions Store Can Become Blocked\n\n\n[SPOI-6383] - Sometimes expired query is still executed.\n\n\n[SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console\n\n\n[SPOI-6446] - Merge PojoEnrichment and TupleEnrichment\n\n\n[SPOI-6505] - Exceptions from asm when uploading apps\n\n\n[SPOI-6603] - Warning messages shown on console are not completely visible\n\n\n[SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks\n\n\n[SPOI-6846] - DT dashboard guide link is not working\n\n\n[SPOI-6855] - Broken links observed on summary page while DT RTS configuration \n\n\n[SPOI-6856] - Correct docs index.html links and title\n\n\n[DTIN-51] - bandwidth option was removed during merge\n\n\n[DTIN-101] - For message-based-input to message-based-output, compression \n encryption options should not be visible on config UI\n\n\n[DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum\n\n\n[DTIN-112] - Message based input to FTP output fails with IOException while appending the data\n\n\n[DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata\n\n\n[DTIN-123] - All files are not getting copied when bandwidth option is specified\n\n\n[DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning\n\n\n[DTIN-126] - 'Compact files' option should be disabled for message based Input type\n\n\n[DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file\n\n\n[DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only\n\n\n[DTIN-155] - Messages are dropped when bandwidth option is enabled\n\n\n[DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError\n\n\n[DTIN-161] - Message based input to S3N output fails with IOException while appending the data\n\n\n[DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka\n\n\n[DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled\n\n\n[DTIN-165] - Data source names for dtingest should not contain 'null'\n\n\n[DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream\n\n\n[DTIN-176] - Parallel read is not working in case of FTP and S3 as input\n\n\n[DTIN-186] - FileMerger failed with unable to merge file exception\n\n\n\n\nTask\n\n\n\n\n[SPOI-5173] - DAG validation: Attribute values Serializable\n\n\n[SPOI-5403] - Ingestion Splunk integration\n\n\n[SPOI-5801] - Make a MapR partner (datatorrent) sandbox\n\n\n[SPOI-5958] - dtView Integration for ingestion metric visualization\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n[SPOI-6439] - Run benchmark for 3.2.0 release\n\n\n[SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies\n\n\n[DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script\n\n\n[DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator\n\n\n[DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch\n\n\n[DTIN-54] - Disabling the splunk from dtIngest script\n\n\n[DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-5369] - Integrate schema support in Cassandra Input/Output\n\n\n[SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output\n\n\n[SPOI-5819] - Bandwidth control for file based sources\n\n\n[SPOI-5820] - Bandwidth control for message based sources\n\n\n[SPOI-5959] - Metrics for compression, encryption\n\n\n[SPOI-6337] - Expose bandwidth metrics\n\n\n[SPOI-6441] - Change console home page to be welcome screen instead of operations summary\n\n\n\n\nVersion 3.1.1\n\n\nImprovement\n\n\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n\n\nBug\n\n\n\n\n[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock\n\n\n[SPOI-6032] - App Builder should not show property from super class\n\n\n[SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6090] - Ingestion: Error decrypting files for message based sources\n\n\n[SPOI-6147] - Launch issue with \"Starter Application Pack\"\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6203] - -ve Memory reported for Application Master\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager\n\n\n[SPOI-6304] - Fix netlet dependency\n\n\n[SPOI-6313] - Work Around APEX-129\n\n\n[SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property\n\n\n\n\nTask\n\n\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-6148] - Update website with release 3.1\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n\n\nVersion 3.1.0\n\n\nNew Feature\n\n\n\n\n[SPOI-4670] - Enable message schema management in App Builder\n\n\n[SPOI-5844] - Retrieve older versions of schemas\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n\n\nBug\n\n\n\n\n[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.\n\n\n[SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5743] - App package import also needs to do the typegraph stuff as upload\n\n\n[SPOI-5831] - Add getter for properties for Twitter Demo\n\n\n[SPOI-5833] - Schema class not loaded during validation\n\n\n[SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts\n\n\n[SPOI-5857] - Gateway has trouble getting to STRAM until after restart\n\n\n[SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml\n\n\n[SPOI-5946] - Port compatibility when port require schemas\n\n\n[SPOI-6002] - AppBuilder fails to load operator due to NullPointerException\n\n\n\n\nTask\n\n\n\n\n[SPOI-5307] - Mocha based tests for Gateway API calls\n\n\n[SPOI-5749] - Certify MapR sandbox\n\n\n[SPOI-5750] - Create Splunk forwarder operators\n\n\n[SPOI-5751] - Create splunk forwarder input operator\n\n\n[SPOI-5752] - Create splunk forwarder output operator\n\n\n[SPOI-5753] - Create splunk forwarder configuration specification\n\n\n[SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-5756] - Test license expiry on sandbox as part of sandbox testing\n\n\n[SPOI-5764] - Gateway to show better error messages on all 500 errors\n\n\n[SPOI-5803] - Support of custom aggregation for ADT\n\n\n[SPOI-5893] - Changes for retrieving container and operator history information\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-4946] - Handle new @useSchema and @description property metadata in UI\n\n\n[SPOI-4980] - Schema Management in the UI\n\n\n[SPOI-5505] - handle Object-to-Object port compatibility with and without schema\n\n\n[SPOI-5506] - handle Object-to-Pojo port compat with schema\n\n\n[SPOI-5513] - handle port compatibility with generic type ports\n\n\n[SPOI-5747] - Automatically generate new eval license when sandbox starts up\n\n\n\n\nVersion 3.0.0\n\n\nSub-task\n\n\n\n\n[SPOI-1901] - Dynamic property changes lost on AM restart \n\n\n[SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package\n\n\n[SPOI-4891] - Example for schema meta data and property doclet tag\n\n\n[SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks\n\n\n[SPOI-4972] - Design Schema API for managing schemas\n\n\n[SPOI-4973] - Create a Schema resource which will contain schema calls\n\n\n[SPOI-4987] - Generate pojo class using schema provided in json\n\n\n[SPOI-4999] - Implement the rest call to save schema on the backend\n\n\n[SPOI-5211] - Support custom visualization link\n\n\n[SPOI-5237] - Type of attributes are wrong when it is an inner class or enum\n\n\n[SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp\n\n\n[SPOI-5361] - Button for dt.phoneHome.enable property on system config page\n\n\n[SPOI-5527] - Implement Queue Option\n\n\n\n\nBug\n\n\n\n\n[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs\n\n\n[SPOI-4633] - Resolve type variable across the type hierarchy \n\n\n[SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times \n\n\n[SPOI-4884] - Intermittent failure for CustomMetricTest\n\n\n[SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.\n\n\n[SPOI-4941] - For kafka operator in app builder certain properties need to be set twice\n\n\n[SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically\n\n\n[SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway\n\n\n[SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency\n\n\n[SPOI-5323] - Malhar operators are not packaged with distribution for 3.0\n\n\n[SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page\n\n\n[SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon\n\n\n[SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes\n\n\n[SPOI-5385] - No error message is returned when DTCli gets an NPE\n\n\n[SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments\n\n\n[SPOI-5433] - Asm code not working for jdk 1.8\n\n\n[SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout\n\n\n[SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle\n\n\n[SPOI-5484] - Ingestion FTP as output does not work with vsftpd\n\n\n[SPOI-5497] - Could not open pi demo in 3.0.0 RC2\n\n\n[SPOI-5498] - Typegraph exception with 3.0.0 RC2\n\n\n[SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404\n\n\n[SPOI-5504] - Make temporary file names unique to avoid lease expiry\n\n\n[SPOI-5530] - Failed to edit pidemo JSON based application\n\n\n[SPOI-5536] - App jar is missing from typegraph\n\n\n[SPOI-5552] - Non-concrete classes being returned with assignableClasses call\n\n\n[SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table\n\n\n[SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException \n\n\n[SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data\n\n\n[SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5583] - DFS root directory check is failing on MapR cluster\n\n\n[SPOI-5593] - App Builder search tooltip\n\n\n[SPOI-5597] - Bad log level WARNING in UI\n\n\n[SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal\n\n\n[SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo\n\n\n[SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators\n\n\n[SPOI-5627] - error message from install script from 3.0.0-RC4 \n\n\n[SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator\n\n\n[SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK \n\n\n[SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group\n\n\n[SPOI-5636] - dtcp is not getting packaged in installation (RC4)\n\n\n[SPOI-5637] - Allatori Configuration errors in ingestion pom.xml\n\n\n[SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out\n\n\n[SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.\n\n\n[SPOI-5644] - isChar validation should strip \\u000 character\n\n\n[SPOI-5646] - Megh demos is failing release build\n\n\n[SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined\n\n\n[SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode\n\n\n[SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user\n\n\n[SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids\n\n\n[SPOI-5661] - Set default store in HDHTReader\n\n\n[SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo\n\n\n[SPOI-5664] - Omit Aggregator Registry From UI\n\n\n[SPOI-5672] - App Data Tracker Compliation is failing\n\n\n[SPOI-5673] - Unable to launch ingestion app with JMS as input source \n\n\n[SPOI-5675] - Kafka keys population error\n\n\n[SPOI-5676] - Fix API doc generation\n\n\n[SPOI-5685] - Complete App Builder category and property name fixes\n\n\n[SPOI-5688] - Correct interactive demo tutorials available in the Learn section\n\n\n[SPOI-5689] - Installer produces error message about removing docs directory\n\n\n[SPOI-5690] - Images missing from lean section tutorials after installing release build\n\n\n[SPOI-5693] - Console does not upload default DT application packages\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5704] - App package references in docs\n\n\n[SPOI-5705] - Build version incorrect\n\n\n[SPOI-5706] - api and user docs not viewable through gateway\n\n\n[SPOI-5707] - Packaging utilities jar in Ingestion\n\n\n[SPOI-5708] - Update netlet dependency to release for 3.0\n\n\n[SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application\n\n\n[SPOI-4889] - Make BucketIdTagger fault tolerant\n\n\n[SPOI-5320] - License URL should open in new window \n\n\n[SPOI-5494] - Support \"queue\" query parameters from launch app modal\n\n\n[SPOI-5535] - Console should validate duplicate app name in the same app package\n\n\n[SPOI-5548] - Check compaction keys from UI\n\n\n[SPOI-5603] - Add syntax highlighting to Markdown code blocks\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n[SPOI-5623] - License upgrade link should open in separate tab\n\n\n[SPOI-5633] - Improve markdown content display styles\n\n\n[SPOI-5657] - Redirect to welcome screen of the Learn section after install\n\n\n[SPOI-5694] - Ingestion default app-config should populate meaningful defaults\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-4672] - Support for secure HA environments\n\n\n[SPOI-5288] - Add \"launch\" button to each item in the list of app packages\n\n\n[SPOI-5304] - Support ingestion install mode\n\n\n[SPOI-5313] - Obfuscate ADT in the distribution build\n\n\n[SPOI-5561] - Markdown documentation support in the Console\n\n\n\n\nTask\n\n\n\n\n[SPOI-4228] - The location of App Data Tracker from installation and from devel mode\n\n\n[SPOI-4658] - App Data Tracker Technical Doc\n\n\n[SPOI-4879] - Hide operators that have no input ports from App builder\n\n\n[SPOI-4983] - Installer for Ingestion app\n\n\n[SPOI-4984] - Obfuscate ingestion jar\n\n\n[SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core\n\n\n[SPOI-5032] - Review comparison for appDataFramework pull request to the master\n\n\n[SPOI-5050] - Licensing-related changes to the UI for 3.0\n\n\n[SPOI-5255] - Support enhancements to uiTypes\n\n\n[SPOI-5314] - Include obfuscated App Data Tracker in the install bundle\n\n\n[SPOI-5370] - Megh obfuscation\n\n\n[SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator\n\n\n[SPOI-5555] - [dtcp] set default app package for ingestion\n\n\n[SPOI-5574] - Update installer README\n\n\n[SPOI-5608] - Docs for backward compatibility in 3.0 \n\n\n[SPOI-5610] - Add testing to Markdown support models, pages, directives\n\n\n[SPOI-5621] - Include Ingestion utilities in DT installer\n\n\n[SPOI-5624] - [Ingestion] Hide message output destination when input is file source\n\n\n[SPOI-5635] - Changing product name to dtIngest\n\n\n[SPOI-5638] - Post ingestion packge to central DT maven repository\n\n\n[SPOI-5639] - App Data Tracker Release Job\n\n\n[SPOI-5645] - support \"short\" primitive type in app builder\n\n\n[SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements\n\n\n\n\nBug\n\n\n\n\n[MLHR-1726] - Bug in PojoUtils\n\n\n[MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives\n\n\n[MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing\n\n\n[MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application\n\n\n[MLHR-1789] - Complete Category and property name fixes\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1725] - Add Support for Setters to PojoUtils\n\n\n[MLHR-1757] - change scope of dt-engine in maven build", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-release-notes", 
            "text": "", 
            "title": "DataTorrent RTS Release Notes"
        }, 
        {
            "location": "/release_notes/#version-381", 
            "text": "Release date: Aug 14, 2017", 
            "title": "Version: 3.8.1"
        }, 
        {
            "location": "/release_notes/#summary", 
            "text": "This minor release primarily addresses issues related to installation of DataTorrent RTS on a Hadoop cluster configured for secure mode.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#rts-bug-fixes", 
            "text": "[SPOI-11694] : Fresh system-wide install fails with \"DFS Installation Location failed to load\" error\n[SPOI-11846] : Gateway fails when launching application in secure mode without authentication enabled\n[SPOI-11848] : Installation wizard fails on the hadoop configuration screen in secure mode\n[SPOI-11849] : The UI calls to retrieve properties in the wizard fail\n[SPOI-12047] : Gateway throws exceptions while getting restarted after security configuration", 
            "title": "RTS Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-bug-fixes", 
            "text": "[APEXCORE-737] : Buffer server may stop processing tuples when backpressure is enabled\n[APEXCORE-745] : AppMaster does not shut down because numRequestedContainers becomes negative", 
            "title": "Apache Apex Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-380", 
            "text": "Release date: Apr 18, 2017", 
            "title": "Version: 3.8.0"
        }, 
        {
            "location": "/release_notes/#summary_1", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#application-templates-apphub", 
            "text": "Pre-built data ingestion templates speed time-to-production  Deploy DataTorrent RTS application templates on a Hadoop distribution either on-premises or on the cloud. As part of this release, DataTorrent is providing AWS - EMR deployment script option for each available application template. As a result, development is simplified, enabling developers to more quickly and easily unlock value for customers. DataTorrent is focused on the goals of reducing complexity and removing dependency on Hadoop deployment, and this release represents progress in that direction.", 
            "title": "Application Templates (AppHub)"
        }, 
        {
            "location": "/release_notes/#application-configurations", 
            "text": "Simplifying customized application launches  Users can start with a single Application Package and create multiple Application Configurations to launch and run the applications on different environments (for instance, in test and development).  And, efficiencies can be realized across business units: one Application Package could have multiple configurations for multiple internal units.  Each Application Configuration introduces a safety feature, which ensures that only one instance of Application Configuration can run at a time.  The status whether Application Configuration is running or not, and controls to launch and stop the application instance are provided in the Application Configuration view.  This set of features improves management, adds safety, and increases transparency when managing and launching applications.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/release_notes/#debugging", 
            "text": "Improving log visualizations to speed up debugging", 
            "title": "Debugging"
        }, 
        {
            "location": "/release_notes/#stram-event-grouping", 
            "text": "The StrAM Events widget helps from development and operations perspectives to visualize notable events from the application launch and throughout its ongoing run.  With this release, StrAM events widgets now offers better readability by organizing these events into related groups.  For example, when multiple downstream operators are re-deployed due to a container failure. All events triggered by the system to restore normal function will be grouped under a single root event which caused the restarts.  With this improved readability, the user can quickly identify failure causes and then drill down into the logs for each event.", 
            "title": "StrAM Event Grouping"
        }, 
        {
            "location": "/release_notes/#garbage-collection-widgets", 
            "text": "With this release, developers can more easily visualize garbage collection data trends\u2014as opposed to sifting through logs. Three widgets are available for GC visualizations:   Garbage collection log chart by heap.  Visualizes when memory is allocated and deallocated   Garbage collection log table.  List memory allocation and deallocation details  Garbage collection log chart by duration.  Visualizes how long it takes to deallocate memory", 
            "title": "Garbage Collection Widgets"
        }, 
        {
            "location": "/release_notes/#log-tailing-search", 
            "text": "Users can follow the logs as they are generated (tailing) with the RTS UI Console.  In this release, users can now also perform searches even when tailing to focus on specific events and exclude the noise.", 
            "title": "Log Tailing &amp; Search"
        }, 
        {
            "location": "/release_notes/#alert-templates", 
            "text": "Simplifying and expanding alert functionality  In 3.7.0, RTS introduced monitoring with alerts that a DevOps engineer could set up based on specific conditions. The 3.8.0 release continues to simplify and expand this functionality by adding:   Nine predefined system alert templates, including cluster and application memory usage, application status, and active container count, killed container alerts, etc.  Option to disable alerts without deleting them.  The ability to configure custom SMTP settings for sending alert emails, instead of relying on Gateway\u2019s local node sendmail facility.", 
            "title": "Alert Templates"
        }, 
        {
            "location": "/release_notes/#security", 
            "text": "Security enhancement in 3.8.0 applies to RTS deployment on secure Hadoop with Kerberos enabled. User's own Kerberos credentials can now be used directly by RTS to launch applications.  It is better from a security perspective.  The previous model involved using a single system user with Kerberos credential (Hadoop impersonation)  to launch applications. That requires access to the system Kerberos credential in order to refresh tokens before they expire.  With user\u2019s own Kerberos credential, that is no longer the case.", 
            "title": "Security"
        }, 
        {
            "location": "/release_notes/#licensing", 
            "text": "With the release of 3.8.0, DataTorrent is updating and simplifying its licensing policy.  What has changed?    Starting with 3.8, the Community Edition is no longer available. We are replacing the Community Edition with Free Edition.  Community Edition limited the features available to you, such as security.   Now with Free Edition you have access to all the features and tools of RTS up to a 128GB processing limit.  Please refer to the DataTorrent website for additional details.", 
            "title": "Licensing"
        }, 
        {
            "location": "/release_notes/#licensing-faq", 
            "text": "I have a Community Edition license. Is that edition still available? \nStarting with 3.8, the Community Edition is no longer available.   You can continue to use the Community Edition with RTS version 3.7.  How is license memory consumption calculated? \nLicense memory consumption is the sum of all running applications as can be seen in Configuration - License Information and Monitor.  What will happen when my memory consumption exceed my license limit? \nYou\u2019ll receive a warning, which is shown for 30 minutes before most RTS features will be disabled. All existing applications will continue to run. Should you need to upgrade, you can easily contact DataTorrent for a new license.  How will I know when my license is going to expire and what happens if the license expires? \nWe provide warnings at 30 days and 7 days before expiration date.  When a license expires, most RTS features will be immediately disabled. All existing applications will continue to run. You can easily contact DataTorrent for a new license.    What can I do once RTS is locked (either because my license memory has been exceeded or its expiration date has passed)? \nUsers can view running applications and enter new license details to unlock RTS. The following capabilities are still accessible:  Configure - System Configuration is available except for App Data Tracker  Configure - License Information is available  Configure - Installation Wizard is available  Monitor - Application kill, inspect and shutdown are available  Monitor - Application Overview (shutdown and kill only) per application is available  Monitor - Containers (kill only)  AppHub (download only, no import)  Learn  Can I reduce the number of applications and return to compliance? \nYes, you can do so if you have exceeded your memory capacity, assuming that your license has not expired  Can I reduce the memory usage of an application and bring it back to license compliance? \nYes, assuming that your license has not expired.  Please note that you will have to restart the application.  I have an enterprise license for my production cluster. Can I use the Free Edition in a non- production cluster? \nYes. It\u2019s worth noting that only community support is available for the Free Edition. Please visit the DataTorrent User group for RTS-related questions: https://groups.google.com/forum/#!aboutgroup/dt-users  For the Apache Apex mailing list and meetups information, please go to\nhttps://apex.apache.org/community.html#mailing-lists  Can I buy DataTorrent support for Free Edition? \nUnfortunately, no. DataTorrent support is sold as part of our Enterprise Edition. If you\u2019re seeking support, you may consider upgrading.  Is Application Master Container memory consumption included in the calculation for processing capacity? \nYes. Application Master Container consumes 1 GB by default and every application has its own Application Master. If application is not running, it does not run as well. It grows based on customer application build. Memory requirements increases along with the size of logical and physical DAG. Partitioners of an operator run in AppMaster.", 
            "title": "Licensing FAQ"
        }, 
        {
            "location": "/release_notes/#additional-features-of-380", 
            "text": "", 
            "title": "Additional Features of 3.8.0"
        }, 
        {
            "location": "/release_notes/#multiple-gateway-support", 
            "text": "This allows simultaneous multiple gateway operations and increase fault tolerance due to management console failure.   When there are multiple gateways (usually for High Availability), different developers may access them at the same time, with different or same user accounts. These activities will often result in simultaneous modification of the same resource stored in HDFS, and invalidate cache entries on each client. For example: When developer A tries to save a configuration package and developer B has edited and saved the same package, developer A will get an error. Developer A would then have to manually merge the differences. This release introduces a new file-based locking mechanism with HTTP ETag header to handle that scenario.\nKnown limitation: Alerts and visualization works correctly with single gateway only.", 
            "title": "Multiple gateway support"
        }, 
        {
            "location": "/release_notes/#retain-metric-selections-when-returning-to-monitor-physicallogical-view", 
            "text": "Better user experience since RTS will keep what users have selected to view (e.g. metrics) as they go from one screen to another.", 
            "title": "Retain metric selections when returning to Monitor - Physical/Logical view"
        }, 
        {
            "location": "/release_notes/#datatorrent-apex-core-fork", 
            "text": "RTS 3.8.0 is bundled with Apache Apex Core 3.5.0 plus forty feature and fixes that will be part of Apache Apex Core 3.6.0. Apex Core commits from Apr 3, 2017 will be included in RTS 3.9.0", 
            "title": "DataTorrent Apex Core fork"
        }, 
        {
            "location": "/release_notes/#new-feature", 
            "text": "[APEXCORE-579]        Custom control tuple support  [APEXCORE-563]        Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvements", 
            "text": "[APEXCORE-676]        Show description for DefaultProperties only when user requests it  [APEXCORE-655]        Support RELEASE as archetype version when creating a project  [APEXCORE-611]        StrAM Event Log Levels  [APEXCORE-605]        Suppress bootstrap compiler warning  [APEXCORE-592]        Returning description field in defaultProperties during apex cli call get-app-package-info  [APEXCORE-572]        Remove dependency on hadoop-common test.jar  [APEXCORE-570]        Prevent upstream operators from getting too far ahead when downstream operators are slow  [APEXCORE-522]        Promote singleton usage pattern for String2String, Long2String and other StringCodecs  [APEXCORE-426]        Support work preserving AM recovery  [APEXCORE-294]        Graceful application shutdown  [APEXCORE-143]        Graceful shutdown of test applications", 
            "title": "Improvements"
        }, 
        {
            "location": "/release_notes/#task", 
            "text": "[APEXCORE-662]        Raise StramEvent for heartbeat miss", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#dependency-upgrade", 
            "text": "[APEXCORE-656]        Upgrade org.apache.httpcomponents.httpclient", 
            "title": "Dependency Upgrade"
        }, 
        {
            "location": "/release_notes/#bug", 
            "text": "[APEXCORE-674]        DTConfiguration utility class ValueEntry access level was changed  [APEXCORE-663]        Application restart not working.  [APEXCORE-648]        Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()  [APEXCORE-645]        StramLocalCluster does not wait for master thread termination  [APEXCORE-644]        get-app-package-operators with parent option does not work  [APEXCORE-636]        Ability to refresh tokens using user's own Kerberos credentials in a managed environment where the application is launched using an admin with impersonation  [APEXCORE-634]        Apex Platform unable to set unifier attributes for modules in DAG  [APEXCORE-627]        Unit test AtMostOnceTest intermittently fails  [APEXCORE-624]        Shutdown does not work because of incorrect logic in the AppMaster  [APEXCORE-617]        InputNodeTest intermittently fails with ConcurrentModificationException  [APEXCORE-616]        Application fails to start Kerberised cluster  [APEXCORE-610]        Avoid multiple getBytes() calls in Tuple.writeString  [APEXCORE-608]        Streaming Containers use stale RPC proxy after connection is closed  [APEXCORE-598]        Embedded mode execution does not use APPLICATION_PATH for checkpointing  [APEXCORE-597]        BufferServer needs to shut down all created execution services  [APEXCORE-596]        Committed method on operators not called when stream locality is THREAD_LOCAL  [APEXCORE-595]        Master incorrectly updates committedWindowId when all partitions are terminated.  [APEXCORE-593]        apex cli get-app-package-info could not retrieve properties defined in properties.xml  [APEXCORE-591]        SubscribeRequestTuple has wrong buffer size when mask is zero  [APEXCORE-585]        Latency should be calculated only after the first window has been complete  [APEXCORE-583]        Buffer Server LogicalNode should not be reused by Subscribers  [APEXCORE-558]        Do not use yellow color to display command strings in help output  [APEXCORE-504]        Possible race condition in StreamingContainerAgent.getStreamCodec()  [APEXCORE-471]        Requests for container allocation are not re-submitted", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-bug-fixes", 
            "text": "[SPOI-10021]  DTX Logical Operator page - BufferServerReadBytesPSMA and BufferServerWriteBytesPSMA to be removed  [SPOI-10107]  Application service returns DAG which is null  [SPOI-10118]  Upon launch of application, application details do not show up.  [SPOI-10153]  Add System Properties \"change\" button should be warm in color  [SPOI-10208]  Container state for failed attempt of app is shown as RUNNING  [SPOI-10266]  \"host\" information is not available in appattempts API call  [SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens  [SPOI-10331]  Delete user modal gives error when clicked outside the frame  [SPOI-10332]  Cancelling delete user action throws TypeError in developer console  [SPOI-10335]  Jersey throwing exceptions   excessive logging when WADL is enabled  [SPOI-10355]  Update Buttons and Text  [SPOI-10357]  Redirect User to Login Page  [SPOI-10363]  CheckPermission should not throw exception when auth is not enabled.  [SPOI-10416]  dtAssemble does not show connection between operators correctly  [SPOI-10421]  Fix oauth login  [SPOI-10438]  Hide Top Nav Menu Dropdown When Item is Clicked  [SPOI-10463]  Enhance Date/Time Picker for StrAM Events Date Range  [SPOI-10587]  Implement Date/Time Picker for Dashboard Widget  [SPOI-10588]  Change Button Label to Close During Launching  [SPOI-10862]  Multiple containers are labelled as AppMaster after dynamic partitioning  [SPOI-10866]  Time Range Selection Saved Settings Not Loaded Correctly  [SPOI-10894]  dtAssemble - Inspector contents not showing up consistently  [SPOI-10911]  determine AppMaster container by id that contains _000001 instead of by including 0 operators  [SPOI-10963]  appInstance page - fail to show \"packagedDashboard\" which is included in appPackage that appInstance is launched from  [SPOI-10970]  AppHub on gateway does not load in HTTPS  [SPOI-10996]  Subscribers/DataListeners may not be scheduled to execute even when they have data to process  [SPOI-10997]  BufferServer needs to shut down all created execution services  [SPOI-11000]  Upgrade org.apache.httpcomponents.httpclient  [SPOI-11024]  Alerts Icon Issue  [SPOI-11057]  Restart of the apps are failing  [SPOI-11108]  DAG View Javascript Error  [SPOI-11127]  Enhance \"lastNbytes\" to behave like \"tail\" command  [SPOI-11142]  Unable to launch app if another app with default APPLICATION_NAME is already running  [SPOI-11152]  Avoid usage of Apache Apex engine core class com.datatorrent.stram.client.DTConfiguration.ValueEntry  [SPOI-11163]  Cannot launch application from application details page  [SPOI-11164]  \"Add default properties\" option under \"Specify launch properties\" is missing  [SPOI-11168]  License API Does Not Return Latest State Information  [SPOI-11179]  Update Root Cause Failure to Use Newer Object Structure  [SPOI-11185]  Invalid license expiration message sent by Gateway  [SPOI-11186]  AppHub should be visible if license is invalid  [SPOI-11188]  App Packages search does not work for \"format\" column  [SPOI-11190]  Toggling \"system apps\" option does not show ended system apps  [SPOI-11192]  Search for \"lifetime\" column on Monitor screen does not work  [SPOI-11193]  Search for \"memory\" column on Monitor screen does not work  [SPOI-11194]  Search on \"state\" column on Monitor page is not alphabetical  [SPOI-11197]  Search on locality/source/sinks columns under Streams widget on logical tab does not work  [SPOI-11198]  Search for \"allocated mem\" and \"free memory\" under Containers widget does not work  [SPOI-11199]  \"download file\" option for empty container log files should be disabled  [SPOI-11200]  Search for \"allocated mem\" and \"started time\" under Containers widget  for app attempt does not work  [SPOI-11208]  DTgateway install screen messed up  [SPOI-11210]  On entering corrupt license file error message should be a proper one  [SPOI-11212]  Trailing and non trailing search should have same string  [SPOI-11213]  Unable to save SMTP configuration using gmail  [SPOI-11214]  Launching application with \u00ef\u00bf\u00bc\"Enable Garbage Collection\" throws 404  [SPOI-11215]  ADMIN_NOT_CONFIGURED warning is only shown to Dev user instead of admin  [SPOI-11220]  Fresh RTS installation fails because of blank response from \"/ws/v2/config\" api call  [SPOI-11222]  StackTrace feature not available from physical tab containers widget  [SPOI-11223]  Show \"Password change\" warning for dtadmin only  [SPOI-11231]  AppHub fails to load previous package versions  [SPOI-11234]  Show all AppHub package versions option is missing in list view  [SPOI-11236]  Extend application-level gc.log API to take in new parameter \"descendingOrder\" (false/true)  [SPOI-11237]  Angular not resolving certain dtText-wrapped expressions in Console modals  [SPOI-11238]  AppHub Check for Updates Does Not Show In All Cases  [SPOI-11242]  \"Upload package\" option on Application Packages should not be available with invalid/no license  [SPOI-11265]  Invalid message displayed when no license is uploaded  [SPOI-11266]  Alert notification is delayed  [SPOI-11270]  System Alert history is empty after relogin  [SPOI-11271]  Developer user cannot create system alert  [SPOI-11281]  .class file generated by tuple schema manager is invalid  [SPOI-11291]  Creating clone of JSON application gives 500 Server Error  [SPOI-11303]  Creating clone of JSON application gives 500 Server Error (UI)  [SPOI-11304]  App package load errors after migrating from 3.7 to 3.8  [SPOI-11307]  Search for \"lifetime\" column on Monitor screen does not work  [SPOI-11318]  Copy To Clipboard option from Failure Message modal does not work  [SPOI-11323]  Upgrade from 3.7.0 evaluation edition to 3.8.0 retains old license  [SPOI-11325]  Selecting KILLED applications and then disabling ended apps, shows shutdown/kill options  [SPOI-11326]  Clean install of 3.8.0 comes with no license  [SPOI-11327]  StramEvents are grouped incorrectly  [SPOI-11366]  Copy to clipboard not working when viewing stram event stack trace  [SPOI-11367]  Wrong 'uptime' value in Application Overview  [SPOI-11368]  Log and info icons should be right aligned in stram events  [SPOI-11369]  Socket is unsubscribed by Console when critical path is not checked  [SPOI-11371]  \"source package\" column in Application Configurations table is not sortable  [SPOI-11372]  Unable to view gc.log on UI  [SPOI-11375]  Wrong stream locality is shown in Physical DAG widget  [SPOI-11377]  Incorrect GC stats for logical operators if they are connected by CONTAINER_LOCAL stream  [SPOI-11379]  Heap reduction percentage is negative for some GC events  [SPOI-11382]  UI hangs when trying to change settings for GC Log Chart widgets  [SPOI-11383]  Selecting a container in GC Log Chart widgets throws error in Developer console  [SPOI-11417]  Memory usage of App Data Tracker is not counted against license  [SPOI-11418]  Event grouping: UI should ignore groupId 0   null  [SPOI-11421]  Unable to use Security Configuration feature with free license  [SPOI-11422]  Admin user should not be able to delete its own user account  [SPOI-11424]  Show tooltip for version string in application overview widget  [SPOI-7887]   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}] should return error instead of success where there is error \"Failed to load\"  [SPOI-8248]   Packaged dashboards do not reconnect with new app instances  [SPOI-8477]   Upgrade License Opens in dtManage window, it should be in opened up in new window  [SPOI-8610]   Disable editing operator properties which are of Object type  [SPOI-9375]   Uptime values shown on UI are out of whack immediately after app is launched  [SPOI-9474]   Failed to restart DT application in MapR secure cluster  [SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors  [SPOI-9945]   Top navigation menu is wrapping into two lines", 
            "title": "DataTorrent RTS Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-371", 
            "text": "Release date: Feb 28, 2017", 
            "title": "Version: 3.7.1"
        }, 
        {
            "location": "/release_notes/#summary_2", 
            "text": "This is primarily for users who install RTS in a Kerberized cluster as application fails to launch in 3.7.0. This release fixes the issue.   Other issues that are also fixed:   In 3.6.0, users have the option to override certain properties from the config file. This ability was missing in 3.7.0.  Gateway fails to recognize AppDataTracker application and it continuously relaunched on a Kerberized cluster.  On a Kerberized cluster, when the configuration parameters are specified in the  dt-site.xml  file in the user's home directory, the installation wizard does not allow the user to continue with the installation.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#bug-fixes", 
            "text": "[SPOI-10698] - Allow Custom Properties with Config XML file while launching an application  [SPOI-10737] - AppDataTracker application relaunches continuously and fails in a Kerberized cluster.  [SPOI-10932] - Installation Wizard does not allow to complete gateway configuration  [SPOI-10773] - Application fails to run in fully enabled Kerberized mode (APEXCORE-616 - https://issues.apache.org/jira/browse/APEXCORE-616)", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-370", 
            "text": "Release date: Dec 30, 2016", 
            "title": "Version: 3.7.0"
        }, 
        {
            "location": "/release_notes/#summary_3", 
            "text": "The new features on this release are functionalities that will ease debugging an application and administering application alerts in production.   Operation related features for a Dev Ops role:     Manage and see a history of previous alerts so that users can be aware of potential issues before they become critical.   View operator ID(s) and name(s) in the dtManage-Physical-Container list table to quickly identify what operators are in each container.   Filter matching tuple recordings by searching across tuple recording data.   Debugging features: When trying to troubleshoot or debug a distributed system, these capabilities allow users to quickly identify problem areas and easily drill down into relevant details (i.e. logs).    Notify users when log files have been removed  New Application Attempts section under Monitor. It is located along other views such as logical and physical  New Application Master logs in Application Overview section  New log button shortcut in Stram Events and Physical Operator section  Show dead container logs in Running applications. That is to show history of physical operator containers and logs in the physical operator view. For each previous incarnation, there are start time, end time, link to corresponding logs, root cause with error code, and recovery window id.  Show the same details for killed or finished application like running application view.  Show container history as default in Physical Operator view  New historic count field in Physical Operator view  Get thread dump from a container. It is useful to analyze issues such as \"stuck operator\", and obtain statistics from the running JVM. In production environments users often don't have direct access to the machines, thus making it available through the REST API will help.   Option to auto tail container logs. When viewing a container log via the UI, there is an option to periodically poll for more data (i.e. \"tail -f\" effect).   dtAssemble   New validate button so that user can validate DAG without having to save.   Remove auto save function. Save will be initiated by user only.  Support custom JSON input for tuple schema creation. This is particularly useful when user needs to add a large number of fields.    dtDashboard   New gauge widget   AppHub   Continued to refine application templates in AppHub (introduced in RTS 3.6.0).     RTS 3.7.0 is based on Apache Apex Core 3.5.0 (released Dec 19, 2016) and Apache Apex Malhar 3.6.0 (released Dec 8, 2016).", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#apache-apex-core-350", 
            "text": "This release upgrades the Apache Hadoop YARN dependency from 2.2 to 2.6. The community determined that current users run on versions equal or higher than 2.6 and Apex can now take advantage of more recent capabilities of YARN. The release contains a number of important bug fixes and operability improvements. \nChange log: https://github.com/apache/apex-core/blob/v3.5.0/CHANGELOG.md", 
            "title": "Apache Apex Core 3.5.0"
        }, 
        {
            "location": "/release_notes/#apache-apex-malhar-360", 
            "text": "The release adds first iteration of SQL support via Apache Calcite. Features include SELECT, INSERT, INNER JOIN with non-empty equi join condition, WHERE clause, SCALAR functions that are implemented in Calcite, custom scalar functions. Endpoint can be file, Kafka or internal streaming port for both input and output. CSV format is implemented for both input and output. See examples for usage of the new API.  The windowed state management has been improved (WindowedOperator). There is now an option to use spillable data structures for the state storage. This enables the operator to store large states and perform efficient checkpointing.  There was also benchmarking on WindowedOperator with the spillable data structures. From the result, the community significantly improved how objects are serialized and reduced garbage collection considerably in the Managed State layer. Work is still in progress for purging state that is not needed any more and further improving the performance of Managed State that the spillable data structures depend on. More information about the windowing support can be found at http://apex.apache.org/docs/malhar/operators/windowedOperator/.  This release also adds a new, alternative Cassandra output operator (non-transactional, upsert based) and support for fixed length file format to the enrichment operator. \nChange log: https://github.com/apache/apex-malhar/blob/v3.6.0/CHANGELOG.md", 
            "title": "Apache Apex Malhar 3.6.0"
        }, 
        {
            "location": "/release_notes/#appendix_1", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues", 
            "text": "[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors  [SPOI-9923]   Custom panes' on operator monitoring page should have character limits for titles  [SPOI-9924]   Can not escape the \"custom\" pane name  [SPOI-9925]   Limit number of custom panes  [SPOI-9947]   JDBC Poll Input operator processes extra records when its container is killed  [SPOI-9948]   JDBC Poll Input operator does not process new records when they are inserted while the app is processing the existing records  [SPOI-9965]   Restarting the KILLED application with JDBC Poll Input operator plays the duplicate data  [SPOI-10046]  Deleting a property directly creates tuple schema with remaining properties  [SPOI-10049]  Limit the number of characters in role name   [SPOI-10107]  Application service returns dag which is null  [SPOI-10151]  Add System Properties modal should validate the properties  [SPOI-10153]  Add System Properties \"change\" button should be warm in color  [SPOI-10154]  Rerun Install wizard allows extension of trial  [SPOI-10158]  Logical/Physical plan view does not retain metric selections in drop-down  [SPOI-10165]  Container logs, dt.log files produced by chklogs.py have HTML escapes  [SPOI-10202]  About API call gives out information without authentication.  [SPOI-10203]  User can set any non existent package name   [SPOI-10208]  Container state for failed attempt of app is shown as RUNNING  [SPOI-10267]  If number of alerts are huge, gateway starts slowing down  [SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens  [SPOI-10275]  Alert contains an exception trace  [SPOI-10292]  Delay in cluster metrics when the authentication is enabled  [SPOI-10315]  \"requires Apex version\" information is missing for latest app packages on AppHub  [SPOI-10332]  Canceling delete user action throws TypeError in developer console  [SPOI-10333]  Configuration issue modal content goes out of modal and is not scrollable  [SPOI-10334]  App restart doesn't take to new page  [SPOI-10373]  FinishedTime/EndTime information is available only after application if killed/shutdown for CDH  [SPOI-10379]  Enable Reporting button should have cool colors  [SPOI-10385]  The app name field should not be editable at launch time  [SPOI-10389]  UI Console should not allow creation of config pkg with special characters  [SPOI-10396]  Application configuration should require save before launch  [SPOI-10398]  UI says \"An error occurred while fetching data\" immediately after launching apps (intermittent)  [SPOI-10399]  New permission do not reflect unless user logs out  [SPOI-10401]  Deleted user can do any operations  [SPOI-10406]  Application Configurations upload modal title should say \"Application Configuration Upload\"  [SPOI-10409]  Updating app packages using \"check for updates\" option from AppHub gives wrong notification  [SPOI-10410]  Deleting a property while creating tuple schema gives error in developer console", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#new-feature_1", 
            "text": "[SPOI-7720]   DT Hub shows just one version of an application  [SPOI-10182]  dtAssemble - if there is unsaved change, keep launch button enabled which will pop up a dialog box asking save-and-launch when being clicked.  [SPOI-8879]   Support custom JSON input for tuple schema creation  [SPOI-9877]   Alerts History  [SPOI-9876]   Alerts Notification  [SPOI-9875]   Alerts Management  [SPOI-8570]   Ability to filter tuple recording  [SPOI-9525]   UI for dtDebug - Logs  [SPOI-8499]   Create diagnostic tool for analyzing RM and container logs  [SPOI-10364]  Update launch and configuration package views for simplified properties  [SPOI-9772]   App Launch properties  [SPOI-8764]   UI Console Configuration Packages support  [SPOI-8611]   Support gateway configuration changes in UI  [SPOI-10323]  Always show User profile even when license type is not enterprise", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement", 
            "text": "[SPOI-9785]   send GA events instead of page views for apphub page events  [SPOI-10290]  Certification tool - RTS Installer Support for tool  [SPOI-10179]  dtAssemble should warn about the unsaved changes when navigating away  [SPOI-8989]   Show operator names under Physical -  Containers  [SPOI-10163]  dtDebug utilities README should have list of requirements    [SPOI-9881]   appAttempt page - Containers table - \"containerLogsUrl\" column - change it from showing a hyperlink to \"logs\" button.  [SPOI-7343]   Ability to obtain a thread dump from a container  [SPOI-3553]   Option to auto-tail container logs  [SPOI-9133]   Gateway restart modal and button has soothing colors  [SPOI-9132]   Gateway restart UI button has soothing colors  [SPOI-9105]   Refactor security validation in console (consolidate resolve:{} from many places into one place)  [SPOI-9047]   Make Package Upload Message clickable  [SPOI-8766]   Make Physical DAG has the same metric selection (Top dropdown, Bottom dropdown) like Logical DAG does.  [SPOI-8645]   Logical DAG, Physical DAG - show spinner in panel instead of blank panel before graph has been rendered and displayed.  [SPOI-8644]   Do not show graph options(Show/Hide Stream Locality, Reset Position, Top dropdown, Bottom dropdown) until graph has been rendered and displayed.  [SPOI-7810]   dtManage: Number of failures for an operator should have 'number search' option instead of 'string search'  [SPOI-7277]   Ability to upload configuration file during app launch ( like -conf in dtcli )  [SPOI-9418]   ConfigPackages backend support  [SPOI-9400]   Change licensing for RTS to be managed/displayed in GB instead of MB  [SPOI-9086]   Add support for DIGEST enabled Hadoop web services environment  [SPOI-7963]   Show container stack trace in dtManage  [SPOI-10230]  containerLogsUrl shown in appattempt table can be pretty-printed", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#task_1", 
            "text": "[SPOI-9291]   Calcite  [SPOI-8027]   TBD: Apex Java high level API - Aggregation Part 1  [SPOI-9769]   Tiles for apps on AppHub  [SPOI-7965]   Productize certification tool to size RTS  [SPOI-6350]   Gauge widget  [SPOI-7433]   Update all relevant docs with AppHub  [SPOI-9693]   Add Validate button in dtAssemble  [SPOI-9688]   Remove autosave from dtAssemble  [SPOI-9971]   Verify that alerts can only be sent by mail  [SPOI-9364]   dtDebug Logs backend feature  [SPOI-9695]   Launch application not using config package name  [SPOI-9413]   Permission changes for tenancy   [SPOI-7966]   Provide user ability to configure security through dtManage UI (only password option)  [SPOI-8736]   dtManage should alert user when there's a potential Hadoop config issue  [SPOI-9575]   Create demo app  [SPOI-9021]   State management benchmark  [SPOI-8933]   Change Megh repository to ASL    [SPOI-8865]   Operator Maturity Framework - Cassandra Output  [SPOI-8788]   Operator Maturity Framework - Enhancement of FS Output Operator  [SPOI-9966]   App-templates misc   [SPOI-9774]   Update Database to HDFS app template   [SPOI-9234]   AppHub - App Pipeline creation with continuous iteration  [SPOI-10063]  Create new apex core build based on master  [SPOI-9859]   Log retrieval tool  [SPOI-9043]   Requirements discussion on Batch Support - Definition of Batch, Scheduling of Batch DAG, State of Batch, Replay of Batch and Monitoring of Batch    [SPOI-8990]   DAG Editor - unable to drag a connection stream from a port if having restriction DISABLE_APP_EDIT_STRUCTURE  [SPOI-9972]   Document DT Gateway System Alerts  [SPOI-9970]   Modify access to allow Admin (and ONLY admin) to set alerts  [SPOI-9653]   Plan and implement (1 item in v1)for Gateway Alerts  [SPOI-10261]  Show friendly message when user logs files have been removed.  [SPOI-9163]   Refactor configPackages to configurations  [SPOI-8223]   Troubleshooting improvements in dtManage  [SPOI-9382]   QA - Operator Maturity Framework - AbstractFileInputOperator  [SPOI-8885]   Operator Testing (Operator Maturity Framework)  [SPOI-9483]   Superuser role and oAuth cleanups  [SPOI-8996]   Add authentication configuration web services spc to gateway REST api doc", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug-fixes_1", 
            "text": "[SPOI-5852]   App Package page has a single word \"ago\" for modification time after importing pi demo  [SPOI-6651]   Launching App from UI ignores APPLICATION_NAME attribute defined in properties.xml file  [SPOI-7062]   dtHub UI - tags column - filter - searching from the beginning of a tag.  [SPOI-7652]   AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings  [SPOI-8039]   UI says \"An error occurred fetching data.\" after launching the application  [SPOI-8349]   User should not be able to delete the default apps in ingestion-solution package  [SPOI-8379]   Property editor for array of enum is not rendered correctly  [SPOI-8489]   Application_Name attribute from the config file is not honored.  [SPOI-8507]   Unable to launch an AppDataTracker application imported from dtHub  [SPOI-8516]   DataTorrent rpm version inconsistency  [SPOI-8522]   Unable to set roles while creating user in secure environment  [SPOI-8523]   Users can kill the app even if privileges get revoked in secure environment   [SPOI-8531]   Multiple MachineData demos are available at dtHub  [SPOI-8534]   README.html for sandbox contains references to 'malhar-users'  [SPOI-8536]   DT RTS gateway log floods with WARN message  [SPOI-8542]   Installation: User home directory is not created by default  [SPOI-8566]   Tuple Recording Modal Fixes  [SPOI-8622]   Exception in retrieving app state in certification when application has not yet reached running state  [SPOI-8630]   \"merge\" configurations option for appPackages also creates new applications  [SPOI-8717]   Updating sandbox generates errors  [SPOI-8781]   Buffer server metrics not available for physical operator in Metrics Chart  [SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available  [SPOI-8888]   Unable to see imported/uploaded/running applications on DT UI in SSL enable envornment   [SPOI-8995]   Running through the unit tests in dtx creates a residual file  [SPOI-9007]   Kryo Exception while re-deploying the DimensionsComputationFlexibleSingleSchemaPOJO operator  [SPOI-9127]   Wrong notification provided by dtConsole when package upload is failed  [SPOI-9134]   Gateway restart modal has incorrect focus  [SPOI-9135]   Gateway restart modal should be horizontally and vertically aligned  [SPOI-9140]   dtConsole shows \"Failed to parse\" error when 'Monitor' tab is refreshed  [SPOI-9152]   Application package link is not working  [SPOI-9153]   Hyperlink not required on AppPackage tab  [SPOI-9161]   Recordings rest API gives wrong number of totalTuples  [SPOI-9199]   Error running application due to YARN API exception  [SPOI-9200]   dt-site.xml has a misguiding warning  [SPOI-9245]    Set logging level  cannot delete the set logs  [SPOI-9431]   Disable tenant option in TenancyFilter  [SPOI-9496]   Use the AppPackageOwner field instead of logged in user, while working with configPackages.  [SPOI-9503]   AbstractFileInputOperator does not honor filePatternRegexp parameter  [SPOI-9580]   APEXMALHAR-2314 Improper functioning in partitioning of sequentialFileRead property of FSRecordReader  [SPOI-9658]   Apps are not filtered correctly using tags column on AppHub   [SPOI-9660]   AppHub navigation tab is still seen as dtHub  [SPOI-9696]   Prevent Navigation in DAG Diagram When Dragging Image Around  [SPOI-9738]   DAG Diagram Doesn't Display Sometimes  [SPOI-9783]   Operators stay in PENDING_DEPLOY  [SPOI-9787]   Configuration package spelling error  [SPOI-9790]   Recording Tuple Dialog Display Bug  [SPOI-9791]   Fix log line format  [SPOI-9793]   Can not start gateway after installation  [SPOI-9794]   Can not start dtcli after installation   [SPOI-9908]   Container StackTrace is not functioning  [SPOI-9914]   Container buttons should be contextual based on state  [SPOI-9916]   Kafka Input Operator (0.9) validation app is missing from QA/test-apps repository  [SPOI-9933]   Unable to run JDBC Poll app on latest SNAPSHOT build (3.7.0)  [SPOI-9934]   Notification History links, when clicked modal doesn't get closed  [SPOI-9939]   Links to log files should not be restricted to enterprise edition  [SPOI-9941]   Gateway password security errors  [SPOI-9974]   Unable to upload packages to the gateway  [SPOI-9977]   Unable to launch applications using apex CLI, gives ClassNotFoundException  [SPOI-10002]  Config Package Page Not Showing Saved Properties  [SPOI-10016]  Config package upload fails  [SPOI-10018]  Unnecessary check boxes present for applications under Develop tab  [SPOI-10020]  Tuple recording feature is not working  [SPOI-10036]  Last modified time for imported packages is always shown with additional 2 minutes.  [SPOI-10043]  User should not be allowed to save tupleSchema with blank values  [SPOI-10044]  Editing existing tuple schema gives TypeError  [SPOI-10046]  Deleting a property while creating tuple schema directly creates final schema with remaining properties  [SPOI-10047]  Tuple schema created using JSON input does not take latest JSON as input  [SPOI-10050]  Can not record samples  [SPOI-10051]  Delete roles modal should have warm colors  [SPOI-10052]  Restore roles modal should have warm colors  [SPOI-10054]  Launch application for configuration package not using local settings for application naming  [SPOI-10056]  Malhar-angular-table temporary fix for application packages and app properties lists  [SPOI-10065]  User is allowed to \"Add System Property\" with blank value  [SPOI-10068]  dtGateway script doesn't return correct status when gateway is down  [SPOI-10080]  Issues with containers table from UI console  [SPOI-10110]  AppPackage get info should not be used to show the configPackage apps  [SPOI-10143]  StramEvents API gives 500 error  [SPOI-10147]  \"cluster/metrics\" API gives 500 error  [SPOI-10148]  Add system properties has weird titles  [SPOI-10150]  Change system properties modal button should not be in cool colors  [SPOI-10152]  Disable appdatatracker modal buttons should be in warm color  [SPOI-10155]  AppDataTracker can not be enabled  [SPOI-10156]  Clicking on \"ended apps\" and \"system apps\" multiple times shows multiple shadows of \"system apps\"  [SPOI-10157]  Inspect port UI hangs  [SPOI-10159]  Can not upload application packages  [SPOI-10181]  killed applications - (1) \"AM Logs\" dropdown is empty. (2) AppMaster container does not have purple label in id column.   [SPOI-10195]  Selected schema doesn't show the fields in the schema  [SPOI-10200]  Button for \"Delete logging level\" is misaligned  [SPOI-10209]  Link is missing for \"originalTrackingUrl\" field on currently running app attempt  [SPOI-10228]  Sorting by  host  in the Physical Plan -  Containers tab is not working  [SPOI-10229]  \"attempts\" tab for dtDebug is available in community edition  [SPOI-10233]  Application attempts API does not return startedTime and finishedTime for FAILED attempts  [SPOI-10235]  Kill Application Master container modal should have warm colors  [SPOI-10236]  Kill selected container title has unwanted text  [SPOI-10242]  Configuration Packages missing  [SPOI-10257]  An error is shown for a while while launching Application Configurations  [SPOI-10258]  Application package launch modal shows wrong \"Use configuration file\" option instead of \"Use configuration package\"   [SPOI-10259]  Security configuration page on console has illegible content  [SPOI-10260]  Links in Alert configuration modal are broken  [SPOI-10277]  Stacktrace is not fully shown in alerts detail  [SPOI-10279]  Update app hub description.  [SPOI-10300]  ValidateApplication   PutApplication should have CheckViewPermission instead of checkModifyPermission  [SPOI-10343]  UI errors while displaying containers in Physical tab  [SPOI-10350]  Jackson jars are missing from the RTS after Hadoop upgrade to 2.6 causing API failures  [SPOI-10356]  Update Buttons Labels  [SPOI-10358]  Schemas in ConfigPackages  [SPOI-10359]  Schema in ConfigPackage Permission on 2 APIs should be reduced to view  [SPOI-10361]  Upload of configPackage failed  [SPOI-10381]  Unable to create configPackage from java application  [SPOI-10387]  Unable to launch application configurations from \"Application Configuration details\" page  [SPOI-10390]  Use Configuration Package Should be Enabled", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-360", 
            "text": "Release date: Nov 9, 2016", 
            "title": "Version: 3.6.0"
        }, 
        {
            "location": "/release_notes/#summary_4", 
            "text": "DataTorrent RTS releases AppHub, a repository of application templates for various Big Data use cases. The key of this release is that RTS now have an infrastructure to distribute application templates easily. Developers can reduce the time to develop Big Data applications using templates. There are five templates in this release with many more to come.    HDFS Sync   Amazon S3 to HDFS Sync  Kafka to HDFS Sync  HDFS to HDFS Line Copy  HDFS to Kafka Sync", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#appendix_2", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#improvement_1", 
            "text": "[SPOI-9136] - Enforce DefaultOutputPort.emit() or Sink.put() thread affinity", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#task_2", 
            "text": "[SPOI-9118] - Publish App Templates for Ingestion on AppHub  [SPOI-9419] - Update AppHub API to include the markdown content  [SPOI-9432] - Update AppHub back end to extract markdown from apa", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task", 
            "text": "[SPOI-3277] - Show app master logs on UI for applications that fail at launch when we upgrade to Hadoop 2.4 or above  [SPOI-9079] - Creation of example application for transform operator  [SPOI-9235] - Allow users to create new configurations from Application Configurations view  [SPOI-9240] - Create individual package view for AppHub artifacts  [SPOI-9464] - Rename all references of AppHub on console UI to AppHub  [SPOI-9574] - Rename title on AppHub list page  [SPOI-9612] - Upgrade AppHub server deployment", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug-fixes_2", 
            "text": "[SPOI-9140] - dtManage shows \"Failed to parse\" error when 'Monitor' tab is refreshed  [SPOI-9522] - DELETE call to /ws/v2/config/properties/{name} returns 500  [SPOI-9727] - DTINSTALL_SOURCE incorrectly assumes file name", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-350", 
            "text": "Release date: Sep 26, 2016", 
            "title": "Version 3.5.0"
        }, 
        {
            "location": "/release_notes/#summary_5", 
            "text": "DataTorrent RTS continues to deliver features that sets it apart in bringing operability in running an enterprise grade big data-in-motion platform. This particular release brought new features such as   Allowing users to analyze \"stuck operator\" by obtaining stats from the running JVM (i.e. GC stats and thread dump)  Ability to show/hide critical path in both logical and physical DAG", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#apache-apex-malhar", 
            "text": "The other important part of going to production is a library of operators that is more than just functional. They need to be fault tolerant, partitionable, support idempotency, and dynamically scalable. The recent release of Apache Apex Malhar 3.5.0 provides new and updated operators and APIs to bring those enterprise features.    Windowed Operator that supports the windowing semantics outlined by Apache Beam and Google Cloud DataFlow, including the concepts of event time windows, session windows, watermarks, allowed lateness, and triggering.  High level Java stream API now uses the aforementioned Windowed Operator to support stateful transformation with Apache Beam style windowing semantics.  Introduction of Spillable Data Structures that make use of Managed State.  Deduper Operator to process  whether a given record is a duplicate or not  Enricher Operator to join a stream with a lookup source and operate on any POJO object  HBase input operator. Improve HBasePOJOInputOperator with support for threaded read  File Record reader module. It is useful for reading from files \"line by line\" in parallel and emit each line as seperate * tuple.  JDBC Poll Input Operator   For the full release note, please go to\nhttps://blogs.apache.org/apex/entry/apache_apex_malhar_3_5", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/release_notes/#appendix_3", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_1", 
            "text": "[SPOI-9232] Dedup with manage state operator marking all impression as duplicate  [SPOI-9203]   \"check for updates\" option says 'no updated versions' and then displays updated packages  [SPOI-9183]   Nested operator properties should follow order specified in the ORB on dtAssemble UI  [SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#improvement_2", 
            "text": "[SPOI-7343] - Ability to obtain a thread dump from a container  [SPOI-8191] - Operator properties should follow order specified in the ORB on dtAssemble UI  [SPOI-8352] - Warning message while restarting app should be changed  [SPOI-8450] - Dedup ports connected to console should not write to log  [SPOI-8722] - Logical DAG, Physical DAG - change \"Show/Hide String Locality\", \"Show/Hide Critical Path\" to checkbox.", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#story", 
            "text": "[SPOI-6794] - HBase Input Operator  [SPOI-6795] - Creation of Concrete Cassandra Output  [SPOI-6932] - Module to read from HDFS Input record by record and emit it downstream  [SPOI-7948] - JDBC Input Operator", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#bug-fixes_3", 
            "text": "[SPOI-7836] - Trend widget fails after dashboard save/reload with PiDemo app  [SPOI-7886] - Duplicate ports show up in the UI for POJO operators  [SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same  [SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500  [SPOI-8721] - Column labeled buffer service size is showing bytes per second  [SPOI-9084] - Refresh tokens failing in some scenarios with a login failure message  [SPOI-9130] - dtConsole shows no applications on monitor tab  [SPOI-9131] - gateway REST and WebSocket APIs for cluster metrics fail to report stats", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#task_3", 
            "text": "[SPOI-8411] - Deduper operator using Managed State  [SPOI-9004] - Deduper Documentation", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_1", 
            "text": "[SPOI-8389] - ORB defaults for FileSystem related operators  [SPOI-8390] - Added operator default values in Application.json for user visibility  [SPOI-8410] - Kafka Input Operator Unit test failed  [SPOI-8461] - AbstractKafkaInputOperator problems", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-340", 
            "text": "", 
            "title": "Version 3.4.0"
        }, 
        {
            "location": "/release_notes/#summary_6", 
            "text": "Affinity rules provides a way to specify hints on how operators should be deployed in a Hadoop cluster. There are two types of rules: affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together. Anti-affinity rule, the new feature in Apache Apex 3.4.0, indicates that the group of operators should be allocated separately.  This release also includes a lot of bug fixes. Please see appendix for full list.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#dtmanage", 
            "text": "User can restart a killed application from dtManage  \"Retrieve Ended Apps\" button changes to \"Hide Ended Apps\" after dtManage retrieves killed apps.  User can use mouse scroll to zoom in/out of physical DAG view", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#apache-apex-340", 
            "text": "Blacklist problem nodes from future container requests  Support adding module to application using property file API.  Ability to obtain a thread dump from a container  RPC timeout is now configurable  When an operator is blocked, it nows print out a warning instead of debug", 
            "title": "Apache Apex 3.4.0"
        }, 
        {
            "location": "/release_notes/#appendix_4", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_2", 
            "text": "[SPOI-8518] - Support links from Dashboard to Application instance pages  [SPOI-8516] - Datatorrent rpm version inconsistency  [SPOI-8470] - Check for already existing app package should be done before uploading the whole package  [SPOI-8436] - Documentation is needed on \"How to use Transform operators in ingestion solution app package\"  [SPOI-8434] - Documentation is needed on \"How to use Generic JDBC/PostgreSQL operators in ingestion solution app package\"  [SPOI-8433] - Documentation is needed on \"How to use Enrichment operator in ingestion solution app package\"  [SPOI-8414] - Drop down is not shown for \"Fields to copy\" property of \"POJO Enricher\" unless output schema is not specified  [SPOI-8352] - Warning message while restarting app should be changed  [SPOI-8296] - \"File Path\" and \"Output File Name\" properties for HDFS File Output Operator should be clubbed together  [SPOI-8295] - \"Include Fields\" parameter for \"POJO Enricher\" has misleading description  [SPOI-8294] - Operator Class Name for \"Delimited Parser\" operator should not be \"CSV Parser\"  [SPOI-8293] - File permission property is not working properly for HDFS File Output Operator  [SPOI-8291] - Parameters on dtAssemble should be logically ordered  [SPOI-8224] - Providing Field Infos value for JDBC POJO Input Operator is too complex  [SPOI-8192] - Clicking on edit option for apps throws validation errors in activity panel  [SPOI-8158] - Options to modify property values should be closer to the property name on dtAssemble canvas  [SPOI-8144] - \"Tuple Schemas\" link at top right corner of dtAssemble canvas should open in new tab  [SPOI-8143] - Tuple Schema page not available directly from Develop tab  [SPOI-8133] - When stream is added in dtAssemble, user should be notified if schema is required  [SPOI-8131] - Couple of parameters for Kafka Input Operator should have dropdown selection in dtAssemble  [SPOI-8130] - Documentation for ingestion beta operators need to be improved  [SPOI-8128] - Port names for operators are not intuitive when displayed on dtAssemble canvas  [SPOI-8087] - JDBC input operator query should not require explicit ordering of column names  [SPOI-8038] - Output file names for HDFS output should not contain timestamp and '.tmp' extension  [SPOI-8522] - Unable to set role while creating user in a secure environment.  There is a workaround by create user with no role and then assign the role  [SPOI-8523] - User can kill app even though privileges got revoked.  This applies to secure environment only.    [SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500  [SPOI-8536] - DT RTS gateway log floods with WARN message  [SPOI-8535] - Need to restart dtgateway for enabling password authentication in sandbox  [SPOI-8534] - README.html for sandbox contains references to 'malhar-users'  [SPOI-8533] - Importing 'Apache Apex Malhar Iteration Demo' throws error for 'property' tag in properties.xml  [SPOI-8532] - Importing packages from dtHub sometimes gives ZipException  [SPOI-8531] - Multiple MachineData demos are available at dtHub  [SPOI-8524] - Clicking on \"generate new dashboard\" first navigates to 'Learn' tab on the dtConsole  [SPOI-8523] - Users can kill the app even if privileges get revoked in secure environment  [SPOI-8522] - Unable to set roles while creating user in secure environment  [SPOI-8519] - Ingestion application on dtHub still shows 'requires Apex version' with \"-incubating\"  [SPOI-8511] - Gateway Websocket API leaks information while unauthorized  [SPOI-8509] - dtAssemble operator documentation shows '@link' markers  [SPOI-8507] - Unable to launch an AppDataTracker application imported from dtHub  [SPOI-8491] - UI mixes the order and ids of tuple recording ports  [SPOI-8490] - gateway issues in SSL enabled cluster  [SPOI-8479] - Uninstall does not work even though the install was successful previously  [SPOI-8477] - Upgrade License Opens in dtManage window, it should be in opened up in new window  [SPOI-8476] - Hadoop-common-tests library shouldn't be part of RTS build  [SPOI-8474] - On addition of huge role name, non-specific errors are shown  [SPOI-8473] - Gateway, console allows impractially longer user roles additions  [SPOI-8472] - Visually ugly error presentation  [SPOI-8469] - If Kerberos tickets are changed, you have to refresh whole UI  [SPOI-8467] - installation script provides incorrect information  [SPOI-8432] - JDBC input operator is failing with exception \"fetching metadata\"  [SPOI-8424] - Dedup does not honor the expiryPeriod when error tuple is introduced in between two valid tuples  [SPOI-8422] - Time properties for operators should have units mentioned for them  [SPOI-8416] - dtAssemble Can't change application name  [SPOI-8365] - HDFS sync app : Unable to sync 500 GB file  [SPOI-8358] - Uptime and latency values are very high exactly after app is launched  [SPOI-8317] - dtIngest 1.1.0 (Compiled against 3.2.0) can not be launched  [SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same  [SPOI-8213] - Application DAG is not displayed when clicked on app link  [SPOI-8202] - Unable to add custom properties while launching apps  [SPOI-8197] - Default values for \"Field Infos\" and \"Bucket Manager\" properties should be set appropriately  [SPOI-7986] - Gateway proxy feature is not working  [SPOI-7934] - Kafka-dedup-HDFS-solution: Ahead in time messages are lost from Dedup operator", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#bug-fixes_4", 
            "text": "[SPOI-7939] - Dynamic repartition causes application to hang  [SPOI-8468] - Can't assign roles for users in secure environment  [SPOI-8464] - \"Disable Reporting\" option on System Configuration page gives NullPointerException  [SPOI-8024] - Gateway is leaving behind dtcheck temp files in HDFS  [SPOI-7640] - Changing dashboard name in dashboard settings modal and then canceling does not revert dashboard name  [SPOI-8077] - Gateway logs NPE if an app master is misbehaving  [SPOI-8046] - Kerberos Cluster: Installation Wizard can not get past beyond Hadoop Configuration screen  [SPOI-8055] - Console references to dt-text-tooltip no longer produce a tool tip  [SPOI-7898] - Default JAAS support classes duplicated in dt-library  [SPOI-7929] - Update log4j.properties in the DTX/dist/install to set debug level for org.apache.apex  [SPOI-7943] - The demo applications fails due to numberOfBuckets is less than 1  [SPOI-8092] - Problems in launching jobs with authentication enabled on secure cluster  [SPOI-7794] - Monitor page not refreshing properly  [SPOI-7889] - Cannot read property 'hideBreadcrumbs' of undefined  [SPOI-7856] - Modify application packages dtHub import/update paths  [SPOI-7907] - Downloads of AppPackages result in corrupted files during local testing  [SPOI-7971] - After dynamic repartition application appears blocked", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-340_1", 
            "text": "", 
            "title": "Apache Apex 3.4.0"
        }, 
        {
            "location": "/release_notes/#new-feature_2", 
            "text": "[APEXCORE-10] - Enable non-affinity of operators per node (not containers)  [APEXCORE-359] - Add clean-app-directories command to CLI to clean up data of terminated apps  [APEXCORE-411] - Restart app without specifying app package", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_3", 
            "text": "[APEXCORE-92] - Blacklist problem nodes from future container requests  [APEXCORE-107] - Support adding module to application using property file API.  [APEXCORE-304] - Ability to add jars to classpath in populateDAG  [APEXCORE-328] - CLI tests should not rely on default maven repository or mvn being on the PATH  [APEXCORE-330] - Ability to obtain a thread dump from a container  [APEXCORE-358] - Make RPC timeout configurable  [APEXCORE-380] - Idle time sleep time should increase from 0 to a configurable max value  [APEXCORE-383] - Time to sleep while reservoirs are full should increase from 0 to a configurable max value  [APEXCORE-384] - For smaller InlineStream port queue size use ArrayBlockingQueueReservoir as default  [APEXCORE-399] - Need better debug information in stram web service filter initializer  [APEXCORE-400] - Create documentation for security  [APEXCORE-401] - Create a separate artifact for checkstyle and other common configurations  [APEXCORE-407] - Adaptive SPIN_MILLIS for input operators  [APEXCORE-409] - Document json application format  [APEXCORE-419] - When operator is blocked, print out a warning instead of debug  [APEXCORE-447] - Document: update AutoMetrics with AppDataTracker link", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_1", 
            "text": "[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block  [APEXCORE-201] - Reported latency is wrong when a downstream operator is behind more than 1000 windows  [APEXCORE-326] - Iteration causes problems when there are multiple streams between two operators  [APEXCORE-335] - StramLocalCluster should teardown StreaminContainerManager after run is complete  [APEXCORE-349] - Application/operator/port attributes should be returned using string codec in REST service  [APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers  [APEXCORE-352] - Temp directories/files not created in temp directory specified by system property java.io.tmpdir  [APEXCORE-353] - Buffer server may stop processing data  [APEXCORE-355] - CLI list-*-attributes command name change  [APEXCORE-362] - NPE in StreamingContainerManager  [APEXCORE-363] - NPE in StreamingContainerManager  [APEXCORE-374] - Block with positive reference count is found during buffer server purge  [APEXCORE-375] - Container killed because of Out of Sequence tuple error.  [APEXCORE-376] - CLI command 'dump-properties-file' does not work when connected to an app  [APEXCORE-385] - Temp directories/files not always cleaned up when launching applications  [APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily  [APEXCORE-393] - Reset failure count when consecutive failed node is removed from blacklist  [APEXCORE-397] - Allow configurability of stram web services authentication  [APEXCORE-398] - Ack may not be delivered from buffer server to it's client  [APEXCORE-403] - DelayOperator unit test fails intermittently  [APEXCORE-413] - Collision between Sink.getCount() and SweepableReservoir.getCount()  [APEXCORE-415] - Input Operator double checkpoint  [APEXCORE-421] - Double Checkpointing May Happen In Input Node On Shutdown  [APEXCORE-422] - Checkstyle rule related to allowSamelineParameterizedAnnotation suppression  [APEXCORE-434] - ClassCastException when making webservice calls to STRAM in secure mode  [APEXCORE-436] - Update log4j.properties in archetype test resources to set debug level for org.apache.apex  [APEXCORE-439] - After dynamic repartition application appears blocked  [APEXCORE-444] - 401 authentication errors when making webservice calls to STRAM in secure mode  [APEXCORE-445] - Race condition in AsynFSStorageAgent.save()", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_4", 
            "text": "[APEXCORE-293] - Add core and malhar documentation to project web site  [APEXCORE-319] - Document backward compatibility guidelines  [APEXCORE-340] - Rename dtcli script to apex  [APEXCORE-345] - Upgrade to 0.7.0 japicmp  [APEXCORE-381] - Upgrade async-http-client dependency version because of security issue  [APEXCORE-410] - Upgrade to netlet 1.2.1  [APEXCORE-423] - Fix style violations in Apex Core  [APEXCORE-446] - Add source jar in the shaded-ning build", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_2", 
            "text": "[APEXCORE-254] - Introduce Abstract and Forwarding Reservoir classes  [APEXCORE-269] - Provide concrete implementation of AbstractReservoir based on SpscArrayQueue  [APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size  [APEXCORE-369] - Fix timeout in AbstractReservoirTest.performanceTest  [APEXCORE-402] - SpscArrayQueue to notify publishing thread on not full condition", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-330", 
            "text": "", 
            "title": "Version 3.3.0"
        }, 
        {
            "location": "/release_notes/#summary_7", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#dthub", 
            "text": "DataTorrent RTS allows companies to quickly build low latency real time Big Data application that can scale and fault tolerant. With the introduction of dtHub, DataTorrent now host and maintain an application distributor. You can access it through dtManage and easily install or update application without having to upgrade the whole RTS platform. Prior to dtHub, RTS installer packaged both platform and applications. In 3.3, they are decoupled with significantly reduced installer size. You can independently choose when to upgrade the platform and when to install/upgrade applications.", 
            "title": "dtHub"
        }, 
        {
            "location": "/release_notes/#dtmanage_1", 
            "text": "Troubleshooting is made easier now that user can view the containers where physical operators have lived  RTS Community Edition user can now view container logs and set log levels via dtManage. API for runtime DAG and property change are also available  RTS Enterprise Edition is changed from 30 days to 60 days  When an application is killed, RTS will delete the app directory according to policy  in dt-site.xml", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#dtdasbhoard", 
            "text": "New widgets for the dashboard: Geo coordinates with circles, Geo regions with gradient fill and single Value", 
            "title": "dtDasbhoard"
        }, 
        {
            "location": "/release_notes/#dtassemble-beta", 
            "text": "Top level \u201cDevelop\u201d takes users directly to Application Page  Navigation changes on how user get to Tuple Schema. User goes to It is now \"Edit Application\" then \"Tuple Schema\"  Development (breadcrumb link)", 
            "title": "dtAssemble (beta)"
        }, 
        {
            "location": "/release_notes/#documentation", 
            "text": "FileSplitter: http://docs.datatorrent.com/operators/file_splitter/  Block Reader: http://docs.datatorrent.com/operators/block_reader/", 
            "title": "Documentation"
        }, 
        {
            "location": "/release_notes/#apache-apex-33", 
            "text": "Support for iterative processing. It is a building block to support machine learning  Ability to populate DAG at application launch time  Pre checkpoint operator callback so that it can execute a logic before the operator gets checkpointed (e.g. flush file to HDFS)  Provide the option for operator to do checkpointing in a distribute in-memory store. It is faster than HDFS due to disk i/o latency  Add group ID information in an applicatin package.  It is visible for application grouping in dtHub.", 
            "title": "Apache Apex 3.3"
        }, 
        {
            "location": "/release_notes/#known-issues-in-33", 
            "text": "[SPOI-7696] - Community edition (which gets activated after expired license) does not have newly added community features  [SPOI-7471] - Temp directories/files not cleaned up in Gateway  [SPOI-7697] - \"Set Logging Levels\" option on application details page does not show initial target/loglevel fields  [SPOI-7668] - If AppDataTracker is disabled, changing the YARN queue does not restart it  [SPOI-7652] - AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings  [SPOI-7678] - On configuration complete, summary page should populate RTS version  [SPOI-7479] - dtHub \"check for updates\" option says 'no updated versions' and then displays updated packages  [SPOI-7626] - Stacked Area Chart widget shows NaN values on Firefox", 
            "title": "Known Issues in 3.3"
        }, 
        {
            "location": "/release_notes/#appendix_5", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#dthub_1", 
            "text": "[SPOI-6787] - dtHub UI - explore, import, download  [SPOI-7023] - dtHub UI - check for updates  [SPOI-3643] - Remove import default packages from Application Packages page  [SPOI-7451] - Add options summary to Application Packages screen  [SPOI-6968] - Please make API return a new property that indicates whether gateway has internet access or not (for dtHub feature)  [SPOI-7059] - add \"tags\" to import list", 
            "title": "dtHub"
        }, 
        {
            "location": "/release_notes/#dtmanage_2", 
            "text": "[SPOI-3549] - dtManage - Ability to view container history where physical operator has lived  [SPOI-7382] - UI evaluation license expiration message colour update  [SPOI-7418] - Visualize AppDataTracker data  [SPOI-7129] - Add countdown and links to enterprise evaluation in dtManage  [SPOI-7226] - Remove choice of community edition and enterprise evaluation in install wizard", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#dtdashboard", 
            "text": "[SPOI-6522] - Geo coordinates with weighted circles widget  [SPOI-6523] - Geo regions with gradient fill widget  [SPOI-7247] - Create dimensional single value widget  [SPOI-7258] - Persist label changes in trend and single value widgets  [SPOI-6023] - dtDashboard - Widgets that support dimension schema can support multi-value keys   [SPOI-6021] - Support tag for  snapshot server and dimension store  [SPOI-6331] - Notify user when widget is unable to automatically load data  [SPOI-6658] - UI says \"no rows testing\" when no data available to display in tables  [SPOI-6887] - Changing choropleth map class does not remove previously selected map  [SPOI-7246] - Change default widget colors to be websafe  [SPOI-7257] - add tags-based dimension query settings in trend widget", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/release_notes/#dtassemble-beta_1", 
            "text": "[SPOI-7133] - Tuple Schemas change and Develop on top navigation bar change", 
            "title": "dtAssemble (beta)"
        }, 
        {
            "location": "/release_notes/#megh", 
            "text": "[SPOI-7665] - Megh enhancements for TelecomDemo  [SPOI-7230] - Add group id information to all megh app packages  [SPOI-7251] - Add directory structure for modules in megh  [SPOI-7693] - Add Telecom Demo To Megh", 
            "title": "Megh"
        }, 
        {
            "location": "/release_notes/#docs", 
            "text": "[SPOI-6471] - Documentation: FileSplitter  [SPOI-6472] - Documentation: BlockReader", 
            "title": "Docs"
        }, 
        {
            "location": "/release_notes/#rts-community-edition", 
            "text": "[SPOI-7670] - Removing dtManage restrictions from community edition  [SPOI-7682] - Lock down all auth/security features in community edition  [SPOI-7130] - Community edition to have an upgrade button to Enterprise eval  [SPOI-7243] - make all locked-feature places(e.g. Visualize link, logs button, new application button) to have the same effect of \"upgrade to enterprise                \" link in community edition, and to have a key icon instead of being crossing out", 
            "title": "RTS Community Edition"
        }, 
        {
            "location": "/release_notes/#rts-enterprise-edition", 
            "text": "[SPOI-7127] - Change enterprise evaluation days from 30 days to 60 days", 
            "title": "RTS Enterprise Edition"
        }, 
        {
            "location": "/release_notes/#bug-fixes_5", 
            "text": "[SPOI-5622] - dtcli: Command 'show-physical-plan' fails with \"Failed web service request\" error  [SPOI-6352] - Gateway's RM Proxy REST calls don't work with HA-enabled  [SPOI-6424] - Gateway cannot determine its own local address to the cluster when RM HA is enabled  [SPOI-6555] - Add Missing datatorrent.apppackage.classpath property to dt-demos  [SPOI-6624] - ADT issues impacting dtingest Dashboard  [SPOI-6674] - AbstractFileOuptutOperator refactoring and fixes  [SPOI-6834] - fixing scope of jars in Megh  [SPOI-6837] - Gateway has a lot of blocked threads under heavy load  [SPOI-6886] - Selected map feature / object should be persisted  [SPOI-6949] - Collation in BucketManager incurs a performance hit while writing to HDFS and is not an optimization  [SPOI-6999] - Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7026] - only lists the ones that have different versions in update section  [SPOI-7027] - fix a bug that loading is forever when there is no updates in check for updates  [SPOI-7028] - compare version number and only list packages whose dtHub version is higher than installed version in update section  [SPOI-7037] - support sorting, filter in string-type columns in import pkgs, update pkgs  [SPOI-7040] - optimize visual layout of import pkgs page  [SPOI-7042] - only shows packages that are compatible with the APEX version in check for update list  [SPOI-7080] - remove bar chart widget that is not in use  [SPOI-7106] - Update Megh japi version and fix the broken build because of semantic version  [SPOI-7138] - Dimension unifier return empty results   [SPOI-7236] - Console build fails due to jsHint issues after updating version  [SPOI-7293] - Post installation links no longer available on datatorrent.com  [SPOI-7296] - gateway spills out error trying to write to /var/log/datatorrent/ in local install  [SPOI-7297] - Local install fails in secure mode  [SPOI-7309] - HDHT Broken and Hangs After Wall And Purge Changes  [SPOI-7338] - After changing configuration \"dt.appDataTracker.queue\" (e.g. to \"root.ashwin\") and kill system application AppDataTracker, AppDataTracker                 incorrectly starts with the default queue (\"root.dtadmin\").  [SPOI-7350] - Installation wizard final step refers to invalid developers URL  [SPOI-7361] - get-operator-attributes fails on CDH  [SPOI-7383] - Update invalid links in UI console info section  [SPOI-7447] - dtHub UI - check for updates - when there is no newer version for any installed package, loading image will be hanging forever  [SPOI-7460] - On Visualize tab, link for documentation does not work  [SPOI-7469] - Remove \"in HDHT\" from System Configuration page  [SPOI-7471] - Temp directories/files not cleaned up in Gateway  [SPOI-7474] - \"Disable App Data Tracker\" option does not work  [SPOI-7481] - Launch macros for demo apps should be removed in dtcli  [SPOI-7482] - Cloning/deleting application under Application package does not refresh the list  [SPOI-7498] - Verify dtingest download link change on datatorrent website  [SPOI-7502] - Lock mishandling in App Package local cache code  [SPOI-7503] - Changing \"dt.appDataTracker.enable=true/false\" using REST API calls doesn't take effect unless gateway restarts  [SPOI-7510] - Application launch notifications no longer show up  [SPOI-7523] - Kill only AppDataTracker whose user is the current login user when App Data Tracker queue is changed.  [SPOI-7542] - Unable to import app package from dtHub  [SPOI-7574] - dtcli command 'dump-properties-file' does not work when connected to an app  [SPOI-7577] - For 3.3.0 release, RTS version is shown as 3.3.1 in System Configuration  [SPOI-7604] - HDHT bucket meta class is obfuscated incorrectly  [SPOI-7615] - dtHub intermittently throws error as \"Failed to import\" while importing multiple pkgs at the same time   [SPOI-7619] - specify bower link malhar-angular-dashboard to version 1.0.1  [SPOI-7644] - Need to update installer script  [SPOI-7679] - Unable to access newly added features for Community edition.", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-33-change-logs", 
            "text": "https://github.com/apache/incubator-apex-core/blob/v3.3.0-incubating/CHANGELOG.md   [SPOI-7061] - Implement retention policy for terminated apps  [SPOI-7492] - DT_GATEWAY_CLIENT_OPTS overrides all JVM options and there is no way to supply additional options to the default options  [SPOI-5735] - Create local file cache for app package  [SPOI-6981] - Move orderedOutput feature to AbstractDeduper. Rename AbstractDeduperOptimized to AbstractBloomFilterDeduper  [SPOI-7448] - Work around the attribute bug detailed in APEXCORE-349 so that ADT still works  [SPOI-7470] - Work around namenode NPE bug in hadoop 2.7.x to avoid throwing NPE to the user. https://issues.apache.org/jira/browse/APEXCORE-45 and https://issues.apache.org/jira/browse/HDFS-9851  [SPOI-6381] - Support Dynamically Updating Enum Values For Keys In The Dimensions Store  [SPOI-6545] - Allow Gateway to directly contact dtHub to install app packages", 
            "title": "Apache Apex 3.3 Change Logs"
        }, 
        {
            "location": "/release_notes/#new-feature_3", 
            "text": "[APEXCORE-3] - Ability for an operator to populate DAG at launch time  [APEXCORE-60] - Iterative processing support  [APEXCORE-78] - Pre-Checkpoint Operator Callback  [APEXCORE-276] - Make App Data Push transport pluggable and configurable  [APEXCORE-283] - Operator checkpointing in distributed in-memory store  [APEXCORE-288] - Add group id information to apex app package", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_4", 
            "text": "[APEXCORE-40] - Semver dependencies should be in Maven Central  [APEXCORE-162] - Enhance StramTestSupport.TestMeta API  [APEXCORE-181] - Expose methods in StramWSFilterInitializer to get the RM webapp address  [APEXCORE-188] - Make type graph lazy load  [APEXCORE-199] - CLI should check for version compatibility when launching app package  [APEXCORE-228] - Add maven 3.0.5 as prerequisites to the Apex parent pom  [APEXCORE-229] - Upgrade checkstyle maven plugin (2.17) and checkstyle dependency (6.11.2)  [APEXCORE-291] - Provide a way for an operator to specify its metric aggregator instance  [APEXCORE-305] - Enable checkstyle violations logging to console during maven build", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_2", 
            "text": "[APEXCORE-58] - endWindow is being called even when the operator is being undeployed  [APEXCORE-83] - beginWindow not called on recovery  [APEXCORE-193] - apex-app-archetype has extraneous entry that generates a warning when running it  [APEXCORE-204] - Update checkstyle and codestyle to be the same  [APEXCORE-211] - Brace placement after static blocks in checkstyle configuration  [APEXCORE-263] - Checkpoint can be performed twice for same window  [APEXCORE-274] - removeTerminatedPartition fails for Unifier operator  [APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection  [APEXCORE-278] - GenericNodeTest clutters test logs with unnecessary statement  [APEXCORE-296] - Memory leak in operator stats processing  [APEXCORE-300] - Fix checkstyle regular expression  [APEXCORE-303] - Launch properties not evaluated", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_5", 
            "text": "[APEXCORE-24] - Takes out usage of Rhino as it is GPL 2.0  [APEXCORE-186] - Enable license check in Travis CI  [APEXCORE-253] - Apex archetype includes dependencies which do not belong to org.apache.apex  [APEXCORE-298] - Reduce the severity of  line length check  [APEXCORE-301] - Add \"io\" as a separate import to checkstyle rules  [APEXCORE-302] - Update NOTICE copyright year  [APEXCORE-308] - Implement findbugs plugin reporting  [APEXCORE-317] - Run performance benchmark for the Apex Core 3.3.0 release", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_3", 
            "text": "[APEXCORE-104] - Expand Module DAG  [APEXCORE-105] - Support injecting properties through xml file on modules.  [APEXCORE-144] - Provide REST api for listing information about module.  [APEXCORE-151] - Provide code style templates for major IDEs (Eclipse, IntelliJ and NetBeans)  [APEXCORE-182] - Add Apache copyright to IntelliJ  [APEXCORE-194] - Add support for ProxyPorts in Modules  [APEXCORE-226] - Strictly enforce wrapping indentation in checkstyle  [APEXCORE-227] - Enforce left brace placement for anonymous class on the next line  [APEXCORE-230] - Limit line lengths to be 120  [APEXCORE-239] - Upgrade checkstyle to 6.12 from 6.11.2  [APEXCORE-248] - Increase wrapping indentation from 2 to 4.  [APEXCORE-249] - Enforce class, method, constructor annotations on a separate line  [APEXCORE-250] - Exclude DtCli from System.out checks  [APEXCORE-267] - Fix existing checkstyle violations in api  [APEXCORE-270] - Enforce checkstyle validations on test classes  [APEXCORE-272] - Attributes added to operator inside Module is not preserved.  [APEXCORE-273] - Fix existing checkstyle violations in bufferserver module  [APEXCORE-306] - Recovery checkpoint handing in iteration loops", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-321", 
            "text": "", 
            "title": "Version 3.2.1"
        }, 
        {
            "location": "/release_notes/#summary_8", 
            "text": "This release bundles a few fixes and features for customers who are on 3.2.0 and not ready to upgrade to 3.3.0", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#improvement_5", 
            "text": "[SPOI-7900] add default role to first time login user  [SPOI-7061] Implement retention policy for terminated apps", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug-fixes_6", 
            "text": "[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7471] Temp directories/files not cleaned up in Gateway  [SPOI-7971] After dynamic repartition application appears blocked", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#known-issues_3", 
            "text": "[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7061] Implement retention policy for terminated apps  [SPOI-7471] Temp directories/files not cleaned up in Gateway  [SPOI-7971]After dynamic repartition application appears blocked", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#apache-apex", 
            "text": "[APEXCORE-327] - Implement proper semantic version checks in patch release branches  [APEXCORE-358] - Make RPC timeout configurable  [APEXCORE-410] - Upgrade to Netlet 1.2.1  [APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/release_notes/#apache-apex-bug-fixes_1", 
            "text": "[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block  [APEXCORE-274] - removeTerminatedPartition fails for unifier operator  [APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection  [APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers  [APEXCORE-353] - Buffer server may stop processing data  [APEXCORE-362] - NPE in StreamingContainerManager  [APEXCORE-363] - NPE in StreamingContainerManager  [APEXCORE-374] - Block with positive reference count is found during buffer server purge  [APEXCORE-375] - Container killed because of Out of Sequence tuple error.  [APEXCORE-385] - Temp directories/files not always cleaned up when launching applications  [APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily  [APEXCORE-398] - Ack may not be delivered from buffer server to it's client", 
            "title": "Apache Apex Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-320", 
            "text": "", 
            "title": "Version 3.2.0"
        }, 
        {
            "location": "/release_notes/#new-feature_4", 
            "text": "[SPOI-6351] - Add feature to REST API to get queue information from cluster", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_6", 
            "text": "[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest  [SPOI-5828] - Stackstraces should not be shown on errors  [SPOI-6641] - Implement \"forever\" bucket in DimensionComputation  [DTIN-40] - Observed unused variables in SplunkBytesInputOperator  [DTIN-69] - Move Query Operators implementation to newer one  [DTIN-50] - [dtIngest] add parameter \"parallel readers\"", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_3", 
            "text": "[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory  [SPOI-5216] - FileSplitter fails with ConcurrentModificationException  [SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'  [SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up  [SPOI-5823] - Downstream container falling behind when buffer spooling is enabled  [SPOI-5899] - Ability to retrieve schemas from an app package  [SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation  [SPOI-6098] - 'single run' doesnt work  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6236] - Add 1s aggregations to App Data Tracker  [SPOI-6298] - Dimensions Store Can Become Blocked  [SPOI-6383] - Sometimes expired query is still executed.  [SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console  [SPOI-6446] - Merge PojoEnrichment and TupleEnrichment  [SPOI-6505] - Exceptions from asm when uploading apps  [SPOI-6603] - Warning messages shown on console are not completely visible  [SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks  [SPOI-6846] - DT dashboard guide link is not working  [SPOI-6855] - Broken links observed on summary page while DT RTS configuration   [SPOI-6856] - Correct docs index.html links and title  [DTIN-51] - bandwidth option was removed during merge  [DTIN-101] - For message-based-input to message-based-output, compression   encryption options should not be visible on config UI  [DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum  [DTIN-112] - Message based input to FTP output fails with IOException while appending the data  [DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata  [DTIN-123] - All files are not getting copied when bandwidth option is specified  [DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning  [DTIN-126] - 'Compact files' option should be disabled for message based Input type  [DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file  [DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only  [DTIN-155] - Messages are dropped when bandwidth option is enabled  [DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError  [DTIN-161] - Message based input to S3N output fails with IOException while appending the data  [DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka  [DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled  [DTIN-165] - Data source names for dtingest should not contain 'null'  [DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream  [DTIN-176] - Parallel read is not working in case of FTP and S3 as input  [DTIN-186] - FileMerger failed with unable to merge file exception", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_6", 
            "text": "[SPOI-5173] - DAG validation: Attribute values Serializable  [SPOI-5403] - Ingestion Splunk integration  [SPOI-5801] - Make a MapR partner (datatorrent) sandbox  [SPOI-5958] - dtView Integration for ingestion metric visualization  [SPOI-6241] - Change in enterprise license  [SPOI-6439] - Run benchmark for 3.2.0 release  [SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies  [DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script  [DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator  [DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch  [DTIN-54] - Disabling the splunk from dtIngest script  [DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_4", 
            "text": "[SPOI-5369] - Integrate schema support in Cassandra Input/Output  [SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output  [SPOI-5819] - Bandwidth control for file based sources  [SPOI-5820] - Bandwidth control for message based sources  [SPOI-5959] - Metrics for compression, encryption  [SPOI-6337] - Expose bandwidth metrics  [SPOI-6441] - Change console home page to be welcome screen instead of operations summary", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-311", 
            "text": "", 
            "title": "Version 3.1.1"
        }, 
        {
            "location": "/release_notes/#improvement_7", 
            "text": "[SPOI-5828] - Stackstraces should not be shown on errors", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_4", 
            "text": "[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock  [SPOI-6032] - App Builder should not show property from super class  [SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6090] - Ingestion: Error decrypting files for message based sources  [SPOI-6147] - Launch issue with \"Starter Application Pack\"  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6203] - -ve Memory reported for Application Master  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager  [SPOI-6304] - Fix netlet dependency  [SPOI-6313] - Work Around APEX-129  [SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_7", 
            "text": "[SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-6148] - Update website with release 3.1  [SPOI-6241] - Change in enterprise license", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-310", 
            "text": "", 
            "title": "Version 3.1.0"
        }, 
        {
            "location": "/release_notes/#new-feature_5", 
            "text": "[SPOI-4670] - Enable message schema management in App Builder  [SPOI-5844] - Retrieve older versions of schemas", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_8", 
            "text": "[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_5", 
            "text": "[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.  [SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5743] - App package import also needs to do the typegraph stuff as upload  [SPOI-5831] - Add getter for properties for Twitter Demo  [SPOI-5833] - Schema class not loaded during validation  [SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts  [SPOI-5857] - Gateway has trouble getting to STRAM until after restart  [SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml  [SPOI-5946] - Port compatibility when port require schemas  [SPOI-6002] - AppBuilder fails to load operator due to NullPointerException", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_8", 
            "text": "[SPOI-5307] - Mocha based tests for Gateway API calls  [SPOI-5749] - Certify MapR sandbox  [SPOI-5750] - Create Splunk forwarder operators  [SPOI-5751] - Create splunk forwarder input operator  [SPOI-5752] - Create splunk forwarder output operator  [SPOI-5753] - Create splunk forwarder configuration specification  [SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file  [SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-5756] - Test license expiry on sandbox as part of sandbox testing  [SPOI-5764] - Gateway to show better error messages on all 500 errors  [SPOI-5803] - Support of custom aggregation for ADT  [SPOI-5893] - Changes for retrieving container and operator history information", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_5", 
            "text": "[SPOI-4946] - Handle new @useSchema and @description property metadata in UI  [SPOI-4980] - Schema Management in the UI  [SPOI-5505] - handle Object-to-Object port compatibility with and without schema  [SPOI-5506] - handle Object-to-Pojo port compat with schema  [SPOI-5513] - handle port compatibility with generic type ports  [SPOI-5747] - Automatically generate new eval license when sandbox starts up", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-300", 
            "text": "", 
            "title": "Version 3.0.0"
        }, 
        {
            "location": "/release_notes/#sub-task_6", 
            "text": "[SPOI-1901] - Dynamic property changes lost on AM restart   [SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package  [SPOI-4891] - Example for schema meta data and property doclet tag  [SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks  [SPOI-4972] - Design Schema API for managing schemas  [SPOI-4973] - Create a Schema resource which will contain schema calls  [SPOI-4987] - Generate pojo class using schema provided in json  [SPOI-4999] - Implement the rest call to save schema on the backend  [SPOI-5211] - Support custom visualization link  [SPOI-5237] - Type of attributes are wrong when it is an inner class or enum  [SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp  [SPOI-5361] - Button for dt.phoneHome.enable property on system config page  [SPOI-5527] - Implement Queue Option", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_6", 
            "text": "[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs  [SPOI-4633] - Resolve type variable across the type hierarchy   [SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times   [SPOI-4884] - Intermittent failure for CustomMetricTest  [SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.  [SPOI-4941] - For kafka operator in app builder certain properties need to be set twice  [SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically  [SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway  [SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency  [SPOI-5323] - Malhar operators are not packaged with distribution for 3.0  [SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page  [SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon  [SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes  [SPOI-5385] - No error message is returned when DTCli gets an NPE  [SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments  [SPOI-5433] - Asm code not working for jdk 1.8  [SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout  [SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle  [SPOI-5484] - Ingestion FTP as output does not work with vsftpd  [SPOI-5497] - Could not open pi demo in 3.0.0 RC2  [SPOI-5498] - Typegraph exception with 3.0.0 RC2  [SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404  [SPOI-5504] - Make temporary file names unique to avoid lease expiry  [SPOI-5530] - Failed to edit pidemo JSON based application  [SPOI-5536] - App jar is missing from typegraph  [SPOI-5552] - Non-concrete classes being returned with assignableClasses call  [SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table  [SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException   [SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data  [SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5583] - DFS root directory check is failing on MapR cluster  [SPOI-5593] - App Builder search tooltip  [SPOI-5597] - Bad log level WARNING in UI  [SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal  [SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo  [SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators  [SPOI-5627] - error message from install script from 3.0.0-RC4   [SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator  [SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK   [SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group  [SPOI-5636] - dtcp is not getting packaged in installation (RC4)  [SPOI-5637] - Allatori Configuration errors in ingestion pom.xml  [SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out  [SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.  [SPOI-5644] - isChar validation should strip \\u000 character  [SPOI-5646] - Megh demos is failing release build  [SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined  [SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode  [SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user  [SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids  [SPOI-5661] - Set default store in HDHTReader  [SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo  [SPOI-5664] - Omit Aggregator Registry From UI  [SPOI-5672] - App Data Tracker Compliation is failing  [SPOI-5673] - Unable to launch ingestion app with JMS as input source   [SPOI-5675] - Kafka keys population error  [SPOI-5676] - Fix API doc generation  [SPOI-5685] - Complete App Builder category and property name fixes  [SPOI-5688] - Correct interactive demo tutorials available in the Learn section  [SPOI-5689] - Installer produces error message about removing docs directory  [SPOI-5690] - Images missing from lean section tutorials after installing release build  [SPOI-5693] - Console does not upload default DT application packages  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5704] - App package references in docs  [SPOI-5705] - Build version incorrect  [SPOI-5706] - api and user docs not viewable through gateway  [SPOI-5707] - Packaging utilities jar in Ingestion  [SPOI-5708] - Update netlet dependency to release for 3.0  [SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_9", 
            "text": "[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application  [SPOI-4889] - Make BucketIdTagger fault tolerant  [SPOI-5320] - License URL should open in new window   [SPOI-5494] - Support \"queue\" query parameters from launch app modal  [SPOI-5535] - Console should validate duplicate app name in the same app package  [SPOI-5548] - Check compaction keys from UI  [SPOI-5603] - Add syntax highlighting to Markdown code blocks  [SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default  [SPOI-5623] - License upgrade link should open in separate tab  [SPOI-5633] - Improve markdown content display styles  [SPOI-5657] - Redirect to welcome screen of the Learn section after install  [SPOI-5694] - Ingestion default app-config should populate meaningful defaults", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_6", 
            "text": "[SPOI-4672] - Support for secure HA environments  [SPOI-5288] - Add \"launch\" button to each item in the list of app packages  [SPOI-5304] - Support ingestion install mode  [SPOI-5313] - Obfuscate ADT in the distribution build  [SPOI-5561] - Markdown documentation support in the Console", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_9", 
            "text": "[SPOI-4228] - The location of App Data Tracker from installation and from devel mode  [SPOI-4658] - App Data Tracker Technical Doc  [SPOI-4879] - Hide operators that have no input ports from App builder  [SPOI-4983] - Installer for Ingestion app  [SPOI-4984] - Obfuscate ingestion jar  [SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core  [SPOI-5032] - Review comparison for appDataFramework pull request to the master  [SPOI-5050] - Licensing-related changes to the UI for 3.0  [SPOI-5255] - Support enhancements to uiTypes  [SPOI-5314] - Include obfuscated App Data Tracker in the install bundle  [SPOI-5370] - Megh obfuscation  [SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator  [SPOI-5555] - [dtcp] set default app package for ingestion  [SPOI-5574] - Update installer README  [SPOI-5608] - Docs for backward compatibility in 3.0   [SPOI-5610] - Add testing to Markdown support models, pages, directives  [SPOI-5621] - Include Ingestion utilities in DT installer  [SPOI-5624] - [Ingestion] Hide message output destination when input is file source  [SPOI-5635] - Changing product name to dtIngest  [SPOI-5638] - Post ingestion packge to central DT maven repository  [SPOI-5639] - App Data Tracker Release Job  [SPOI-5645] - support \"short\" primitive type in app builder  [SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_7", 
            "text": "[MLHR-1726] - Bug in PojoUtils  [MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives  [MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing  [MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application  [MLHR-1789] - Complete Category and property name fixes", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_10", 
            "text": "[MLHR-1725] - Add Support for Setters to PojoUtils  [MLHR-1757] - change scope of dt-engine in maven build", 
            "title": "Improvement"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary of Terms\n\n\nApache Apex\n\n\n\n\nApache Hadoop\n\u00a0- \u00a0\nApache Hadoop\n is a programming framework that supports the processing of large data sets in a distributed computing environment.\n\n\nApplication\n\u00a0- unified batch and real-time stream processing application running on Apache Apex platform.\n\n\nContainer\n\u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.\n\n\nCurrent Window Id\n\u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.\n\n\nDAG\n\u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their \nLogical\n / \nPhysical\n plans, composed of operators connected by streams.\n\n\nData Tuples Emitted\n\u00a0- Number of data objects emitted by the operators with an output port.\n\n\nData Tuples Processed\n\u00a0- Number of data objects processed by the operators in an Apache Apex application.\n\n\nLogical Plan\n\u00a0- Logical representation of an Apache Apex application, where the computational nodes are called \nOperators\n and the data-flow edges are called\u00a0\nStreams\n.\n\n\nOperator\n\u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.\n\n\nPhysical Operator\n - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.\n\n\nPhysical Plan\n\u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.\n\n\nPort\n\u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.\n\n\nRecovery Window Id\n\u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.\n\n\nResource Manager\n\u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.\n\n\nSTRAM\n - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.\n\n\nStream\n\u00a0- A stream consists of data tuples that flow from one port of an operator to another.\n\n\nYARN\n\u00a0- \nApache Hadoop YARN\n (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.\n\n\n\n\nDataTorrent RTS\n\n\n\n\ndtAssemble\n\u00a0- graphical application assembly tool used to develop applications.\n\n\ndtDashboard\n\u00a0- graphical visualization tool to view and query system and application data.\n\n\ndtGateway\n\u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS\n\n\ndtManage\n\u00a0- the web based interface to install, configure, manage \n monitor Apache Apex applications running in a Hadoop Cluster", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary-of-terms", 
            "text": "", 
            "title": "Glossary of Terms"
        }, 
        {
            "location": "/glossary/#apache-apex", 
            "text": "Apache Hadoop \u00a0- \u00a0 Apache Hadoop  is a programming framework that supports the processing of large data sets in a distributed computing environment.  Application \u00a0- unified batch and real-time stream processing application running on Apache Apex platform.  Container \u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.  Current Window Id \u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.  DAG \u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their  Logical  /  Physical  plans, composed of operators connected by streams.  Data Tuples Emitted \u00a0- Number of data objects emitted by the operators with an output port.  Data Tuples Processed \u00a0- Number of data objects processed by the operators in an Apache Apex application.  Logical Plan \u00a0- Logical representation of an Apache Apex application, where the computational nodes are called  Operators  and the data-flow edges are called\u00a0 Streams .  Operator \u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.  Physical Operator  - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.  Physical Plan \u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.  Port \u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.  Recovery Window Id \u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.  Resource Manager \u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.  STRAM  - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.  Stream \u00a0- A stream consists of data tuples that flow from one port of an operator to another.  YARN \u00a0-  Apache Hadoop YARN  (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/glossary/#datatorrent-rts", 
            "text": "dtAssemble \u00a0- graphical application assembly tool used to develop applications.  dtDashboard \u00a0- graphical visualization tool to view and query system and application data.  dtGateway \u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS  dtManage \u00a0- the web based interface to install, configure, manage   monitor Apache Apex applications running in a Hadoop Cluster", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Apache Apex\n\n\nFor more information, please go to \nApache Apex\n.  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.\n\n\nDataTorrent RTS\n\n\n\n\nApex Comparison\n\n\nArchitecture\n\n\nFeatures Overview\n\n\nBlogs\n\n\nFeatured Resources\n\n\nWebinars\n\n\nRelease Notes\n\n\n\n\nDocumentation Archive\n\n\n\n\nDataTorrent RTS 3.1.0\n\n\nDataTorrent RTS 3.0.0\n\n\nDataTorrent RTS 2.0.1\n\n\nDataTorrent RTS 2.0.0\n\n\nDataTorrent RTS 1.0.4\n\n\nWebsite Documentation Links", 
            "title": "Resources"
        }, 
        {
            "location": "/additional_docs/#apache-apex", 
            "text": "For more information, please go to  Apache Apex .  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/additional_docs/#datatorrent-rts", 
            "text": "Apex Comparison  Architecture  Features Overview  Blogs  Featured Resources  Webinars  Release Notes", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/#documentation-archive", 
            "text": "DataTorrent RTS 3.1.0  DataTorrent RTS 3.0.0  DataTorrent RTS 2.0.1  DataTorrent RTS 2.0.0  DataTorrent RTS 1.0.4  Website Documentation Links", 
            "title": "Documentation Archive"
        }
    ]
}