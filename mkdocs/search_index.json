{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the DataTorrent RTS!\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-the-datatorrent-rts", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "Welcome to the DataTorrent RTS!"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nDataTorrent RTS Elastic Scalability Demo\n\n\n\n\n\nDataTorrent RTS Dimensional Computing Demo\n\n\n\n\n\nDataTorrent RTS Application Builder Demo\n\n\n\n\n\nDataTorrent RTS Fault Tolerance Demo", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "DataTorrent RTS Elastic Scalability Demo   DataTorrent RTS Dimensional Computing Demo   DataTorrent RTS Application Builder Demo   DataTorrent RTS Fault Tolerance Demo", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demo_sales/", 
            "text": "Sales Dimensions - Transform, Analyze and Alert\n\n\nSales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.\n\n\nA large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.\n\n\nIn order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.\n\n\nApplication setup will require following steps:\n\n\n\n\nInput\n - receive individual sales transactions\n\n\nTransform\n - convert incoming records into consumable form\n\n\nEnrich\n - provide additional information for each record by performing additional lookups\n\n\nCompute\n - perform aggregate computations on all possible key field combinations\n\n\nStore\n - store computed results for further analysis and visualizations\n\n\nAnalyze\n, Alert \n Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.\n\n\n\n\nCreate New Application\n\n\nDataTorrent platforms supports building new applications with \nGraphical Application Builder\n, which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.\n\n\nGo to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.\n\n\nThis will bring up the Application Builder interface\n\n\n\n\nAdd and Connect Operators\n\n\nFrom the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.\n\n\n\n\nJSON Sales Event Generator (Input)\n - generates synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.\n\n\nEnrichment (Enrich)\n - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n - stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n - dashboard connector for visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n - dashboard connector for visualization data results.\n\n\n\n\nConnect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.\n\n\n\n\nCustomize Application and Operator Settings\n\n\nBy clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:\n\n\n\n\n\n\nCopy the Sales schema below and paste the contents into the \nEvent Schema JSON\n field of \nInput\n operator, and \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n operators.\n\n\n{\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.  Set the \nBase Path\n to \nSalesDimensionsDemoStore\n value.  This sets the HDHT storage path to write dimensions computation results to the \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n\n\n\n\n\n\n\n\nClick on the stream and set the \nStream Locality\n to \nCONTAINER_LOCAL\n for all the streams between \nInput\n and \nCompute\n operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.\n\n\n\n\n\n\nLaunch Application\n\n\nOnce application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it. \n\n\n\n\nAfter launching, go to \nSales Dimensions\n application operations page in the \nMonitor\n section.\n\n\n\n\nConfirm that the application is launched successfully by looking for \nRunning\n state in the \nApplication Overview\n section, and all confirming all the operators are successfully started under \nStram Events\n section.  By navigating to \nPhysical\n view tab, and looking at \nInput\n, \nParse\n, \nEnrich\n, or \nCompute\n operators, notice that they are all deployed to the single container, thanks to the stream locality setting of \nCONTAINER_LOCAL\n we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.\n\n\n\n\nVisualize Data\n\n\nDataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see \nData Visualization\n tutorial.\n\n\nAfter application is started, a \nvisualize\n button, available in the \nApplication Overview\n section, can be used to quickly generate a new dashboard for the Sales Dimensions application.\n\n\n\n\nOnce dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/demo_sales/#sales-dimensions-transform-analyze-and-alert", 
            "text": "Sales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.  A large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.  In order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.  Application setup will require following steps:   Input  - receive individual sales transactions  Transform  - convert incoming records into consumable form  Enrich  - provide additional information for each record by performing additional lookups  Compute  - perform aggregate computations on all possible key field combinations  Store  - store computed results for further analysis and visualizations  Analyze , Alert   Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.", 
            "title": "Sales Dimensions - Transform, Analyze and Alert"
        }, 
        {
            "location": "/demo_sales/#create-new-application", 
            "text": "DataTorrent platforms supports building new applications with  Graphical Application Builder , which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.  Go to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.  This will bring up the Application Builder interface", 
            "title": "Create New Application"
        }, 
        {
            "location": "/demo_sales/#add-and-connect-operators", 
            "text": "From the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.   JSON Sales Event Generator (Input)  - generates synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.  Enrichment (Enrich)  - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.  Simple App Data Dimensions Store (Store)  - stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)  - dashboard connector for visualization queries.  App Data Pub Sub Result (Result)  - dashboard connector for visualization data results.   Connect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.", 
            "title": "Add and Connect Operators"
        }, 
        {
            "location": "/demo_sales/#customize-application-and-operator-settings", 
            "text": "By clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:    Copy the Sales schema below and paste the contents into the  Event Schema JSON  field of  Input  operator, and  Configuration Schema JSON  of the  Compute  and  Store  operators.  {\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to  SalesDimensionsQuery  and  SalesDimensionsResult  respectively.    Select the  Store  operator, and edit the  File Store  property.  Set the  Base Path  to  SalesDimensionsDemoStore  value.  This sets the HDHT storage path to write dimensions computation results to the  /user/ username /SalesDimensionsDemoStore  on HDFS.     Click on the stream and set the  Stream Locality  to  CONTAINER_LOCAL  for all the streams between  Input  and  Compute  operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.", 
            "title": "Customize Application and Operator Settings"
        }, 
        {
            "location": "/demo_sales/#launch-application", 
            "text": "Once application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.    After launching, go to  Sales Dimensions  application operations page in the  Monitor  section.   Confirm that the application is launched successfully by looking for  Running  state in the  Application Overview  section, and all confirming all the operators are successfully started under  Stram Events  section.  By navigating to  Physical  view tab, and looking at  Input ,  Parse ,  Enrich , or  Compute  operators, notice that they are all deployed to the single container, thanks to the stream locality setting of  CONTAINER_LOCAL  we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.", 
            "title": "Launch Application"
        }, 
        {
            "location": "/demo_sales/#visualize-data", 
            "text": "DataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see  Data Visualization  tutorial.  After application is started, a  visualize  button, available in the  Application Overview  section, can be used to quickly generate a new dashboard for the Sales Dimensions application.   Once dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Visualize Data"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nOverview\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\nWhen a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port). \n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201claunch\u201d button in the top left once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#overview", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:   Operator Library Navigator   Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.   Canvas   The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.   Inspector  The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.  Operator Inspector   When an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.   Port Inspector  When a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).   Stream Inspector   The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "Pi Demo  As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201claunch\u201d button in the top left once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nOverview\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.\n\n\nAnd below is the DAG for the Ads Dimension Demo:\n\n\n\n\nIn this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.\n\n\nAll these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.  \nApp Data Framework Programming Guide\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new button on the Dashboards screen\n\n\ngenerate new dashboard option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nSelect compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.\n\n\n\n\n\n\nChoose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nModifying Dashboards\n\n\nDashboards controls are presented as a row of buttons just below the dashboard title and description.\n\n\n\n\nNew widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget \n\n\nAlternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.\n\n\n\n\nWhether using auto generate buttons, results are not persisted until save changes is applied.\n\n\nEach data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "Twitter Example  The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:   Ads Dimension Example  The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.  And below is the DAG for the Ads Dimension Demo:   In this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.  All these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.   App Data Framework Programming Guide", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview_1", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.   Below is an example of accessing the data visualization dashboard from a running application.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new button on the Dashboards screen  generate new dashboard option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)    Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Select compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.    Choose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#modifying-dashboards", 
            "text": "Dashboards controls are presented as a row of buttons just below the dashboard title and description.   New widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.", 
            "title": "Modifying Dashboards"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget   Alternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.   Whether using auto generate buttons, results are not persisted until save changes is applied.  Each data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest\n - Unified Streaming \n Batch Data Ingestion for Hadoop\n\n\nThe Hadoop data lake is only as good as the data in the lake. Given the variety of data sources that need to pump data into Hadoop, customers often need to set-up one-off data ingestion jobs. These one-off jobs copy files using FTP \n NFS mounts or try to use standalone tools like \u2018distCP\u2019 to move data in and out of hadoop. Since these jobs stitch together multiple tools, they encounter several problems around manageability, failure recovery, ability to scale to handle data skews. The DataTorrent data ingestion \n distribution application is \n\n\nKey Features\n\n\nDataTorrent dtIngest makes configuring and running Hadoop data ingestion and data extraction a point-and-click process and includes enterprise-grade features not available in the market today:\n\n\n\n\nApache 2.0 open-source Project Apex based\n \u2013 Built on \nProject Apex\n, dtIngest is a native YARN application. It is completely fault tolerant so unlike other tools like distCP, it can \u2018resume\u2019 file ingest on failure. It is horizontally scalable and support extremely high throughput and low latency data ingest.\n\n\nSimple to use \n manage\n \u2013 A point-and-click application user interface makes it easy to configure, save \n launch multiple data ingestion \n distribution pipelines. Centralized management, visibility, monitoring and summary logs\n\n\nBatch as well as stream data\n -  dtIngest supports moving data between NFS, (S)FTP, HDFS, AWS S3n, Kafka and JMS so you can use one platform to exchange data across multiple endpoints.\n\n\nHDFS small file ingest using \u2018compaction\u2019\n \u2013  Configurable automatic compaction of small files into large files during ingest into HDFS. Helps prevent running out of HDFS namenode namespace\n\n\nSecure and efficient data movement\n \u2013 Supports compression and encryption during ingest. Works with kerberos enabled secure Hadoop clusters.\n\n\nRuns in any Hadoop 2.0 Cluster\n -  \nCertified\n to run across all major Hadoop distros in physical, virtual or in the cloud deployments.\n\n\n\n\nSample Use Cases for dtIngest\n\n\n\n\nBulk as well as incremental data loading of large as well as small files into Hadoop\n\n\nDistributing cleansed/normalized data from Hadoop\n\n\nIngesting change data from Kafka/JMS into Hadoop\n\n\nSelectively replicating data from one Hadoop cluster to another\n\n\nIngest streaming event data into hadoop\n\n\nReplaying log data stored in HDFS as Kafka/JMS streams\n\n\n\n\nUsing dtIngest\n\n\n\n\ndtIngest is free to use with all three DataTorrent \neditions\n It is available under the application package named \u2018dtIngest\u2019\n\n\nNavigate the the \u2018Develop\u2019 tab \u2018Launch\u2019 to launch a new ingestion application\n\n\n\n\ndtIngest is designed to move data between any of the supported data sources \n destinations. Just pick \n configure the ones you need. Also, when moving data from file based sources, you can choose for the pipeline to run \u2018one time\u2019 or to continuously poll the input directories to look for files that match the filtering criteria\n\n\n\n\n\n\n\n\nWhen copying data to \u2018file\u2019 based destinations several useful options are available. These include being able to keep source directory structure, overwrite files on destination \n automatically creating a hourly directory structure to track when data was written at the output. \n\n\n\n\n\n\n\n\nBefore saving the data into the destination location, it can be compressed as well as encrypted.\n\n\n\n\n\n\n\n\nWhen ingesting data into HDFS, small files can be combined into \u2018large\u2019 files to help work around the namenode namespace restrictions.  \n\n\n\n\n\n\n\n\nAfter configuring the ingestion application hit the \u2018Launch\u2019 button to launch the ingestion application. All the requisite connectors like Kafka, JMS, HDFS etc. will be automatically instantiated and the right operators for compression, encryption etc will be inserted into the application to generate the \u2018DAG\u2019.\n\nThe screenshot below shows sample of a DAG that is generated to read files from FTP and ingest them into HDFS\n\n\n\n\n\n\n\n\nOnce the application is launched, you can navigate the the \u2018Monitor\u2019 tab and find your application instance in the list. Clicking on the application instance will give you complete visibility into the metrics of the application. \n\n\n\n\nSummary logs from the application will be available under the \u2018summary\u2019 folder in the HDFS directory dedicated for the application. On the sandbox, you can substitute the \nuser-id\n with \ndtadmin\n/user/\nuser-ID\n/datatorrents/apps/APP_ID/summary", 
            "title": "dtIngest - Unified Batch and Streaming Ingestion"
        }, 
        {
            "location": "/dtingest/#dtingest-unified-streaming-batch-data-ingestion-for-hadoop", 
            "text": "The Hadoop data lake is only as good as the data in the lake. Given the variety of data sources that need to pump data into Hadoop, customers often need to set-up one-off data ingestion jobs. These one-off jobs copy files using FTP   NFS mounts or try to use standalone tools like \u2018distCP\u2019 to move data in and out of hadoop. Since these jobs stitch together multiple tools, they encounter several problems around manageability, failure recovery, ability to scale to handle data skews. The DataTorrent data ingestion   distribution application is", 
            "title": "dtIngest - Unified Streaming &amp; Batch Data Ingestion for Hadoop"
        }, 
        {
            "location": "/dtingest/#key-features", 
            "text": "DataTorrent dtIngest makes configuring and running Hadoop data ingestion and data extraction a point-and-click process and includes enterprise-grade features not available in the market today:   Apache 2.0 open-source Project Apex based  \u2013 Built on  Project Apex , dtIngest is a native YARN application. It is completely fault tolerant so unlike other tools like distCP, it can \u2018resume\u2019 file ingest on failure. It is horizontally scalable and support extremely high throughput and low latency data ingest.  Simple to use   manage  \u2013 A point-and-click application user interface makes it easy to configure, save   launch multiple data ingestion   distribution pipelines. Centralized management, visibility, monitoring and summary logs  Batch as well as stream data  -  dtIngest supports moving data between NFS, (S)FTP, HDFS, AWS S3n, Kafka and JMS so you can use one platform to exchange data across multiple endpoints.  HDFS small file ingest using \u2018compaction\u2019  \u2013  Configurable automatic compaction of small files into large files during ingest into HDFS. Helps prevent running out of HDFS namenode namespace  Secure and efficient data movement  \u2013 Supports compression and encryption during ingest. Works with kerberos enabled secure Hadoop clusters.  Runs in any Hadoop 2.0 Cluster  -   Certified  to run across all major Hadoop distros in physical, virtual or in the cloud deployments.", 
            "title": "Key Features"
        }, 
        {
            "location": "/dtingest/#sample-use-cases-for-dtingest", 
            "text": "Bulk as well as incremental data loading of large as well as small files into Hadoop  Distributing cleansed/normalized data from Hadoop  Ingesting change data from Kafka/JMS into Hadoop  Selectively replicating data from one Hadoop cluster to another  Ingest streaming event data into hadoop  Replaying log data stored in HDFS as Kafka/JMS streams", 
            "title": "Sample Use Cases for dtIngest"
        }, 
        {
            "location": "/dtingest/#using-dtingest", 
            "text": "dtIngest is free to use with all three DataTorrent  editions  It is available under the application package named \u2018dtIngest\u2019  Navigate the the \u2018Develop\u2019 tab \u2018Launch\u2019 to launch a new ingestion application   dtIngest is designed to move data between any of the supported data sources   destinations. Just pick   configure the ones you need. Also, when moving data from file based sources, you can choose for the pipeline to run \u2018one time\u2019 or to continuously poll the input directories to look for files that match the filtering criteria     When copying data to \u2018file\u2019 based destinations several useful options are available. These include being able to keep source directory structure, overwrite files on destination   automatically creating a hourly directory structure to track when data was written at the output.      Before saving the data into the destination location, it can be compressed as well as encrypted.     When ingesting data into HDFS, small files can be combined into \u2018large\u2019 files to help work around the namenode namespace restrictions.       After configuring the ingestion application hit the \u2018Launch\u2019 button to launch the ingestion application. All the requisite connectors like Kafka, JMS, HDFS etc. will be automatically instantiated and the right operators for compression, encryption etc will be inserted into the application to generate the \u2018DAG\u2019. \nThe screenshot below shows sample of a DAG that is generated to read files from FTP and ingest them into HDFS     Once the application is launched, you can navigate the the \u2018Monitor\u2019 tab and find your application instance in the list. Clicking on the application instance will give you complete visibility into the metrics of the application.    Summary logs from the application will be available under the \u2018summary\u2019 folder in the HDFS directory dedicated for the application. On the sandbox, you can substitute the  user-id  with  dtadmin /user/ user-ID /datatorrents/apps/APP_ID/summary", 
            "title": "Using dtIngest"
        }, 
        {
            "location": "/apex_core/", 
            "text": "Apache Apex\n\n\nApache Apex (incubating) is the industry\u2019s only Apache 2.0 licensed open source enterprise grade unified stream and batch processing engine.  Project Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex Core"
        }, 
        {
            "location": "/apex_core/#apache-apex", 
            "text": "Apache Apex (incubating) is the industry\u2019s only Apache 2.0 licensed open source enterprise grade unified stream and batch processing engine.  Project Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex-Malhar is an open source operator and codec library that can be used with the DataTorrent platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of DataTorrent RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 DataTorrent Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 DataTorrent Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code. \nSome operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application. \n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted. \n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like DataTorrent RTS is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis. \n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows. \n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into a DataTorrent streaming application. E.g. In the Telco space, a Java based CDR parser can be directly plugged into DataTorrent RTS. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files \n Syslog.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nDataTorrent supports an operator to connect to the popular Twitter stream fire hose", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex-Malhar is an open source operator and codec library that can be used with the DataTorrent platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of DataTorrent RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 DataTorrent Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 DataTorrent Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.   Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.    Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like DataTorrent RTS is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.   Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.   Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into a DataTorrent streaming application. E.g. In the Telco space, a Java based CDR parser can be directly plugged into DataTorrent RTS. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files   Syslog.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "DataTorrent supports an operator to connect to the popular Twitter stream fire hose", 
            "title": "Social Media"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Additional Documentation\n\n\nDataTorrent RTS\n\n\nFor more DataTorrent documentation visit\n\n\n\n\nFeatured Resources\n\n\nProduct Features\n\n\nArchitecture Overview\n\n\nDocumentation and Guides\n\n\n\n\nFor webinars and videos check out\n\n\n\n\nWebinars\n\n\nSolution Demos\n\n\n\n\nApache Apex\n\n\nTo find out more about Apache Apex visit\n\n\n\n\nApache Apex (incubating): \nhttp://apex.incubator.apache.org/\n\n\nApex Mailing List: \ndev@apex.incubator.apache.org\n\n\nApex Overview and Comparison", 
            "title": "Additinal Docs"
        }, 
        {
            "location": "/additional_docs/#additional-documentation", 
            "text": "DataTorrent RTS  For more DataTorrent documentation visit   Featured Resources  Product Features  Architecture Overview  Documentation and Guides   For webinars and videos check out   Webinars  Solution Demos   Apache Apex  To find out more about Apache Apex visit   Apache Apex (incubating):  http://apex.incubator.apache.org/  Apex Mailing List:  dev@apex.incubator.apache.org  Apex Overview and Comparison", 
            "title": "Additional Documentation"
        }
    ]
}